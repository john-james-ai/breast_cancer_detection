
@article{kozegarComputerAidedDetection2020,
	title = {Computer aided detection in automated 3-{D} breast ultrasound images: a survey},
	volume = {53},
	issn = {0269-2821, 1573-7462},
	shorttitle = {Computer aided detection in automated 3-{D} breast ultrasound images},
	url = {http://link.springer.com/10.1007/s10462-019-09722-7},
	doi = {10.1007/s10462-019-09722-7},
	abstract = {Nowadays, breast cancer is the leading cause of cancer death for women all over the world. Since the reason of breast cancer is unknown, early detection of the disease plays an important role in cancer control, saving lives and reducing costs. Among different modalities, automated 3-D breast ultrasound (3-D ABUS) is a new and effective imaging modality which has attracted a lot of interest as an adjunct to mammography for women with dense breasts. However, reading ABUS images is time consuming for radiologists and subtle abnormalities may be overlooked. Hence, computer aided detection (CADe) systems can be utilized as a second interpreter to assist radiologists to increase their screening speed and sensitivity. In this paper, a general architecture representing different CADe systems for ABUS images is introduced and the approaches for implementation of each block are categorized and reviewed. In addition, the limitations of these systems are discussed and their performance in terms of sensitivity and number of false positives per volume are compared.},
	language = {en},
	number = {3},
	urldate = {2023-05-24},
	journal = {Artificial Intelligence Review},
	author = {Kozegar, Ehsan and Soryani, Mohsen and Behnam, Hamid and Salamati, Masoumeh and Tan, Tao},
	month = mar,
	year = {2020},
	pages = {1919--1941},
	file = {Kozegar et al. - 2020 - Computer aided detection in automated 3-D breast u.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WU6VX75Y\\Kozegar et al. - 2020 - Computer aided detection in automated 3-D breast u.pdf:application/pdf},
}

@article{mahmoodAutomatedInDepthFeature2021,
	title = {An {Automated} {In}-{Depth} {Feature} {Learning} {Algorithm} for {Breast} {Abnormality} {Prognosis} and {Robust} {Characterization} from {Mammography} {Images} {Using} {Deep} {Transfer} {Learning}},
	volume = {10},
	issn = {2079-7737},
	url = {https://www.mdpi.com/2079-7737/10/9/859},
	doi = {10.3390/biology10090859},
	abstract = {Background: Diagnosing breast cancer masses and calcification clusters have paramount significance in mammography, which aids in mitigating the disease’s complexities and curing it at early stages. However, a wrong mammogram interpretation may lead to an unnecessary biopsy of the false-positive findings, which reduces the patient’s survival chances. Consequently, approaches that learn to discern breast masses can reduce the number of misconceptions and incorrect diagnoses. Conventionally used classification models focus on feature extraction techniques specific to a particular problem based on domain information. Deep learning strategies are becoming promising alternatives to solve the many challenges of feature-based approaches. Methods: This study introduces a convolutional neural network (ConvNet)-based deep learning method to extract features at varying densities and discern mammography’s normal and suspected regions. Two different experiments were carried out to make an accurate diagnosis and classification. The first experiment consisted of five end-to-end pre-trained and fine-tuned deep convolution neural networks (DCNN). The in-depth features extracted from the ConvNet are also used to train the support vector machine algorithm to achieve excellent performance in the second experiment. Additionally, DCNN is the most frequently used image interpretation and classification method, including VGGNet, GoogLeNet, MobileNet, ResNet, and DenseNet. Moreover, this study pertains to data cleaning, preprocessing, and data augmentation, and improving mass recognition accuracy. The efficacy of all models is evaluated by training and testing three mammography datasets and has exhibited remarkable results. Results: Our deep learning ConvNet+SVM model obtained a discriminative training accuracy of 97.7\% and validating accuracy of 97.8\%, contrary to this, VGGNet16 method yielded 90.2\%, 93.5\% for VGGNet19, 63.4\% for GoogLeNet, 82.9\% for MobileNetV2, 75.1\% for ResNet50, and 72.9\% for DenseNet121. Conclusions: The proposed model’s improvement and validation are appropriated in conventional pathological practices that conceivably reduce the pathologist’s strain in predicting clinical outcomes by analyzing patients’ mammography images.},
	language = {en},
	number = {9},
	urldate = {2023-05-24},
	journal = {Biology},
	author = {Mahmood, Tariq and Li, Jianqiang and Pei, Yan and Akhtar, Faheem},
	month = sep,
	year = {2021},
	pages = {859},
	file = {Mahmood et al. - 2021 - An Automated In-Depth Feature Learning Algorithm f.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SLQBIMXG\\Mahmood et al. - 2021 - An Automated In-Depth Feature Learning Algorithm f.pdf:application/pdf},
}

@article{NoTitleFound,
	title = {[{No} title found]},
	issn = {25825208},
	abstract = {Breast cancer is the second leading cause of cancer deaths worldwide and occurs in one in eight women Breast cancer is a malignant tumor that develops when cells divide in the breast tissue and grows without normal control of cell death and cell division. These factors include attributes such as age, genetic risk, and family history. Breast cancer is again indicated by prognostic variables such as age, lymph nodes with high nuclear grade, large tumors, estrogen and progesterone receptors, low levels of a high synthesized phase, high histologic grade and involvement of cancer cells. It can be used to identify and exploit knowledge and relationships among a large number of variables expected to re-predict breast cancer using historical cases stored in datasets. The first Multivariate Analysis Algorithm (MAA) has filed a collaborative approach to the study of the correlation between the reductions of intrinsic variables. For rapid growth due to lack of information, solutions are considered and these data sets must be provided in order to handle useful information or knowledge. In addition, decision makers need to be able to develop the best prediction they need to have a machine learning approach, and performance enhancement in the Multivariate analysis algorithm (MAA) so that such fluctuation is capable of rapidly increasing significant bits of knowledge Accuracy and speed of recurrence of breast cancer. Investigating the set up for this second task is a large example SEER data filed with such as the Contact Rule Processing Technology for Successful Detection of Breast Cancer Recurrence.},
	language = {en},
	journal = {International Research Journal of Modernization in Engineering Technology and Science},
	file = {[No title found].pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FNE4NTK7\\[No title found].pdf:application/pdf},
}

@article{jaamourBreastCancerDetection,
	title = {Breast {Cancer} {Detection} in {Mammograms} using {Deep} {Learning} {Techniques}},
	language = {en},
	author = {Jaamour, Adam},
	file = {Jaamour - Breast Cancer Detection in Mammograms using Deep L.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DXZ246GX\\Jaamour - Breast Cancer Detection in Mammograms using Deep L.pdf:application/pdf},
}

@article{montahaBreastNet18HighAccuracy2021,
	title = {{BreastNet18}: {A} {High} {Accuracy} {Fine}-{Tuned} {VGG16} {Model} {Evaluated} {Using} {Ablation} {Study} for {Diagnosing} {Breast} {Cancer} from {Enhanced} {Mammography} {Images}},
	volume = {10},
	issn = {2079-7737},
	shorttitle = {{BreastNet18}},
	url = {https://www.mdpi.com/2079-7737/10/12/1347},
	doi = {10.3390/biology10121347},
	abstract = {Background: Identification and treatment of breast cancer at an early stage can reduce mortality. Currently, mammography is the most widely used effective imaging technique in breast cancer detection. However, an erroneous mammogram based interpretation may result in false diagnosis rate, as distinguishing cancerous masses from adjacent tissue is often complex and error-prone. Methods: Six pre-trained and fine-tuned deep CNN architectures: VGG16, VGG19, MobileNetV2, ResNet50, DenseNet201, and InceptionV3 are evaluated to determine which model yields the best performance. We propose a BreastNet18 model using VGG16 as foundational base, since VGG16 performs with the highest accuracy. An ablation study is performed on BreastNet18, to evaluate its robustness and achieve the highest possible accuracy. Various image processing techniques with suitable parameter values are employed to remove artefacts and increase the image quality. A total dataset of 1442 preprocessed mammograms was augmented using seven augmentation techniques, resulting in a dataset of 11,536 images. To investigate possible overfitting issues, a k-fold cross validation is carried out. The model was then tested on noisy mammograms to evaluate its robustness. Results were compared with previous studies. Results: Proposed BreastNet18 model performed best with a training accuracy of 96.72\%, a validating accuracy of 97.91\%, and a test accuracy of 98.02\%. In contrast to this, VGGNet19 yielded test accuracy of 96.24\%, MobileNetV2 77.84\%, ResNet50 79.98\%, DenseNet201 86.92\%, and InceptionV3 76.87\%. Conclusions: Our proposed approach based on image processing, transfer learning, fine-tuning, and ablation study has demonstrated a high correct breast cancer classification while dealing with a limited number of complex medical images.},
	language = {en},
	number = {12},
	urldate = {2023-05-24},
	journal = {Biology},
	author = {Montaha, Sidratul and Azam, Sami and Rafid, Abul Kalam Muhammad Rakibul Haque and Ghosh, Pronab and Hasan, Md. Zahid and Jonkman, Mirjam and De Boer, Friso},
	month = dec,
	year = {2021},
	pages = {1347},
	file = {Montaha et al. - 2021 - BreastNet18 A High Accuracy Fine-Tuned VGG16 Mode.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PD5U9UJN\\Montaha et al. - 2021 - BreastNet18 A High Accuracy Fine-Tuned VGG16 Mode.pdf:application/pdf},
}

@article{wuCS231NProjectBreast,
	title = {{CS231N} {Project}: {Breast} {Cancer} {Tumor} {Detection}},
	abstract = {We implemented a breast cancer detection convolutional neural network (CNN), giving it ultrasound images of breast tumors as input and receiving per-image labels of ”benign” or malignant” as output. We surveyed multiple literature sources both in the area of breast cancer detection as well as image segmentation as a general field during this process, and ultimately used a Mask R-CNN as our network architecture, which we manually implemented via PyTorch. For the dataset, we utilized the Breast Ultrasound Images Dataset [1] as made publicly available on Kaggle. We experimented with various components of our architecture, including the Region Proposal Network (RPN) architecture and (hyper)parameter values, and achieved an 45.24\% training accuracy, 48.27\% validation accuracy, and 43.41\% test accuracy on our best-performing model.},
	language = {en},
	author = {Wu, Ann and Truong, Takara},
	file = {Wu and Truong - CS231N Project Breast Cancer Tumor Detection.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2RXRVHPE\\Wu and Truong - CS231N Project Breast Cancer Tumor Detection.pdf:application/pdf},
}

@article{fallahiExpertSystemDetection2011,
	title = {An {Expert} {System} for {Detection} of {Breast} {Cancer} {Using} {Data} {Preprocessing} and {Bayesian} {Network}},
	volume = {34},
	abstract = {This paper presents an automatic system for detection of breast cancer using data preprocessing and Bayesian network. In this study, ReliefF algorithm is used for reducing the dimension of breast cancer database then a pre-processing is done on the data and ultimately Bayesian network classifier is used for classification. The system performance has been compared with model NN (neural network) and AR + NN (neural networks combined with association rules). The dimension of input feature space is reduced from nine to eight by using ReliefF. In test stage, 3-fold cross validation method was applied to the Wisconsin breast cancer database to evaluate the proposed system performance. The correct classification rate of proposed system is 98.1\%.This research offered that the preprocessing is necessary on this data and combination of ReliefF and Bayesian network can be used to obtain fast automatic diagnostic systems for breast cancer.},
	language = {en},
	journal = {International Journal of Advanced Science and Technology},
	author = {Fallahi, Amir and Jafari, Shahram},
	year = {2011},
	file = {Fallahi and Jafari - 2011 - An Expert System for Detection of Breast Cancer Us.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\U54556CP\\Fallahi and Jafari - 2011 - An Expert System for Detection of Breast Cancer Us.pdf:application/pdf},
}

@article{charatePreprocessingMethodsMammogram,
	title = {The {Preprocessing} {Methods} of {Mammogram} {Images} for {Breast} {Cancer} {Detection}},
	volume = {5},
	language = {en},
	number = {1},
	journal = {International Journal on Recent and Innovation Trends in Computing and Communication},
	author = {Charate, A P and Jamge, S B},
	file = {Charate and Jamge - The Preprocessing Methods of Mammogram Images for .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ABBJJEZF\\Charate and Jamge - The Preprocessing Methods of Mammogram Images for .pdf:application/pdf},
}

@article{beeravoluPreprocessingBreastCancer2021,
	title = {Preprocessing of {Breast} {Cancer} {Images} to {Create} {Datasets} for {Deep}-{CNN}},
	volume = {9},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9352727/},
	doi = {10.1109/ACCESS.2021.3058773},
	abstract = {Breast cancer is the most diagnosed cancer in Australia with crude incidence rates increasing drastically from 62.8 at ages 35-39 to 271.4 at ages 50-54 (cases per 100,000 women). Various researchers have proposed methods and tools based on Machine Learning and Convolutional Neural Networks for assessing mammographic images, but these methods have produced detection and interpretation errors resulting in false-positive and false-negative cases when used in the real world. We believe that this problem can potentially be resolved by implementing effective image pre-processing techniques to create training data for Deep-CNN. Therefore, the main aim of this research is to propose effective image pre-processing methods to create datasets that can save computational time for the neural network and improve accuracy and classiﬁcation rates. To do so, this research proposes methods for background removal, pectoral muscle removal, adding noise to the images, and image enhancements. Adding noise without affecting the quality of details in the images makes the input images for the neural network more representative, which may improve the performance of the neural network model when used in the real world. The proposed method for background removal is the ‘‘Rolling Ball Algorithm’’ and ‘‘Huang’s Fuzzy Thresholding’’, which succeed in removing background from 100\% of the images. For pectoral muscle removal ‘‘Canny Edge Detection’’ and ‘‘Hough’s Line Transform’’ are used, which removed muscle from 99.06\% of the images. ‘‘Invert’’, ‘‘CTI\_RAS’’ and ‘‘ISOCONTOUR’’ lookup tables (LUTs) were used for image enhancements to outline the ROIs and regions within the ROIs.},
	language = {en},
	urldate = {2023-05-24},
	journal = {IEEE Access},
	author = {Beeravolu, Abhijith Reddy and Azam, Sami and Jonkman, Mirjam and Shanmugam, Bharanidharan and Kannoorpatti, Krishnan and Anwar, Adnan},
	year = {2021},
	pages = {33438--33463},
	file = {Beeravolu et al. - 2021 - Preprocessing of Breast Cancer Images to Create Da.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2RUIVMMR\\Beeravolu et al. - 2021 - Preprocessing of Breast Cancer Images to Create Da.pdf:application/pdf},
}

@article{chaudhuryEffectiveImageProcessing2022,
	title = {Effective {Image} {Processing} and {Segmentation}-{Based} {Machine} {Learning} {Techniques} for {Diagnosis} of {Breast} {Cancer}},
	volume = {2022},
	issn = {1748-6718, 1748-670X},
	url = {https://www.hindawi.com/journals/cmmm/2022/6841334/},
	doi = {10.1155/2022/6841334},
	abstract = {Breast cancer is the second leading cause of death among women, behind only heart disease. However, despite the high incidence and mortality rates associated with breast cancer, it is still unclear as to what is responsible for its development in the first place. The prevention of breast cancer is not possible with any of the current available methods. Patients who are diagnosed and treated for breast cancer at an early stage have a better chance of having a successful treatment and recovery. In the field of breast cancer detection, digital mammography is widely acknowledged to be a highly effective method of detecting the disease early on. We may be able to improve early detection of breast cancer with the use of image processing techniques, thereby boosting our chances of survival and treatment success. This article discusses a breast cancer image processing and machine learning framework that was developed. The input data set for this framework is a sequence of mammography images, which are used as input data. The CLAHE approach is then utilized to improve the overall quality of the photographs by means of image processing. It is called contrast restricted adaptive histogram equalization (CLAHE), and it is an improvement on the original histogram equalization technique. This aids in the removal of noise from photographs while simultaneously improving picture quality. The segmentation of images is the next step in the framework’s development. An image is divided into distinct portions at this point because the pixels are labeled at this step. This assists in the identification of objects and the delineation of boundaries. To categorize these preprocessed images, techniques such as fuzzy SVM, Bayesian classifier, and random forest are employed, among others.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Computational and Mathematical Methods in Medicine},
	author = {Chaudhury, Sushovan and Krishna, Alla Naveen and Gupta, Suneet and Sankaran, K. Sakthidasan and Khan, Samiullah and Sau, Kartik and Raghuvanshi, Abhishek and Sammy, F.},
	editor = {Koundal, Deepika},
	month = apr,
	year = {2022},
	pages = {1--6},
	file = {Chaudhury et al. - 2022 - Effective Image Processing and Segmentation-Based .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\H7WGKB64\\Chaudhury et al. - 2022 - Effective Image Processing and Segmentation-Based .pdf:application/pdf},
}

@article{devakumariStudyBreastCancer,
	title = {Study of {Breast} {Cancer} {Detection} {Methods} using {Image} {Processing} with {Data} {Mining} {Techniques}},
	abstract = {Breast cancer disturbs one in eight women. It is absolutely dreadful and a life threatening disease. The causal agent of breast cancer is still under research. But there are some jeopardy factors such as age, gene, obesity, taking birth control pills and smoking. Normally breast cancer is a malignant tumour that initiates in the cells of the breast and eventually it extents to the surrounding tissues. The disease can be preserved if it is detected early. As stages increase, the chance of preserving decreases. There are numerous imaging techniques that play a vital role in detecting breast cancer. This research study analyses various breast cancer detection techniques based on image processing techniques, data mining methods, various features used and a brief comparative study of the existing breast cancer detection system.},
	language = {en},
	author = {Devakumari, Dr D and Punithavathi, V},
	file = {Devakumari and Punithavathi - Study of Breast Cancer Detection Methods using Ima.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\C9XSUCVA\\Devakumari and Punithavathi - Study of Breast Cancer Detection Methods using Ima.pdf:application/pdf},
}

@article{avciNovelMedicalImage2023,
	title = {A {Novel} {Medical} {Image} {Enhancement} {Algorithm} for {Breast} {Cancer} {Detection} on {Mammography} {Images} {Using} {Machine} {Learning}},
	volume = {13},
	issn = {2075-4418},
	url = {https://www.mdpi.com/2075-4418/13/3/348},
	doi = {10.3390/diagnostics13030348},
	abstract = {Mammography is the most preferred method for breast cancer screening. In this study, computer-aided diagnosis (CAD) systems were used to improve the image quality of mammography images and to detect suspicious areas. The main contribution of this study is to reveal the optimal combination of various pre-processing algorithms to enable better interpretation and classiﬁcation of mammography images because pre-processing algorithms signiﬁcantly affect the accuracy of segmentation and classiﬁcation methods. In this study, the effect of combinations of different preprocessing methods in differentiating benign and malignant breast lesions was investigated. All image processing algorithms used for lesion detection were used in the mini-MIAS database. In the ﬁrst step, label information and pectoral muscle resulting from the acquisition of mammography images were removed. In the second step, median ﬁlter (MF), contrast limited adaptive histogram equalization (CLAHE), and unsharp masking (USM) algorithms with different combinations of the resolution and visibility of images are increased. In the third step, suspicious regions are extracted from the mammograms using the k-means clustering technique. Then, features were extracted from the obtained ROIs. Finally, feature datasets were classiﬁed as normal/abnormal, and benign/malign (two class classiﬁcation) using Machine Learning algorithms. Test performance measures of the classiﬁcation methods were examined. In both classiﬁcations made in the study, lower classiﬁcation performance values were obtained when the CLAHE algorithm was used alone as a pre-processing method compared to other pre-processing combinations. When the median ﬁlter and unsharp masking algorithms are added to the CLAHE algorithm, the performance of the classiﬁcation methods has increased. In terms of classiﬁcation success, Support Vector Machines, Random Forest, and Neural Networks showed the best performance. It was found by comparing the performances of the classiﬁcation methods that different preprocessing algorithms were effective in detecting the presence of breast lesions and distinguishing benign and malignant.},
	language = {en},
	number = {3},
	urldate = {2023-05-24},
	journal = {Diagnostics},
	author = {Avcı, Hanife and Karakaya, Jale},
	month = jan,
	year = {2023},
	pages = {348},
	file = {Avcı and Karakaya - 2023 - A Novel Medical Image Enhancement Algorithm for Br.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TD2UM654\\Avcı and Karakaya - 2023 - A Novel Medical Image Enhancement Algorithm for Br.pdf:application/pdf},
}

@article{almalkiComputerizedAnalysisMammogram2022,
	title = {Computerized {Analysis} of {Mammogram} {Images} for {Early} {Detection} of {Breast} {Cancer}},
	volume = {10},
	issn = {2227-9032},
	url = {https://www.mdpi.com/2227-9032/10/5/801},
	doi = {10.3390/healthcare10050801},
	abstract = {Breast cancer is widespread worldwide and can be cured if diagnosed early. Using digital mammogram images and image processing with artiﬁcial intelligence can play an essential role in breast cancer diagnosis. As many computerized algorithms for breast cancer diagnosis have signiﬁcant limitations, such as noise handling and varying or low contrast in the images, it can be difﬁcult to segment the abnormal region. These challenges could be overcome by proposing a new preprocessing model, exploring its impact on the post-processing module, and testing it on an extensive database. In this research work, the three-step method is proposed and validated on large databases of mammography images. The ﬁrst step corresponded to the database classiﬁcation, followed by the second step, which removed the pectoral muscle from the mammogram image. The third stage utilized new image-enhancement techniques and a new segmentation module to detect abnormal regions in a well-enhanced image to diagnose breast cancer. The pre-and post-processing modules are based on novel image processing techniques. The proposed method was tested using data collected from different hospitals in the Qassim Health Cluster, Qassim Province, Saudi Arabia. This database contained the ﬁve categories in the Breast Imaging and Reporting and Data System and consisted of 2892 images; the proposed method is analyzed using the publicly available Mammographic Image Analysis Society database, which contained 322 images. The proposed method gives good contrast enhancement with peak-signal to noise ratio improvement of 3 dB. The proposed method provides an accuracy of approximately 92\% on 2892 images of Qassim Health Cluster, Qassim Province, Saudi Arabia. The proposed method gives approximately 97\% on the Mammographic Image Analysis Society database. The novelty of the proposed work is that it could work on all Breast Imaging and Reporting and Data System categories. The performance of the proposed method demonstrated its ability to improve the diagnostic performance of the computerized breast cancer detection method.},
	language = {en},
	number = {5},
	urldate = {2023-05-24},
	journal = {Healthcare},
	author = {Almalki, Yassir Edrees and Soomro, Toufique Ahmed and Irfan, Muhammad and Alduraibi, Sharifa Khalid and Ali, Ahmed},
	month = apr,
	year = {2022},
	pages = {801},
	file = {Almalki et al. - 2022 - Computerized Analysis of Mammogram Images for Earl.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\G2RLBZII\\Almalki et al. - 2022 - Computerized Analysis of Mammogram Images for Earl.pdf:application/pdf},
}

@article{budaDataSetDeep2021,
	title = {A {Data} {Set} and {Deep} {Learning} {Algorithm} for the {Detection} of {Masses} and {Architectural} {Distortions} in {Digital} {Breast} {Tomosynthesis} {Images}},
	volume = {4},
	issn = {2574-3805},
	url = {https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2783046},
	doi = {10.1001/jamanetworkopen.2021.19100},
	abstract = {OBJECTIVES To curate, annotate, and make publicly available a large-scale data set of digital breast tomosynthesis (DBT) images to facilitate the development and evaluation of artificial intelligence algorithms for breast cancer screening; to develop a baseline deep learning model for breast cancer detection; and to test this model using the data set to serve as a baseline for future research. DESIGN, SETTING, AND PARTICIPANTS In this diagnostic study, 16 802 DBT examinations with at least 1 reconstruction view available, performed between August 26, 2014, and January 29, 2018, were obtained from Duke Health System and analyzed. From the initial cohort, examinations were divided into 4 groups and split into training and test sets for the development and evaluation of a deep learning model. Images with foreign objects or spot compression views were excluded. Data analysis was conducted from January 2018 to October 2020. EXPOSURES Screening DBT. MAIN OUTCOMES AND MEASURES The detection algorithm was evaluated with breast-based free-response receiver operating characteristic curve and sensitivity at 2 false positives per volume.
RESULTS The curated data set contained 22 032 reconstructed DBT volumes that belonged to 5610 studies from 5060 patients with a mean (SD) age of 55 (11) years and 5059 (100.0\%) women. This included 4 groups of studies: (1) 5129 (91.4\%) normal studies; (2) 280 (5.0\%) actionable studies, for which where additional imaging was needed but no biopsy was performed; (3) 112 (2.0\%) benign biopsied studies; and (4) 89 studies (1.6\%) with cancer. Our data set included masses and architectural distortions that were annotated by 2 experienced radiologists. Our deep learning model reached breast-based sensitivity of 65\% (39 of 60; 95\% CI, 56\%-74\%) at 2 false positives per DBT volume on a test set of 460 examinations from 418 patients.
CONCLUSIONS AND RELEVANCE The large, diverse, and curated data set presented in this study could facilitate the development and evaluation of artificial intelligence algorithms for breast cancer screening by providing data for training as well as a common set of cases for model validation. The performance of the model developed in this study showed that the task remains challenging; its performance could serve as a baseline for future model development.},
	language = {en},
	number = {8},
	urldate = {2023-05-24},
	journal = {JAMA Network Open},
	author = {Buda, Mateusz and Saha, Ashirbani and Walsh, Ruth and Ghate, Sujata and Li, Nianyi and Swiecicki, Albert and Lo, Joseph Y. and Mazurowski, Maciej A.},
	month = aug,
	year = {2021},
	pages = {e2119100},
	file = {Buda et al. - 2021 - A Data Set and Deep Learning Algorithm for the Det.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\W8DEZGIN\\Buda et al. - 2021 - A Data Set and Deep Learning Algorithm for the Det.pdf:application/pdf},
}

@article{bahlArtificialIntelligencePrimer2020,
	title = {Artificial {Intelligence}: {A} {Primer} for {Breast} {Imaging} {Radiologists}},
	volume = {2},
	issn = {2631-6110, 2631-6129},
	shorttitle = {Artificial {Intelligence}},
	url = {https://academic.oup.com/jbi/article/2/4/304/5859939},
	doi = {10.1093/jbi/wbaa033},
	abstract = {Artificial intelligence (AI) is a branch of computer science dedicated to developing computer algorithms that emulate intelligent human behavior. Subfields of AI include machine learning and deep learning. Advances in AI technologies have led to techniques that could increase breast cancer detection, improve clinical efficiency in breast imaging practices, and guide decision-making regarding screening and prevention strategies. This article reviews key terminology and concepts, discusses common AI models and methods to validate and evaluate these models, describes emerging AI applications in breast imaging, and outlines challenges and future directions. Familiarity with AI terminology, concepts, methods, and applications is essential for breast imaging radiologists to critically evaluate these emerging technologies, recognize their strengths and limitations, and ultimately ensure optimal patient care.},
	language = {en},
	number = {4},
	urldate = {2023-05-24},
	journal = {Journal of Breast Imaging},
	author = {Bahl, Manisha},
	month = aug,
	year = {2020},
	pages = {304--314},
	file = {Bahl - 2020 - Artificial Intelligence A Primer for Breast Imagi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RPIZH6PV\\Bahl - 2020 - Artificial Intelligence A Primer for Breast Imagi.pdf:application/pdf},
}

@article{rangayyanReviewComputeraidedDiagnosis2007,
	title = {A review of computer-aided diagnosis of breast cancer: {Toward} the detection of subtle signs},
	volume = {344},
	issn = {00160032},
	shorttitle = {A review of computer-aided diagnosis of breast cancer},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S001600320600127X},
	doi = {10.1016/j.jfranklin.2006.09.003},
	abstract = {Mammography is the best available tool for screening for the early detection of breast cancer. Mammographic screening has been shown to be effective in reducing breast cancer mortality rates: screening programs have reduced mortality rates by 30–70\%.},
	language = {en},
	number = {3-4},
	urldate = {2023-05-24},
	journal = {Journal of the Franklin Institute},
	author = {Rangayyan, Rangaraj M. and Ayres, Fábio J. and Leo Desautels, J.E.},
	month = may,
	year = {2007},
	pages = {312--348},
	file = {Rangayyan et al. - 2007 - A review of computer-aided diagnosis of breast can.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9TK4K3RP\\Rangayyan et al. - 2007 - A review of computer-aided diagnosis of breast can.pdf:application/pdf},
}

@article{abdelhafizDeepConvolutionalNeural2019,
	title = {Deep convolutional neural networks for mammography: advances, challenges and applications},
	volume = {20},
	issn = {1471-2105},
	shorttitle = {Deep convolutional neural networks for mammography},
	url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2823-4},
	doi = {10.1186/s12859-019-2823-4},
	abstract = {Background: The limitations of traditional computer-aided detection (CAD) systems for mammography, the extreme importance of early detection of breast cancer and the high impact of the false diagnosis of patients drive researchers to investigate deep learning (DL) methods for mammograms (MGs). Recent breakthroughs in DL, in particular, convolutional neural networks (CNNs) have achieved remarkable advances in the medical fields. Specifically, CNNs are used in mammography for lesion localization and detection, risk assessment, image retrieval, and classification tasks. CNNs also help radiologists providing more accurate diagnosis by delivering precise quantitative analysis of suspicious lesions.
Results: In this survey, we conducted a detailed review of the strengths, limitations, and performance of the most recent CNNs applications in analyzing MG images. It summarizes 83 research studies for applying CNNs on various tasks in mammography. It focuses on finding the best practices used in these research studies to improve the diagnosis accuracy. This survey also provides a deep insight into the architecture of CNNs used for various tasks. Furthermore, it describes the most common publicly available MG repositories and highlights their main features and strengths.
Conclusions: The mammography research community can utilize this survey as a basis for their current and future studies. The given comparison among common publicly available MG repositories guides the community to select the most appropriate database for their application(s). Moreover, this survey lists the best practices that improve the performance of CNNs including the pre-processing of images and the use of multi-view images. In addition, other listed techniques like transfer learning (TL), data augmentation, batch normalization, and dropout are appealing solutions to reduce overfitting and increase the generalization of the CNN models. Finally, this survey identifies the research challenges and directions that require further investigations by the community.},
	language = {en},
	number = {S11},
	urldate = {2023-05-24},
	journal = {BMC Bioinformatics},
	author = {Abdelhafiz, Dina and Yang, Clifford and Ammar, Reda and Nabavi, Sheida},
	month = jun,
	year = {2019},
	pages = {281},
	file = {Abdelhafiz et al. - 2019 - Deep convolutional neural networks for mammography.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PBRN29KB\\Abdelhafiz et al. - 2019 - Deep convolutional neural networks for mammography.pdf:application/pdf},
}

@article{abdelrahmanConvolutionalNeuralNetworks2021,
	title = {Convolutional neural networks for breast cancer detection in mammography: {A} survey},
	volume = {131},
	issn = {00104825},
	shorttitle = {Convolutional neural networks for breast cancer detection in mammography},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010482521000421},
	doi = {10.1016/j.compbiomed.2021.104248},
	abstract = {Despite its proven record as a breast cancer screening tool, mammography remains labor-intensive and has recognized limitations, including low sensitivity in women with dense breast tissue. In the last ten years, Neural Network advances have been applied to mammography to help radiologists increase their efficiency and accu­ racy. This survey aims to present, in an organized and structured manner, the current knowledge base of con­ volutional neural networks (CNNs) in mammography. The survey first discusses traditional Computer Assisted Detection (CAD) and more recently developed CNN-based models for computer vision in mammography. It then presents and discusses the literature on available mammography training datasets. The survey then presents and discusses current literature on CNNs for four distinct mammography tasks: (1) breast density classification, (2) breast asymmetry detection and classification, (3) calcification detection and classification, and (4) mass detection and classification, including presenting and comparing the reported quantitative results for each task and the pros and cons of the different CNN-based approaches. Then, it offers real-world applications of CNN CAD algorithms by discussing current Food and Drug Administration (FDA) approved models. Finally, this survey highlights the potential opportunities for future work in this field. The material presented and discussed in this survey could serve as a road map for developing CNN-based solutions to improve mammographic detection of breast cancer further.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Computers in Biology and Medicine},
	author = {Abdelrahman, Leila and Al Ghamdi, Manal and Collado-Mesa, Fernando and Abdel-Mottaleb, Mohamed},
	month = apr,
	year = {2021},
	pages = {104248},
	file = {Abdelrahman et al. - 2021 - Convolutional neural networks for breast cancer de.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7T3QAGIA\\Abdelrahman et al. - 2021 - Convolutional neural networks for breast cancer de.pdf:application/pdf},
}

@article{michaelBreastCancerSegmentation2021,
	title = {Breast {Cancer} {Segmentation} {Methods}: {Current} {Status} and {Future} {Potentials}},
	volume = {2021},
	issn = {2314-6141, 2314-6133},
	shorttitle = {Breast {Cancer} {Segmentation} {Methods}},
	url = {https://www.hindawi.com/journals/bmri/2021/9962109/},
	doi = {10.1155/2021/9962109},
	abstract = {Early breast cancer detection is one of the most important issues that need to be addressed worldwide as it can help increase the survival rate of patients. Mammograms have been used to detect breast cancer in the early stages; if detected in the early stages, it can drastically reduce treatment costs. The detection of tumours in the breast depends on segmentation techniques. Segmentation plays a significant role in image analysis and includes detection, feature extraction, classification, and treatment. Segmentation helps physicians quantify the volume of tissue in the breast for treatment planning. In this work, we have grouped segmentation methods into three groups: classical segmentation that includes region-, threshold-, and edge-based segmentation; machine learning segmentation; and supervised and unsupervised and deep learning segmentation. The findings of our study revealed that region-based segmentation is frequently used for classical methods, and the most frequently used techniques are region growing. Further, a median filter is a robust tool for removing noise. Moreover, the MIAS database is frequently used in classical segmentation methods. Meanwhile, in machine learning segmentation, unsupervised machine learning methods are more frequently used, and U-Net is frequently used for mammogram image segmentation because it does not require many annotated images compared with other deep learning models. Furthermore, reviewed papers revealed that it is possible to train a deep learning model without performing any preprocessing or postprocessing and also showed that the U-Net model is frequently used for mammogram segmentation. The U-Net model is frequently used because it does not require many annotated images and also because of the presence of high-performance GPU computing, which makes it easy to train networks with more layers. Additionally, we identified mammograms and utilised widely used databases, wherein 3 and 28 are public and private databases, respectively.},
	language = {en},
	urldate = {2023-05-24},
	journal = {BioMed Research International},
	author = {Michael, Epimack and Ma, He and Li, Hong and Kulwa, Frank and Li, Jing},
	editor = {Harrison, Paul},
	month = jul,
	year = {2021},
	pages = {1--29},
	file = {Michael et al. - 2021 - Breast Cancer Segmentation Methods Current Status.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2JGQI7PW\\Michael et al. - 2021 - Breast Cancer Segmentation Methods Current Status.pdf:application/pdf},
}

@article{debeleeSurveyDeepLearning2020,
	title = {Survey of deep learning in breast cancer image analysis},
	volume = {11},
	issn = {1868-6478, 1868-6486},
	url = {http://link.springer.com/10.1007/s12530-019-09297-2},
	doi = {10.1007/s12530-019-09297-2},
	abstract = {Computer-aided image analysis for better understanding of images has been time-honored approaches in the medical computing field. In the conventional machine learning approach, the domain experts in medical images are mandatory for image annotation that subsequently to be used for feature engineering. However, in deep learning, a big jump has been made to help the researchers do segmentation, feature extraction, classification, and detection from raw medical images obtained using digital breast tomosynthesis, digital mammography, magnetic resonance imaging, and ultrasound imaging modalities. As a result, deep learning (DL) has gained a state-of-the-art in many application areas, for example, breast cancer image analysis. In this survey paper, we reviewed the most common breast cancer imaging modalities, public, most cited and recently updated breast cancer databases, histopathological based breast cancer image analysis, and DL application types in medical image analysis. We finally conclude by pointing out the research gaps to be addressed in the future.},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {Evolving Systems},
	author = {Debelee, Taye Girma and Schwenker, Friedhelm and Ibenthal, Achim and Yohannes, Dereje},
	month = mar,
	year = {2020},
	pages = {143--163},
	file = {Debelee et al. - 2020 - Survey of deep learning in breast cancer image ana.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IJVWXVK6\\Debelee et al. - 2020 - Survey of deep learning in breast cancer image ana.pdf:application/pdf},
}

@article{caoAutoDenseUNetSearchableNeural2022,
	title = {Auto-{DenseUNet}: {Searchable} neural network architecture for mass segmentation in {3D} automated breast ultrasound},
	volume = {82},
	issn = {13618415},
	shorttitle = {Auto-{DenseUNet}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841522002250},
	doi = {10.1016/j.media.2022.102589},
	abstract = {Accurate segmentation of breast mass in 3D automated breast ultrasound (ABUS) plays an important role in breast cancer analysis. Deep convolutional networks have become a promising approach in segmenting ABUS images. However, designing an effective network architecture is time-consuming, and highly relies on specialist’s experience and prior knowledge. To address this issue, we introduce a searchable segmentation network (denoted as Auto-DenseUNet) based on the neural architecture search (NAS) to search the optimal architecture automatically for the ABUS mass segmentation task. Concretely, a novel search space is designed based on a densely connected structure to enhance the gradient and information flows throughout the network. Then, to encourage multiscale information fusion, a set of searchable multiscale aggregation nodes between the down-sampling and up-sampling parts of the network are further designed. Thus, all the operators within the dense connection structure or between any two aggregation nodes can be searched to find the optimal structure. Finally, a novel decoupled search training strategy during architecture search is also introduced to alleviate the memory limitation caused by continuous relaxation in NAS. The proposed Auto-DeseUNet method has been evaluated on our ABUS dataset with 170 volumes (from 107 patients), including 120 training volumes and 50 testing volumes split at patient level. Experimental results on testing volumes show that our searched architecture performed better than several human-designed segmentation models on the 3D ABUS mass segmentation task, indicating the effectiveness of our proposed method.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Medical Image Analysis},
	author = {Cao, Xuyang and Chen, Houjin and Li, Yanfeng and Peng, Yahui and Zhou, Yue and Cheng, Lin and Liu, Tianming and Shen, Dinggang},
	month = nov,
	year = {2022},
	pages = {102589},
	file = {Cao et al. - 2022 - Auto-DenseUNet Searchable neural network architec.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TZ9B3XV7\\Cao et al. - 2022 - Auto-DenseUNet Searchable neural network architec.pdf:application/pdf},
}

@inproceedings{zhaoCrossViewAttentionNetwork2020,
	address = {Barcelona, Spain},
	title = {Cross-{View} {Attention} {Network} for {Breast} {Cancer} {Screening} from {Multi}-{View} {Mammograms}},
	isbn = {978-1-5090-6631-5},
	url = {https://ieeexplore.ieee.org/document/9054612/},
	doi = {10.1109/ICASSP40776.2020.9054612},
	abstract = {In this paper, we address the problem of breast caner detection from multi-view mammograms. We present a novel cross-view attention module (CvAM) which implicitly learns to focus on the cancerrelated local abnormal regions and highlighting salient features by exploring cross-view information among four views of a screening mammography exam, e.g. asymmetries between left and right breasts and lesion correspondence between two views of the same breast. More speciﬁcally, the proposed CvAM calculates spatial attention maps based on the same view of different breasts to enhance bilateral asymmetric regions, and channel attention maps based on two different views of the same breast to enhance the feature channels corresponding to the same lesion in a single breast. CvAMs can be easily integrated into standard convolutional neural networks (CNN) architectures such as ResNet to form a multi-view classiﬁcation model. Experiments are conducted on DDSM dataset, and results show that CvAMs can not only provide better classiﬁcation accuracy over non-attention and single-view attention models, but also demonstrate better abnormality localization power using CNN visualization tools.},
	language = {en},
	urldate = {2023-05-24},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Zhao, Xuran and Yu, Luyang and Wang, Xun},
	month = may,
	year = {2020},
	pages = {1050--1054},
	file = {Zhao et al. - 2020 - Cross-View Attention Network for Breast Cancer Scr.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\X98J2JUN\\Zhao et al. - 2020 - Cross-View Attention Network for Breast Cancer Scr.pdf:application/pdf},
}

@article{sunAUNetAttentionguidedDenseupsampling2020,
	title = {{AUNet}: attention-guided dense-upsampling networks for breast mass segmentation in whole mammograms},
	volume = {65},
	issn = {1361-6560},
	shorttitle = {{AUNet}},
	url = {https://iopscience.iop.org/article/10.1088/1361-6560/ab5745},
	doi = {10.1088/1361-6560/ab5745},
	abstract = {Mammography is one of the most commonly applied tools for early breast cancer screening. Automatic segmentation of breast masses in mammograms is essential but challenging due to the low signal-to-noise ratio and the wide variety of mass shapes and sizes. Existing methods deal with these challenges mainly by extracting mass-centered image patches manually or automatically. However, manual patch extraction is time-consuming and automatic patch extraction brings errors that could not be compensated in the following segmentation step. In this study, we propose a novel attention-guided dense-upsampling network (AUNet) for accurate breast mass segmentation in whole mammograms directly. In AUNet, we employ an asymmetrical encoder–decoder structure and propose an effective upsampling block, attention-guided dense-upsampling block (AU block). Especially, the AU block is designed to have three merits. Firstly, it compensates the information loss of bilinear upsampling by dense upsampling. Secondly, it designs a more effective method to fuse high- and low-level features. Thirdly, it includes a channel-attention function to highlight rich-information channels. We evaluated the proposed method on two publicly available datasets, CBIS-DDSM and INbreast. Compared to three state-of-the-art fully convolutional networks, AUNet achieved the best performances with an average Dice similarity coefficient of 81.8\% for CBIS-DDSM and 79.1\% for INbreast.},
	language = {en},
	number = {5},
	urldate = {2023-05-24},
	journal = {Physics in Medicine \& Biology},
	author = {Sun, Hui and Li, Cheng and Liu, Boqiang and Liu, Zaiyi and Wang, Meiyun and Zheng, Hairong and Dagan Feng, David and Wang, Shanshan},
	month = feb,
	year = {2020},
	pages = {055005},
	file = {Sun et al. - 2020 - AUNet attention-guided dense-upsampling networks .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SPUZFYSJ\\Sun et al. - 2020 - AUNet attention-guided dense-upsampling networks .pdf:application/pdf},
}

@article{abdelrahmanConvolutionalNeuralNetworks2021a,
	title = {Convolutional neural networks for breast cancer detection in mammography: {A} survey},
	volume = {131},
	issn = {00104825},
	shorttitle = {Convolutional neural networks for breast cancer detection in mammography},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010482521000421},
	doi = {10.1016/j.compbiomed.2021.104248},
	abstract = {Despite its proven record as a breast cancer screening tool, mammography remains labor-intensive and has recognized limitations, including low sensitivity in women with dense breast tissue. In the last ten years, Neural Network advances have been applied to mammography to help radiologists increase their efficiency and accu­ racy. This survey aims to present, in an organized and structured manner, the current knowledge base of con­ volutional neural networks (CNNs) in mammography. The survey first discusses traditional Computer Assisted Detection (CAD) and more recently developed CNN-based models for computer vision in mammography. It then presents and discusses the literature on available mammography training datasets. The survey then presents and discusses current literature on CNNs for four distinct mammography tasks: (1) breast density classification, (2) breast asymmetry detection and classification, (3) calcification detection and classification, and (4) mass detection and classification, including presenting and comparing the reported quantitative results for each task and the pros and cons of the different CNN-based approaches. Then, it offers real-world applications of CNN CAD algorithms by discussing current Food and Drug Administration (FDA) approved models. Finally, this survey highlights the potential opportunities for future work in this field. The material presented and discussed in this survey could serve as a road map for developing CNN-based solutions to improve mammographic detection of breast cancer further.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Computers in Biology and Medicine},
	author = {Abdelrahman, Leila and Al Ghamdi, Manal and Collado-Mesa, Fernando and Abdel-Mottaleb, Mohamed},
	month = apr,
	year = {2021},
	pages = {104248},
	file = {Abdelrahman et al. - 2021 - Convolutional neural networks for breast cancer de.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\27HTGJYU\\Abdelrahman et al. - 2021 - Convolutional neural networks for breast cancer de.pdf:application/pdf},
}

@article{gerasArtificialIntelligenceMammography2019,
	title = {Artificial {Intelligence} for {Mammography} and {Digital} {Breast} {Tomosynthesis}: {Current} {Concepts} and {Future} {Perspectives}},
	volume = {293},
	issn = {0033-8419, 1527-1315},
	shorttitle = {Artificial {Intelligence} for {Mammography} and {Digital} {Breast} {Tomosynthesis}},
	url = {http://pubs.rsna.org/doi/10.1148/radiol.2019182627},
	doi = {10.1148/radiol.2019182627},
	abstract = {Because of the advances in deep learning, the quality of artificial intelligence is rapidly improving for breast imaging and it will likely play an important role for mammography and digital breast tomosynthesis in all steps—from image generation and denoising to risk prediction, cancer detection, and, ultimately, therapy selection and outcome prediction.},
	language = {en},
	number = {2},
	urldate = {2023-05-24},
	journal = {Radiology},
	author = {Geras, Krzysztof J. and Mann, Ritse M. and Moy, Linda},
	month = nov,
	year = {2019},
	pages = {246--259},
	file = {Geras et al. - 2019 - Artificial Intelligence for Mammography and Digita.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\U7YA26PZ\\Geras et al. - 2019 - Artificial Intelligence for Mammography and Digita.pdf:application/pdf},
}

@article{petriniBreastCancerDiagnosis2022,
	title = {Breast {Cancer} {Diagnosis} in {Two}-{View} {Mammography} {Using} {End}-to-{End} {Trained} {EfficientNet}-{Based} {Convolutional} {Network}},
	volume = {10},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9837037/},
	doi = {10.1109/ACCESS.2022.3193250},
	language = {en},
	urldate = {2023-05-24},
	journal = {IEEE Access},
	author = {Petrini, Daniel G. P. and Shimizu, Carlos and Roela, Rosimeire A. and Valente, Gabriel Vansuita and Folgueira, Maria Aparecida Azevedo Koike and Kim, Hae Yong},
	year = {2022},
	pages = {77723--77731},
	file = {Petrini et al. - 2022 - Breast Cancer Diagnosis in Two-View Mammography Us.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NKRCAFH3\\Petrini et al. - 2022 - Breast Cancer Diagnosis in Two-View Mammography Us.pdf:application/pdf},
}

@article{sechopoulosArtificialIntelligenceBreast2021,
	title = {Artificial intelligence for breast cancer detection in mammography and digital breast tomosynthesis: {State} of the art},
	volume = {72},
	issn = {1044579X},
	shorttitle = {Artificial intelligence for breast cancer detection in mammography and digital breast tomosynthesis},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1044579X20301358},
	doi = {10.1016/j.semcancer.2020.06.002},
	abstract = {Screening for breast cancer with mammography has been introduced in various countries over the last 30 years, initially using analog screen-film-based systems and, over the last 20 years, transitioning to the use of fully digital systems. With the introduction of digitization, the computer interpretation of images has been a subject of intense interest, resulting in the introduction of computer-aided detection (CADe) and diagnosis (CADx) algo­ rithms in the early 2000′s. Although they were introduced with high expectations, the potential improvement in the clinical realm failed to materialize, mostly due to the high number of false positive marks per analyzed image. In the last five years, the artificial intelligence (AI) revolution in computing, driven mostly by deep learning and convolutional neural networks, has also pervaded the field of automated breast cancer detection in digital mammography and digital breast tomosynthesis. Research in this area first involved comparison of its capabil­ ities to that of conventional CADe/CADx methods, which quickly demonstrated the potential of this new tech­ nology. In the last couple of years, more mature and some commercial products have been developed, and studies of their performance compared to that of experienced breast radiologists are showing that these algorithms are on par with human-performance levels in retrospective data sets. Although additional studies, especially pro­ spective evaluations performed in the real screening environment, are needed, it is becoming clear that AI will have an important role in the future breast cancer screening realm. Exactly how this new player will shape this field remains to be determined, but recent studies are already evaluating different options for implementation of this technology.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Seminars in Cancer Biology},
	author = {Sechopoulos, Ioannis and Teuwen, Jonas and Mann, Ritse},
	month = jul,
	year = {2021},
	pages = {214--225},
	file = {Sechopoulos et al. - 2021 - Artificial intelligence for breast cancer detectio.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5RP8A9TX\\Sechopoulos et al. - 2021 - Artificial intelligence for breast cancer detectio.pdf:application/pdf},
}

@article{honjoVisualQuantitativeEvaluation2022,
	title = {Visual and quantitative evaluation of microcalcifications in mammograms with deep learning-based super-resolution},
	volume = {154},
	issn = {0720048X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0720048X22002832},
	doi = {10.1016/j.ejrad.2022.110433},
	abstract = {Purpose: To evaluate visually and quantitatively the performance of a deep-learning-based super-resolution (SR) model for microcalcifications in digital mammography.
Method: Mammograms were consecutively collected from 5080 patients who underwent breast cancer screening from January 2015 to March 2017. Of these, 93 patients (136 breasts, mean age, 50 ± 7 years) had micro­ calcifications in their breasts on mammograms. We applied an artificial intelligence model known as a fast SR convolutional neural network to the mammograms. SR and original mammograms were visually evaluated by four breast radiologists using a 5-point scale (1: original mammograms are strongly preferred, 5: SR mammo­ grams are strongly preferred) for the detection, diagnostic quality, contrast, sharpness, and noise of micro­ calcifications. Mammograms were quantitatively evaluated using a perception-based image-quality evaluator (PIQE).
Results: All radiologists rated the SR mammograms better than the original ones in terms of detection, diagnostic quality, contrast, and sharpness of microcalcifications. These ratings were significantly different according to the Wilcoxon signed-rank test (p {\textless}.001), while the noise score of the three radiologists was significantly lower (p {\textless}.001). According to PIQE, SR mammograms were rated better than the original mammograms, showing a significant difference by paired t-test (p {\textless}.001).
Conclusion: An SR model based on deep learning can improve the visibility of microcalcifications in mammog­ raphy and help detect and diagnose them in mammograms.},
	language = {en},
	urldate = {2023-05-24},
	journal = {European Journal of Radiology},
	author = {Honjo, Takashi and Ueda, Daiju and Katayama, Yutaka and Shimazaki, Akitoshi and Jogo, Atsushi and Kageyama, Ken and Murai, Kazuki and Tatekawa, Hiroyuki and Fukumoto, Shinya and Yamamoto, Akira and Miki, Yukio},
	month = sep,
	year = {2022},
	pages = {110433},
	file = {Honjo et al. - 2022 - Visual and quantitative evaluation of microcalcifi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YBVRCHYZ\\Honjo et al. - 2022 - Visual and quantitative evaluation of microcalcifi.pdf:application/pdf},
}

@article{niuMultiScaleAttention2021,
	title = {Multi‐scale attention‐based convolutional neural network for classification of breast masses in mammograms},
	volume = {48},
	issn = {0094-2405, 2473-4209},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/mp.14942},
	doi = {10.1002/mp.14942},
	abstract = {Purpose: Breast cancer is the cancer with the highest incidence in women, and early detection can effectively improve the survival rate of patients. Mammography is an important method for physicians to screening breast cancer, but the diagnosis of mammograms by physicians depends largely on clinical practice experience. Studies have shown that using computer-aided diagnosis techniques can help doctors diagnose breast cancer.
Methods: In this paper, the method of convolutional neural network is mainly used to classify benign and malignant breast masses in the mammograms. First, we use multi-scale residual networks and densely connected networks as backbone networks to extract the features of global image patches and local image patches. Second, we use the attention module named convolutional block attention module (CBAM) to improve the two feature extraction networks to enhance the network’s feature expression ability. Finally, we fuse the features of multi-scale image patches to achieve the classification of benign and malignant breast masses.
Results: In the digital database for screening mammography (DDSM) database, the accuracy, sensitivity, AUC value and corresponding standard deviation of our method are 0.9626 Æ 0.0110, 0.9719 Æ 0.0126, and 0.9576 Æ 0.0064, respectively. Compared with the commonly used ResNet (AUC = 0.8823 Æ 0.0112) and DenseNet (AUC = 0.9141 Æ 0.0085), the performance of our method has improved. In addition, we also used the INbreast database to train and validate the proposed method. The accuracy, sensitivity, AUC and corresponding standard deviations are 0.9554 Æ 0.0296, 0.9605 Æ 0.0228, and 0.9468 Æ 0.0085, respectively.
Conclusions: Compared with the previous work, our proposed method uses multi-scale image features, has better classification performance in breast mass patches classification tasks, and can effectively assist physicians in breast cancer diagnosis. © 2021 American Association of Physicists in Medicine [https://doi.org/10.1002/mp.14942]},
	language = {en},
	number = {7},
	urldate = {2023-05-24},
	journal = {Medical Physics},
	author = {Niu, Jing and Li, Hua and Zhang, Chen and Li, Dengao},
	month = jul,
	year = {2021},
	pages = {3878--3892},
	file = {Niu et al. - 2021 - Multi‐scale attention‐based convolutional neural n.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7RA8N6ZJ\\Niu et al. - 2021 - Multi‐scale attention‐based convolutional neural n.pdf:application/pdf},
}

@article{haqBTSGANComputeraidedSegmentation2022,
	title = {{BTS}-{GAN}: {Computer}-aided segmentation system for breast tumor using {MRI} and conditional adversarial networks},
	volume = {36},
	issn = {22150986},
	shorttitle = {{BTS}-{GAN}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2215098622000623},
	doi = {10.1016/j.jestch.2022.101154},
	abstract = {Breast tumor is one of the most prominent indicators for the diagnosis of breast cancer. The precise segmentation of tumors is crucial for enhancing the accuracy of breast cancer detection. A physician’s assessment of the MRI scan is time-consuming and require a lot of human effort and expertise. Furthermore, traditional medical segmentation approaches frequently need prior information or manual feature extraction, resulting in a subjective diagnosis. Therefore, the development of an automated image segmentation approach is essential for clinical applications. This work presents BTS-GAN, an automatic breast tumor segmentation process using conditional GAN (cGAN) in Magnetic Resonance Imaging (MRI) scans. First, we used an encoder-decoder deep network with skip connections between encoder and decoder for the generator to increase the localization efﬁciency. Second, we utilized a parallel dilated convolution (PDC) module to retain the features of various sizes of masses and to effectively extract information about the masses’ edges and interior texture. Third, an extra classiﬁcation-related constraint is included to the loss function of the cGAN for mitigating the hard-to-converge challenge in image-toimage (I2I) translation tasks based on classiﬁcation. The generator side of our proposed model learns to detect the tumor and construct a binary mask, while the discriminator learns to distinguish between ground truth and synthetic masks, driving the generator to produce masks as genuine as possible. The experimental results demonstrate that our BTS-GAN is more efﬁcient and reliable for breast tumor segmentation and outperform other segmentation techniques in terms of the IoU and Dice coefﬁcient on the publicly available RIDER breast cancer MRI dataset. Our proposed model achieved an average IoU and Dice scores of 77\% and 85\% respectively.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Engineering Science and Technology, an International Journal},
	author = {Haq, Imran Ul and Ali, Haider and Wang, Hong Yu and Cui, Lei and Feng, Jun},
	month = dec,
	year = {2022},
	pages = {101154},
	file = {Haq et al. - 2022 - BTS-GAN Computer-aided segmentation system for br.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SB4QNGDT\\Haq et al. - 2022 - BTS-GAN Computer-aided segmentation system for br.pdf:application/pdf},
}

@article{wuDeepNeuralNetworks2020,
	title = {Deep {Neural} {Networks} {Improve} {Radiologists}’ {Performance} in {Breast} {Cancer} {Screening}},
	volume = {39},
	issn = {0278-0062, 1558-254X},
	url = {https://ieeexplore.ieee.org/document/8861376/},
	doi = {10.1109/TMI.2019.2945514},
	language = {en},
	number = {4},
	urldate = {2023-05-24},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Wu, Nan and Phang, Jason and Park, Jungkyu and Shen, Yiqiu and Huang, Zhe and Zorin, Masha and Jastrzebski, Stanislaw and Fevry, Thibault and Katsnelson, Joe and Kim, Eric and Wolfson, Stacey and Parikh, Ujas and Gaddam, Sushma and Lin, Leng Leng Young and Ho, Kara and Weinstein, Joshua D. and Reig, Beatriu and Gao, Yiming and Toth, Hildegard and Pysarenko, Kristine and Lewin, Alana and Lee, Jiyon and Airola, Krystal and Mema, Eralda and Chung, Stephanie and Hwang, Esther and Samreen, Naziya and Kim, S. Gene and Heacock, Laura and Moy, Linda and Cho, Kyunghyun and Geras, Krzysztof J.},
	month = apr,
	year = {2020},
	pages = {1184--1194},
	file = {Wu et al. - 2020 - Deep Neural Networks Improve Radiologists’ Perform.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FURKSV4K\\Wu et al. - 2020 - Deep Neural Networks Improve Radiologists’ Perform.pdf:application/pdf},
}

@article{fanMassDetectionSegmentation2020,
	title = {Mass {Detection} and {Segmentation} in {Digital} {Breast} {Tomosynthesis} {Using} {3D}-{Mask} {Region}-{Based} {Convolutional} {Neural} {Network}: {A} {Comparative} {Analysis}},
	volume = {7},
	issn = {2296-889X},
	shorttitle = {Mass {Detection} and {Segmentation} in {Digital} {Breast} {Tomosynthesis} {Using} {3D}-{Mask} {Region}-{Based} {Convolutional} {Neural} {Network}},
	url = {https://www.frontiersin.org/articles/10.3389/fmolb.2020.599333/full},
	doi = {10.3389/fmolb.2020.599333},
	abstract = {Digital breast tomosynthesis (DBT) is an emerging breast cancer screening and diagnostic modality that uses quasi-three-dimensional breast images to provide detailed assessments of the dense tissue within the breast. In this study, a framework of a 3D-Mask region-based convolutional neural network (3D-Mask RCNN) computeraided diagnosis (CAD) system was developed for mass detection and segmentation with a comparative analysis of performance on patient subgroups with different clinicopathological characteristics. To this end, 364 samples of DBT data were used and separated into a training dataset (n = 201) and a testing dataset (n = 163). The detection and segmentation results were evaluated on the testing set and on subgroups of patients with different characteristics, including different age ranges, lesion sizes, histological types, lesion shapes and breast densities. The results of our 3D-Mask RCNN framework were compared with those of the 2D-Mask RCNN and Faster RCNN methods. For lesion-based mass detection, the sensitivity of 3D-Mask RCNN-based CAD was 90\% with 0.8 false positives (FPs) per lesion, whereas the sensitivity of the 2D-Mask RCNN- and Faster RCNN-based CAD was 90\% at 1.3 and 2.37 FPs/lesion, respectively. For breast-based mass detection, the 3D-Mask RCNN generated a sensitivity of 90\% at 0.83 FPs/breast, and this framework is better than the 2D-Mask RCNN and Faster RCNN, which generated a sensitivity of 90\% with 1.24 and 2.38 FPs/breast, respectively. Additionally, the 3D-Mask RCNN achieved signiﬁcantly (p {\textless} 0.05) better performance than the 2D methods on subgroups of samples with characteristics of ages ranged from 40 to 49 years, malignant tumors, spiculate and irregular masses and dense breast, respectively. Lesion segmentation using the 3DMask RCNN achieved an average precision (AP) of 0.934 and a false negative rate (FNR) of 0.053, which are better than those achieved by the 2D methods. The results suggest that the 3D-Mask RCNN CAD framework has advantages over 2D-based mass detection on both the whole data and subgroups with different characteristics.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Frontiers in Molecular Biosciences},
	author = {Fan, Ming and Zheng, Huizhong and Zheng, Shuo and You, Chao and Gu, Yajia and Gao, Xin and Peng, Weijun and Li, Lihua},
	month = nov,
	year = {2020},
	pages = {599333},
	file = {Fan et al. - 2020 - Mass Detection and Segmentation in Digital Breast .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2QM3MD28\\Fan et al. - 2020 - Mass Detection and Segmentation in Digital Breast .pdf:application/pdf},
}

@article{hamidinekooDeepLearningMammography2018,
	title = {Deep learning in mammography and breast histology, an overview and future trends},
	volume = {47},
	issn = {13618415},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841518300902},
	doi = {10.1016/j.media.2018.03.006},
	language = {en},
	urldate = {2023-05-24},
	journal = {Medical Image Analysis},
	author = {Hamidinekoo, Azam and Denton, Erika and Rampun, Andrik and Honnor, Kate and Zwiggelaar, Reyer},
	month = jul,
	year = {2018},
	pages = {45--67},
	file = {Hamidinekoo et al. - 2018 - Deep learning in mammography and breast histology,.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\X5EUC8ZM\\Hamidinekoo et al. - 2018 - Deep learning in mammography and breast histology,.pdf:application/pdf},
}

@article{ganesanComputerAidedBreastCancer2013,
	title = {Computer-{Aided} {Breast} {Cancer} {Detection} {Using} {Mammograms}: {A} {Review}},
	volume = {6},
	issn = {1937-3333, 1941-1189},
	shorttitle = {Computer-{Aided} {Breast} {Cancer} {Detection} {Using} {Mammograms}},
	url = {http://ieeexplore.ieee.org/document/6378398/},
	doi = {10.1109/RBME.2012.2232289},
	abstract = {The American Cancer Society (ACS) recommends women aged 40 and above to have a mammogram every year and calls it a gold standard for breast cancer detection. Early detection of breast cancer can improve survival rates to a great extent. Inter-observer and intra-observer errors occur frequently in analysis of medical images, given the high variability between interpretations of different radiologists. Also, the sensitivity of mammographic screening varies with image quality and expertise of the radiologist. So, there is no golden standard for the screening process. To offset this variability and to standardize the diagnostic procedures, efforts are being made to develop automated techniques for diagnosis and grading of breast cancer images. A few papers have documented the general trend of computer-aided diagnosis of breast cancer, making a broad study of the several techniques involved. But, there is no deﬁnitive documentation focusing on the mathematical techniques used in breast cancer detection. This review aims at providing an overview about recent advances and developments in the ﬁeld of Computer-Aided Diagnosis (CAD) of breast cancer using mammograms, speciﬁcally focusing on the mathematical aspects of the same, aiming to act as a mathematical primer for intermediates and experts in the ﬁeld.},
	language = {en},
	urldate = {2023-05-24},
	journal = {IEEE Reviews in Biomedical Engineering},
	author = {Ganesan, Karthikeyan and Acharya, U. Rajendra and Chua, Chua Kuang and Min, Lim Choo and Abraham, K. Thomas and Ng, Kwan-Hoong},
	year = {2013},
	pages = {77--98},
	file = {Ganesan et al. - 2013 - Computer-Aided Breast Cancer Detection Using Mammo.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\C9AFU4GP\\Ganesan et al. - 2013 - Computer-Aided Breast Cancer Detection Using Mammo.pdf:application/pdf},
}

@article{mustraReviewRecentAdvances2016,
	title = {Review of recent advances in segmentation of the breast boundary and the pectoral muscle in mammograms},
	volume = {54},
	issn = {0140-0118, 1741-0444},
	url = {http://link.springer.com/10.1007/s11517-015-1411-7},
	doi = {10.1007/s11517-015-1411-7},
	abstract = {This paper presents a review of recent advances in the development of methods for segmentation of the breast boundary and the pectoral muscle in mammograms. Regardless of improvement of imaging technology, accurate segmentation of the breast boundary and detection of the pectoral muscle are still challenging tasks for image processing algorithms. In this paper, we discuss problems related to mammographic image preprocessing and accurate segmentation. We review specific methods that were commonly used in most of the techniques proposed for segmentation of mammograms and discuss their advantages and disadvantages. Comparative analysis of the methods reported on is made difficult by variations in the datasets and procedures of evaluation used by the authors. We attempt to overcome some of these limitations by trying to compare methods which used the same dataset and have some similarities in approaches to the breast boundary segmentation and detection of the pectoral muscle. In this paper, we will address the most often used methods for segmentation such as thresholding, morphology, region growing, active contours, and wavelet filtering. These methods, or their combinations, are the ones most used in the last decade by the majority of work published in this image processing domain.},
	language = {en},
	number = {7},
	urldate = {2023-05-24},
	journal = {Medical \& Biological Engineering \& Computing},
	author = {Mustra, Mario and Grgic, Mislav and Rangayyan, Rangaraj M.},
	month = jul,
	year = {2016},
	pages = {1003--1024},
	file = {Mustra et al. - 2016 - Review of recent advances in segmentation of the b.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZK78MWKW\\Mustra et al. - 2016 - Review of recent advances in segmentation of the b.pdf:application/pdf},
}

@article{pedroMassClassificationMammograms2019,
	title = {Is mass classification in mammograms a solved problem? - {A} critical review over the last 20 years},
	volume = {119},
	issn = {09574174},
	shorttitle = {Is mass classification in mammograms a solved problem?},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417418306821},
	doi = {10.1016/j.eswa.2018.10.032},
	language = {en},
	urldate = {2023-05-24},
	journal = {Expert Systems with Applications},
	author = {Pedro, Ricardo Wandré Dias and Machado-Lima, Ariane and Nunes, Fátima L.S.},
	month = apr,
	year = {2019},
	pages = {90--103},
	file = {Pedro et al. - 2019 - Is mass classification in mammograms a solved prob.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VA4UR4MQ\\Pedro et al. - 2019 - Is mass classification in mammograms a solved prob.pdf:application/pdf},
}

@article{shenDeepLearningImprove2019,
	title = {Deep {Learning} to {Improve} {Breast} {Cancer} {Detection} on {Screening} {Mammography}},
	volume = {9},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-019-48995-4},
	doi = {10.1038/s41598-019-48995-4},
	abstract = {Abstract
            
              The rapid development of deep learning, a family of machine learning techniques, has spurred much interest in its application to medical imaging problems. Here, we develop a deep learning algorithm that can accurately detect breast cancer on screening mammograms using an “end-to-end” training approach that efficiently leverages training datasets with either complete clinical annotation or only the cancer status (label) of the whole image. In this approach, lesion annotations are required only in the initial training stage, and subsequent stages require only image-level labels, eliminating the reliance on rarely available lesion annotations. Our all convolutional network method for classifying screening mammograms attained excellent performance in comparison with previous methods. On an independent test set of digitized film mammograms from the Digital Database for Screening Mammography (CBIS-DDSM), the best single model achieved a per-image AUC of 0.88, and four-model averaging improved the AUC to 0.91 (sensitivity: 86.1\%, specificity: 80.1\%). On an independent test set of full-field digital mammography (FFDM) images from the INbreast database, the best single model achieved a per-image AUC of 0.95, and four-model averaging improved the AUC to 0.98 (sensitivity: 86.7\%, specificity: 96.1\%). We also demonstrate that a whole image classifier trained using our end-to-end approach on the CBIS-DDSM digitized film mammograms can be transferred to INbreast FFDM images using only a subset of the INbreast data for fine-tuning and without further reliance on the availability of lesion annotations. These findings show that automatic deep learning methods can be readily trained to attain high accuracy on heterogeneous mammography platforms, and hold tremendous promise for improving clinical tools to reduce false positive and false negative screening mammography results. Code and model available at:
              https://github.com/lishen/end2end-all-conv
              .},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {Scientific Reports},
	author = {Shen, Li and Margolies, Laurie R. and Rothstein, Joseph H. and Fluder, Eugene and McBride, Russell and Sieh, Weiva},
	month = aug,
	year = {2019},
	pages = {12495},
	file = {Shen et al. - 2019 - Deep Learning to Improve Breast Cancer Detection o.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\23RH3TBJ\\Shen et al. - 2019 - Deep Learning to Improve Breast Cancer Detection o.pdf:application/pdf},
}

@article{moghbelReviewBreastBoundary2020,
	title = {A review of breast boundary and pectoral muscle segmentation methods in computer-aided detection/diagnosis of breast mammography},
	volume = {53},
	issn = {0269-2821, 1573-7462},
	url = {http://link.springer.com/10.1007/s10462-019-09721-8},
	doi = {10.1007/s10462-019-09721-8},
	abstract = {Mammography can be considered as the current gold standard for detecting early signs of breast cancer and is in wide use throughout the world. As conﬁrmed by many studies, breast cancer screening using mammography can reduce breast cancer-related mortality by 30–70\%. However, although the interpretation of mammography images by a second reader has been shown to increase the cancer detection rate, this practice is not widespread due to the cost associated. As a result, computer-aided detection/diagnosis (CAD) of breast mammography has been gaining popularity with various studies illustrating the positive effects of using computers in detecting early breast cancer signs by providing the radiologists with a second opinion with most of these CAD systems requiring the breast outline and pectoral muscle regions (in images acquired using Medio-Lateral-Oblique view) to be segmented from mammograms prior to the classiﬁcation. This paper discusses recent developments and methods proposed for segmenting the breast and pectoral muscle regions and compares the performance and shortcomings of different approaches grouped together based on the techniques used. While it is arduous to compare these methods using comparative analysis, a set of common performance evaluation criterion is deﬁned in this study and various methods are compared based on their methodology and the validation dataset used. Although many methods can achieve promising results, there is still room for further development, especially in pre-processing and image enhancement steps where most methods do not take the necessary steps for ensuring a smooth segmentation of boundaries. In this paper, the most effective pre-processing, image enhancement and segmentation concepts proposed for breast boundary and pectoral muscle segmentation are identiﬁed and discussed in hopes of aiding the readers with identifying the best possible solutions for these segmentation problems.},
	language = {en},
	number = {3},
	urldate = {2023-05-24},
	journal = {Artificial Intelligence Review},
	author = {Moghbel, Mehrdad and Ooi, Chia Yee and Ismail, Nordinah and Hau, Yuan Wen and Memari, Nogol},
	month = mar,
	year = {2020},
	pages = {1873--1918},
	file = {Moghbel et al. - 2020 - A review of breast boundary and pectoral muscle se.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NV4WK9H4\\Moghbel et al. - 2020 - A review of breast boundary and pectoral muscle se.pdf:application/pdf},
}

@article{gerasArtificialIntelligenceMammography2019a,
	title = {Artificial {Intelligence} for {Mammography} and {Digital} {Breast} {Tomosynthesis}: {Current} {Concepts} and {Future} {Perspectives}},
	volume = {293},
	issn = {0033-8419, 1527-1315},
	shorttitle = {Artificial {Intelligence} for {Mammography} and {Digital} {Breast} {Tomosynthesis}},
	url = {http://pubs.rsna.org/doi/10.1148/radiol.2019182627},
	doi = {10.1148/radiol.2019182627},
	abstract = {Because of the advances in deep learning, the quality of artificial intelligence is rapidly improving for breast imaging and it will likely play an important role for mammography and digital breast tomosynthesis in all steps—from image generation and denoising to risk prediction, cancer detection, and, ultimately, therapy selection and outcome prediction.},
	language = {en},
	number = {2},
	urldate = {2023-05-24},
	journal = {Radiology},
	author = {Geras, Krzysztof J. and Mann, Ritse M. and Moy, Linda},
	month = nov,
	year = {2019},
	pages = {246--259},
	file = {Geras et al. - 2019 - Artificial Intelligence for Mammography and Digita.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\G37ILUXE\\Geras et al. - 2019 - Artificial Intelligence for Mammography and Digita.pdf:application/pdf},
}

@article{mridhaComprehensiveSurveyDeepLearningBased2021,
	title = {A {Comprehensive} {Survey} on {Deep}-{Learning}-{Based} {Breast} {Cancer} {Diagnosis}},
	volume = {13},
	issn = {2072-6694},
	url = {https://www.mdpi.com/2072-6694/13/23/6116},
	doi = {10.3390/cancers13236116},
	abstract = {Breast cancer is now the most frequently diagnosed cancer in women, and its percentage is gradually increasing. Optimistically, there is a good chance of recovery from breast cancer if identiﬁed and treated at an early stage. Therefore, several researchers have established deep-learningbased automated methods for their efﬁciency and accuracy in predicting the growth of cancer cells utilizing medical imaging modalities. As of yet, few review studies on breast cancer diagnosis are available that summarize some existing studies. However, these studies were unable to address emerging architectures and modalities in breast cancer diagnosis. This review focuses on the evolving architectures of deep learning for breast cancer detection. In what follows, this survey presents existing deep-learning-based architectures, analyzes the strengths and limitations of the existing studies, examines the used datasets, and reviews image pre-processing techniques. Furthermore, a concrete review of diverse imaging modalities, performance metrics and results, challenges, and research directions for future researchers is presented.},
	language = {en},
	number = {23},
	urldate = {2023-05-24},
	journal = {Cancers},
	author = {Mridha, Muhammad Firoz and Hamid, Md. Abdul and Monowar, Muhammad Mostafa and Keya, Ashfia Jannat and Ohi, Abu Quwsar and Islam, Md. Rashedul and Kim, Jong-Myon},
	month = dec,
	year = {2021},
	pages = {6116},
	file = {Mridha et al. - 2021 - A Comprehensive Survey on Deep-Learning-Based Brea.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2H866XZ5\\Mridha et al. - 2021 - A Comprehensive Survey on Deep-Learning-Based Brea.pdf:application/pdf},
}

@article{zebariSystematicReviewComputing2021,
	title = {Systematic {Review} of {Computing} {Approaches} for {Breast} {Cancer} {Detection} {Based} {Computer} {Aided} {Diagnosis} {Using} {Mammogram} {Images}},
	volume = {35},
	issn = {0883-9514, 1087-6545},
	url = {https://www.tandfonline.com/doi/full/10.1080/08839514.2021.2001177},
	doi = {10.1080/08839514.2021.2001177},
	abstract = {Breast cancer is one of the most prevalent types of cancer that plagues females. Mortality from breast cancer could be reduced by diagnosing and identifying it at an early stage. To detect breast cancer, various imaging modalities can be used, such as mammography. Computer-Aided Detection/Diagnosis (CAD) systems can assist an expert radiologist to diagnose breast cancer at an early stage. This paper introduces the findings of a systematic review that seeks to examine the state-of-the-art CAD systems for breast cancer detection. This review is based on 118 publications published in 2018–2021 and retrieved from major scientific publication databases while using a rigorous methodology of a systematic review. We provide a general description and analysis of existing CAD systems that use machine learning methods as well as their current state based on mammogram image modalities and classification methods. This systematic review presents all stages of CAD including preprocessing, segmentation, feature extraction, feature selection, and classification. We identify research gaps and outline recom­ mendations for future research. This systematic review may be helpful for both clinicians, who use CAD systems for early diag­ nosis of breast cancer, as well as for researchers to find knowl­ edge gaps and create more contributions for breast cancer diagnostics.},
	language = {en},
	number = {15},
	urldate = {2023-05-24},
	journal = {Applied Artificial Intelligence},
	author = {Zebari, Dilovan Asaad and Ibrahim, Dheyaa Ahmed and Zeebaree, Diyar Qader and Haron, Habibollah and Salih, Merdin Shamal and Damaševičius, Robertas and Mohammed, Mazin Abed},
	month = dec,
	year = {2021},
	pages = {2157--2203},
	file = {Zebari et al. - 2021 - Systematic Review of Computing Approaches for Brea.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IS6ZZFHE\\Zebari et al. - 2021 - Systematic Review of Computing Approaches for Brea.pdf:application/pdf},
}

@article{ozaBottomUpReviewImage2021,
	title = {A {Bottom}-{Up} {Review} of {Image} {Analysis} {Methods} for {Suspicious} {Region} {Detection} in {Mammograms}},
	volume = {7},
	issn = {2313-433X},
	url = {https://www.mdpi.com/2313-433X/7/9/190},
	doi = {10.3390/jimaging7090190},
	abstract = {Breast cancer is one of the most common death causes amongst women all over the world. Early detection of breast cancer plays a critical role in increasing the survival rate. Various imaging modalities, such as mammography, breast MRI, ultrasound and thermography, are used to detect breast cancer. Though there is a considerable success with mammography in biomedical imaging, detecting suspicious areas remains a challenge because, due to the manual examination and variations in shape, size, other mass morphological features, mammography accuracy changes with the density of the breast. Furthermore, going through the analysis of many mammograms per day can be a tedious task for radiologists and practitioners. One of the main objectives of biomedical imaging is to provide radiologists and practitioners with tools to help them identify all suspicious regions in a given image. Computer-aided mass detection in mammograms can serve as a second opinion tool to help radiologists avoid running into oversight errors. The scientiﬁc community has made much progress in this topic, and several approaches have been proposed along the way. Following a bottom-up narrative, this paper surveys different scientiﬁc methodologies and techniques to detect suspicious regions in mammograms spanning from methods based on low-level image features to the most recent novelties in AI-based approaches. Both theoretical and practical grounds are provided across the paper sections to highlight the pros and cons of different methodologies. The paper’s main scope is to let readers embark on a journey through a fully comprehensive description of techniques, strategies and datasets on the topic.},
	language = {en},
	number = {9},
	urldate = {2023-05-24},
	journal = {Journal of Imaging},
	author = {Oza, Parita and Sharma, Paawan and Patel, Samir and Bruno, Alessandro},
	month = sep,
	year = {2021},
	pages = {190},
	file = {Oza et al. - 2021 - A Bottom-Up Review of Image Analysis Methods for S.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2YRID3X8\\Oza et al. - 2021 - A Bottom-Up Review of Image Analysis Methods for S.pdf:application/pdf},
}

@article{mendesBreastCancerRisk2021,
	title = {Breast {Cancer} {Risk} {Assessment}: {A} {Review} on {Mammography}-{Based} {Approaches}},
	volume = {7},
	issn = {2313-433X},
	shorttitle = {Breast {Cancer} {Risk} {Assessment}},
	url = {https://www.mdpi.com/2313-433X/7/6/98},
	doi = {10.3390/jimaging7060098},
	abstract = {Breast cancer affects thousands of women across the world, every year. Methods to predict risk of breast cancer, or to stratify women in different risk levels, could help to achieve an early diagnosis, and consequently a reduction of mortality. This paper aims to review articles that extracted texture features from mammograms and used those features along with machine learning algorithms to assess breast cancer risk. Besides that, deep learning methodologies that aimed for the same goal were also reviewed. In this work, ﬁrst, a brief introduction to breast cancer statistics and screening programs is presented; after that, research done in the ﬁeld of breast cancer risk assessment are analyzed, in terms of both methodologies used and results obtained. Finally, considerations about the analyzed papers are conducted. The results of this review allow to conclude that both machine and deep learning methodologies provide promising results in the ﬁeld of risk analysis, either in a stratiﬁcation in risk groups, or in a prediction of a risk score. Although promising, future endeavors in this ﬁeld should consider the possibility of the implementation of the methodology in clinical practice.},
	language = {en},
	number = {6},
	urldate = {2023-05-24},
	journal = {Journal of Imaging},
	author = {Mendes, João and Matela, Nuno},
	month = jun,
	year = {2021},
	pages = {98},
	file = {Mendes and Matela - 2021 - Breast Cancer Risk Assessment A Review on Mammogr.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QM4PDZNN\\Mendes and Matela - 2021 - Breast Cancer Risk Assessment A Review on Mammogr.pdf:application/pdf},
}

@article{yuSystematicSurveyDeep2022,
	title = {A systematic survey of deep learning in breast cancer},
	volume = {37},
	issn = {0884-8173, 1098-111X},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/int.22622},
	doi = {10.1002/int.22622},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {International Journal of Intelligent Systems},
	author = {Yu, Xiang and Zhou, Qinghua and Wang, Shuihua and Zhang, Yu‐Dong},
	month = jan,
	year = {2022},
	pages = {152--216},
	file = {Yu et al. - 2022 - A systematic survey of deep learning in breast can.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VBG9EGCX\\Yu et al. - 2022 - A systematic survey of deep learning in breast can.pdf:application/pdf},
}

@article{sechopoulosArtificialIntelligenceBreast2021a,
	title = {Artificial intelligence for breast cancer detection in mammography and digital breast tomosynthesis: {State} of the art},
	volume = {72},
	issn = {1044579X},
	shorttitle = {Artificial intelligence for breast cancer detection in mammography and digital breast tomosynthesis},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1044579X20301358},
	doi = {10.1016/j.semcancer.2020.06.002},
	abstract = {Screening for breast cancer with mammography has been introduced in various countries over the last 30 years, initially using analog screen-film-based systems and, over the last 20 years, transitioning to the use of fully digital systems. With the introduction of digitization, the computer interpretation of images has been a subject of intense interest, resulting in the introduction of computer-aided detection (CADe) and diagnosis (CADx) algo­ rithms in the early 2000′s. Although they were introduced with high expectations, the potential improvement in the clinical realm failed to materialize, mostly due to the high number of false positive marks per analyzed image. In the last five years, the artificial intelligence (AI) revolution in computing, driven mostly by deep learning and convolutional neural networks, has also pervaded the field of automated breast cancer detection in digital mammography and digital breast tomosynthesis. Research in this area first involved comparison of its capabil­ ities to that of conventional CADe/CADx methods, which quickly demonstrated the potential of this new tech­ nology. In the last couple of years, more mature and some commercial products have been developed, and studies of their performance compared to that of experienced breast radiologists are showing that these algorithms are on par with human-performance levels in retrospective data sets. Although additional studies, especially pro­ spective evaluations performed in the real screening environment, are needed, it is becoming clear that AI will have an important role in the future breast cancer screening realm. Exactly how this new player will shape this field remains to be determined, but recent studies are already evaluating different options for implementation of this technology.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Seminars in Cancer Biology},
	author = {Sechopoulos, Ioannis and Teuwen, Jonas and Mann, Ritse},
	month = jul,
	year = {2021},
	pages = {214--225},
	file = {Sechopoulos et al. - 2021 - Artificial intelligence for breast cancer detectio.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4YT5VFHL\\Sechopoulos et al. - 2021 - Artificial intelligence for breast cancer detectio.pdf:application/pdf},
}

@article{zebariSystematicReviewComputing2021a,
	title = {Systematic {Review} of {Computing} {Approaches} for {Breast} {Cancer} {Detection} {Based} {Computer} {Aided} {Diagnosis} {Using} {Mammogram} {Images}},
	volume = {35},
	issn = {0883-9514, 1087-6545},
	url = {https://www.tandfonline.com/doi/full/10.1080/08839514.2021.2001177},
	doi = {10.1080/08839514.2021.2001177},
	abstract = {Breast cancer is one of the most prevalent types of cancer that plagues females. Mortality from breast cancer could be reduced by diagnosing and identifying it at an early stage. To detect breast cancer, various imaging modalities can be used, such as mammography. Computer-Aided Detection/Diagnosis (CAD) systems can assist an expert radiologist to diagnose breast cancer at an early stage. This paper introduces the findings of a systematic review that seeks to examine the state-of-the-art CAD systems for breast cancer detection. This review is based on 118 publications published in 2018–2021 and retrieved from major scientific publication databases while using a rigorous methodology of a systematic review. We provide a general description and analysis of existing CAD systems that use machine learning methods as well as their current state based on mammogram image modalities and classification methods. This systematic review presents all stages of CAD including preprocessing, segmentation, feature extraction, feature selection, and classification. We identify research gaps and outline recom­ mendations for future research. This systematic review may be helpful for both clinicians, who use CAD systems for early diag­ nosis of breast cancer, as well as for researchers to find knowl­ edge gaps and create more contributions for breast cancer diagnostics.},
	language = {en},
	number = {15},
	urldate = {2023-05-24},
	journal = {Applied Artificial Intelligence},
	author = {Zebari, Dilovan Asaad and Ibrahim, Dheyaa Ahmed and Zeebaree, Diyar Qader and Haron, Habibollah and Salih, Merdin Shamal and Damaševičius, Robertas and Mohammed, Mazin Abed},
	month = dec,
	year = {2021},
	pages = {2157--2203},
	file = {Zebari et al. - 2021 - Systematic Review of Computing Approaches for Brea.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\398F85Y5\\Zebari et al. - 2021 - Systematic Review of Computing Approaches for Brea.pdf:application/pdf},
}

@inproceedings{boschModelingClassifyingBreast2006,
	address = {New York, NY, USA},
	title = {Modeling and {Classifying} {Breast} {Tissue} {Density} in {Mammograms}},
	volume = {2},
	isbn = {978-0-7695-2597-6},
	url = {http://ieeexplore.ieee.org/document/1640941/},
	doi = {10.1109/CVPR.2006.188},
	abstract = {We present a new approach to model and classify breast parenchymal tissue. Given a mammogram, ﬁrst, we will discover the distribution of the different tissue densities in an unsupervised manner, and second, we will use this tissue distribution to perform the classiﬁcation. We achieve this using a classiﬁer based on local descriptors and probabilistic Latent Semantic Analysis (pLSA), a generative model from the statistical text literature.},
	language = {en},
	urldate = {2023-05-24},
	booktitle = {2006 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} - {Volume} 2 ({CVPR}'06)},
	publisher = {IEEE},
	author = {Bosch, A. and Munoz, X. and Oliver, A. and Marti, J.},
	year = {2006},
	pages = {1552--1558},
	file = {Bosch et al. - 2006 - Modeling and Classifying Breast Tissue Density in .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SZ3LS8A3\\Bosch et al. - 2006 - Modeling and Classifying Breast Tissue Density in .pdf:application/pdf},
}

@article{rojasdominguezBreastCancerDiagnosis2009,
	title = {Toward breast cancer diagnosis based on automated segmentation of masses in mammograms},
	volume = {42},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320308003154},
	doi = {10.1016/j.patcog.2008.08.006},
	abstract = {This work explores the use of characterization features extracted based on breast-mass contours obtained by automated segmentation methods, for the classification of masses in mammograms according to their diagnosis (benign or malignant). Two sets of mass contours were obtained via two segmentation methods (a dynamic-programming-based method and a constrained region-growing method), and simplified versions of these contours (modeling the contours as ellipses) were employed to extract a set of six features designed for characterization of mass margins (contrast between foreground region and background region, coefficient of variation of edge strength, two measures of the fuzziness of mass margins, a measure of spiculation based on relative gradient orientation, and a measure of spiculation based on edge-signature information). Three popular classifiers (Bayesian classifier, Fisher's linear discriminant, and a support vector machine) were then used to predict the diagnosis of a set of 349 masses based on each of said features and some combinations of these. The systems (each system consists of a segmentation method, a featureset, and a classifier) were compared with each other in terms of their performance on the diagnosis of the set of breast masses. It was found that, although there was a percent difference of about 14\% in the average segmentation quality between methods, this was translated into an average percent difference of only 4\% in the classification performance. It was also observed that the spiculation feature based on edgesignature information was distinctly better than the rest of the features, although it is not very robust to changes in the quality of the segmentation. All systems were more efficient in predicting the diagnosis of benign masses than that of the malignant masses, resulting in low sensitivity and high specificity values (e.g. 0.6 and 0.8, respectively) since the positive class in the classification experiments is the set of malignant masses. It was concluded that features extracted from automated contours can contribute to the diagnosis of breast masses in screening programs by correctly identifying a majority of benign masses. © 2008 Elsevier Ltd. All rights reserved.},
	language = {en},
	number = {6},
	urldate = {2023-05-24},
	journal = {Pattern Recognition},
	author = {Rojas Domínguez, Alfonso and Nandi, Asoke K.},
	month = jun,
	year = {2009},
	pages = {1138--1148},
	file = {Rojas Domínguez and Nandi - 2009 - Toward breast cancer diagnosis based on automated .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4MIQW7GV\\Rojas Domínguez and Nandi - 2009 - Toward breast cancer diagnosis based on automated .pdf:application/pdf},
}

@article{wangAutomaticDetectionBreast2009,
	title = {Automatic detection of breast cancers in mammograms using structured support vector machines},
	volume = {72},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231209000873},
	doi = {10.1016/j.neucom.2009.02.015},
	abstract = {Breast cancer is one of the most common cancers diagnosed in women. Large margin classiﬁers like the support vector machine (SVM) have been reported effective in computer-assisted diagnosis systems for breast cancers. However, since the separating hyperplane determination exclusively relies on support vectors, the SVM is essentially a local classiﬁer and its performance can be further improved. In this work, we introduce a structured SVM model to determine if each mammographic region is normal or cancerous by considering the cluster structures in the training set. The optimization problem in this new model can be solved efﬁciently by being formulated as one second order cone programming problem. Experimental evaluation is performed on the Digital Database for Screening Mammography (DDSM) dataset. Various types of features, including curvilinear features, texture features, Gabor features, and multi-resolution features, are extracted from the sample images. We then select the salient features using the recursive feature elimination algorithm. The structured SVM achieves better detection performance compared with a well-tested SVM classiﬁer in terms of the area under the ROC curve.},
	language = {en},
	number = {13-15},
	urldate = {2023-05-24},
	journal = {Neurocomputing},
	author = {Wang, Defeng and Shi, Lin and Ann Heng, Pheng},
	month = aug,
	year = {2009},
	pages = {3296--3302},
	file = {Wang et al. - 2009 - Automatic detection of breast cancers in mammogram.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\F5QF4XFY\\Wang et al. - 2009 - Automatic detection of breast cancers in mammogram.pdf:application/pdf},
}

@article{brazjuniorClassificationBreastTissues2009,
	title = {Classification of breast tissues using {Moran}'s index and {Geary}'s coefficient as texture signatures and {SVM}},
	volume = {39},
	issn = {00104825},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010482509001620},
	doi = {10.1016/j.compbiomed.2009.08.009},
	abstract = {Female breast cancer is the major cause of cancer-related deaths in western countries. Efforts in computer vision have been made in order to help improving the diagnostic accuracy by radiologists. In this paper, we present a methodology that uses Moran's index and Geary's coefficient measures in breast tissues extracted from mammogram images. These measures are used as input features for a support vector machine classifier with the purpose of distinguishing tissues between normal and abnormal cases as well as classifying them into benign and malignant cancerous cases. The use of both proposed techniques showed to be very promising, since we obtained an accuracy of 96.04\% and Az ROC of 0.946 with Geary's coefficient and an accuracy of 99.39\% and Az ROC of 1 with Moran's index to discriminate tissues in mammograms as normal or abnormal. We also obtained accuracy of 88.31\% and Az ROC of 0.804 with Geary's coefficient and accuracy of 87.80\% and Az ROC of 0.89 with Moran's index to discriminate tissues in mammograms as benign and malignant.},
	language = {en},
	number = {12},
	urldate = {2023-05-24},
	journal = {Computers in Biology and Medicine},
	author = {Braz Junior, Geraldo and Cardoso De Paiva, Anselmo and Corrêa Silva, Aristófanes and Cesar Muniz De Oliveira, Alexandre},
	month = dec,
	year = {2009},
	pages = {1063--1072},
	file = {Braz Junior et al. - 2009 - Classification of breast tissues using Moran's ind.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\846HV2VV\\Braz Junior et al. - 2009 - Classification of breast tissues using Moran's ind.pdf:application/pdf},
}

@article{akaySupportVectorMachines2009,
	title = {Support vector machines combined with feature selection for breast cancer diagnosis},
	volume = {36},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417408000912},
	doi = {10.1016/j.eswa.2008.01.009},
	abstract = {Breast cancer is the second largest cause of cancer deaths among women. At the same time, it is also among the most curable cancer types if it can be diagnosed early. Research eﬀorts have reported with increasing conﬁrmation that the support vector machines (SVM) have greater accurate diagnosis ability. In this paper, breast cancer diagnosis based on a SVM-based method combined with feature selection has been proposed. Experiments have been conducted on diﬀerent training-test partitions of the Wisconsin breast cancer dataset (WBCD), which is commonly used among researchers who use machine learning methods for breast cancer diagnosis. The performance of the method is evaluated using classiﬁcation accuracy, sensitivity, speciﬁcity, positive and negative predictive values, receiver operating characteristic (ROC) curves and confusion matrix. The results show that the highest classiﬁcation accuracy (99.51\%) is obtained for the SVM model that contains ﬁve features, and this is very promising compared to the previously reported results.},
	language = {en},
	number = {2},
	urldate = {2023-05-24},
	journal = {Expert Systems with Applications},
	author = {Akay, Mehmet Fatih},
	month = mar,
	year = {2009},
	pages = {3240--3247},
	file = {Akay - 2009 - Support vector machines combined with feature sele.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4E6TN2LK\\Akay - 2009 - Support vector machines combined with feature sele.pdf:application/pdf},
}

@article{meselhyeltoukhyComparisonWaveletCurvelet2010,
	title = {A comparison of wavelet and curvelet for breast cancer diagnosis in digital mammogram},
	volume = {40},
	issn = {00104825},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S001048251000017X},
	doi = {10.1016/j.compbiomed.2010.02.002},
	abstract = {This paper presents a comparative study between wavelet and curvelet transform for breast cancer diagnosis in digital mammogram. Using multiresolution analysis, mammogram images are decomposed into different resolution levels, which are sensitive to different frequency bands. A set of the biggest coefﬁcients from each decomposition level is extracted. Then a supervised classiﬁer system based on Euclidian distance is constructed. The performance of the classiﬁer is evaluated using a 2 Â 5-fold cross validation followed by a statistical analysis. The experimental results suggest that curvelet transform outperforms wavelet transform and the difference is statistically signiﬁcant.},
	language = {en},
	number = {4},
	urldate = {2023-05-24},
	journal = {Computers in Biology and Medicine},
	author = {Meselhy Eltoukhy, Mohamed and Faye, Ibrahima and Belhaouari Samir, Brahim},
	month = apr,
	year = {2010},
	pages = {384--391},
	file = {Meselhy Eltoukhy et al. - 2010 - A comparison of wavelet and curvelet for breast ca.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZBBW7I83\\Meselhy Eltoukhy et al. - 2010 - A comparison of wavelet and curvelet for breast ca.pdf:application/pdf},
}

@inproceedings{mencattiniBilateralAsymmetryIdentification2011,
	address = {Bari, Italy},
	title = {Bilateral asymmetry identification for the early detection of breast cancer},
	isbn = {978-1-4244-9336-4},
	url = {http://ieeexplore.ieee.org/document/5966746/},
	doi = {10.1109/MeMeA.2011.5966746},
	abstract = {Breast cancer is the second most common cancer overall and the leading cause of cancer deaths in women. Mammography is, at present, the only viable method for detecting most of tumors early enough for effective treatment. The secret of setting up the accurate diagnosis is to detect and understand the most subtle signs of breast lesions. Analysis of asymmetry between the left and right mammograms can provide clues about the presence of early signs of tumors. In this work we present an automated procedure for bilateral asymmetry detection composed of the following steps: (1) mammography density analysis and ﬁbro-glandular disc detection through adaptive clustering techniques, (2) analysis and implementation of bilateral asymmetries detection algorithms based on Gabor ﬁlters analysis, (3) use of a linear Bayes classiﬁer with the leave-one-out method to asses the asymmetry degree of the two breasts, (4) metrological evaluation of the whole system through random and systematic measurement uncertainty contributions modeling.},
	language = {en},
	urldate = {2023-05-24},
	booktitle = {2011 {IEEE} {International} {Symposium} on {Medical} {Measurements} and {Applications}},
	publisher = {IEEE},
	author = {Mencattini, A. and Salmeri, M. and Casti, P.},
	month = may,
	year = {2011},
	pages = {613--618},
	file = {Mencattini et al. - 2011 - Bilateral asymmetry identification for the early d.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\AU8W5UB9\\Mencattini et al. - 2011 - Bilateral asymmetry identification for the early d.pdf:application/pdf},
}

@article{makrogiannisDiscriminativeLocalizedSparse2021,
	title = {Discriminative {Localized} {Sparse} {Approximations} for {Mass} {Characterization} in {Mammograms}},
	volume = {11},
	issn = {2234-943X},
	url = {https://www.frontiersin.org/articles/10.3389/fonc.2021.725320/full},
	doi = {10.3389/fonc.2021.725320},
	abstract = {The most common form of cancer among women in both developed and developing countries is breast cancer. The early detection and diagnosis of this disease is significant because it may reduce the number of deaths caused by breast cancer and improve the quality of life of those effected. Computer-aided detection (CADe) and computer-aided diagnosis (CADx) methods have shown promise in recent years for aiding in the human expert reading analysis and improving the accuracy and reproducibility of pathology results. One significant application of CADe and CADx is for breast cancer screening using mammograms. In image processing and machine learning research, relevant results have been produced by sparse analysis methods to represent and recognize imaging patterns. However, application of sparse analysis techniques to the biomedical field is challenging, as the objects of interest may be obscured because of contrast limitations or background tissues, and their appearance may change because of anatomical variability. We introduce methods for label-specific and label-consistent dictionary learning to improve the separation of benign breast masses from malignant breast masses in mammograms. We integrated these approaches into our Spatially Localized Ensemble Sparse Analysis (SLESA) methodology. We performed 10- and 30-fold cross validation (CV) experiments on multiple mammography datasets to measure the classification performance of our methodology and compared it to deep learning models and conventional sparse representation. Results from these experiments show the potential of this methodology for separation of malignant from benign masses as a part of a breast cancer screening workflow.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Frontiers in Oncology},
	author = {Makrogiannis, Sokratis and Zheng, Keni and Harris, Chelsea},
	month = dec,
	year = {2021},
	pages = {725320},
	file = {Makrogiannis et al. - 2021 - Discriminative Localized Sparse Approximations for.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FTWYMG2I\\Makrogiannis et al. - 2021 - Discriminative Localized Sparse Approximations for.pdf:application/pdf},
}

@article{liBreastMassesMammography2017,
	title = {Breast masses in mammography classification with local contour features},
	volume = {16},
	issn = {1475-925X},
	url = {http://biomedical-engineering-online.biomedcentral.com/articles/10.1186/s12938-017-0332-0},
	doi = {10.1186/s12938-017-0332-0},
	abstract = {Background:  Mammography is one of the most popular tools for early detection of breast cancer. Contour of breast mass in mammography is very important information to distinguish benign and malignant mass. Contour of benign mass is smooth and round or oval, while malignant mass has irregular shape and spiculated contour. Several studies have shown that 1D signature translated from 2D contour can describe the contour features well.
Methods:  In this paper, we propose a new method to translate 2D contour of breast mass in mammography into 1D signature. The method can describe not only the contour features but also the regularity of breast mass. Then we segment the whole 1D signature into different subsections. We extract four local features including a new contour descriptor from the subsections. The new contour descriptor is root mean square (RMS) slope. It can describe the roughness of the contour. KNN, SVM and ANN classifier are used to classify benign breast mass and malignant mass.
Results:  The proposed method is tested on a set with 323 contours including 143 benign masses and 180 malignant ones from digital database of screening mammography (DDSM). The best accuracy of classification is 99.66\% using the feature of root mean square slope with SVM classifier.
Conclusion:  The performance of the proposed method is better than traditional method. In addition, RMS slope is an effective feature comparable to most of the existing features.},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {BioMedical Engineering OnLine},
	author = {Li, Haixia and Meng, Xianjing and Wang, Tingwen and Tang, Yuchun and Yin, Yilong},
	month = dec,
	year = {2017},
	pages = {44},
	file = {Li et al. - 2017 - Breast masses in mammography classification with l.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7YBR9B7L\\Li et al. - 2017 - Breast masses in mammography classification with l.pdf:application/pdf},
}

@article{jiaoDeepFeatureBased2016,
	title = {A deep feature based framework for breast masses classification},
	volume = {197},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231216003611},
	doi = {10.1016/j.neucom.2016.02.060},
	abstract = {Characteristic classiﬁcation of mass plays a role of vital importance in diagnosis of breast cancer. The existing computer aided diagnosis (CAD) methods used to beneﬁt a lot from low-level or middle-level features which are not that good at the simulation of real diagnostic processes, adding difﬁculties in improving the classiﬁcation performance. In this paper, we design a deep feature based framework for breast mass classiﬁcation task. It mainly contains a convolutional neural network (CNN) and a decision mechanism. Combining intensity information and deep features automatically extracted by the trained CNN from the original image, our proposed method could better simulate the diagnostic procedure operated by doctors and achieved state-of-art performance. In this framework, doctors' global and local impressions left by mass images were represented by deep features extracted from two different layers called high-level and middle-level features. Meanwhile, the original images were regarded as detailed descriptions of the breast mass. Then, classiﬁers based on features above were used in combination to predict classes of test images. And outcomes of classiﬁers based on different features were analyzed jointly to determine the types of test images. With the help of two kinds of feature visualization methods, deep features extracted from different layers illustrate effective in classiﬁcation performance and diagnosis simulation. In addition, our method was applied to DDSM dataset and achieved high accuracy under two objective evaluation measures.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Neurocomputing},
	author = {Jiao, Zhicheng and Gao, Xinbo and Wang, Ying and Li, Jie},
	month = jul,
	year = {2016},
	pages = {221--231},
	file = {Jiao et al. - 2016 - A deep feature based framework for breast masses c.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PWUSKT9Y\\Jiao et al. - 2016 - A deep feature based framework for breast masses c.pdf:application/pdf},
}

@incollection{dhungelAutomatedLearningDeep2016,
	address = {Cham},
	title = {The {Automated} {Learning} of {Deep} {Features} for {Breast} {Mass} {Classification} from {Mammograms}},
	volume = {9901},
	isbn = {978-3-319-46722-1 978-3-319-46723-8},
	url = {https://link.springer.com/10.1007/978-3-319-46723-8_13},
	abstract = {The classiﬁcation of breast masses from mammograms into benign or malignant has been commonly addressed with machine learning classiﬁers that use as input a large set of hand-crafted features, usually based on general geometrical and texture information. In this paper, we propose a novel deep learning method that automatically learns features based directly on the optmisation of breast mass classiﬁcation from mammograms, where we target an improved classiﬁcation performance compared to the approach described above. The novelty of our approach lies in the two-step training process that involves a pre-training based on the learning of a regressor that estimates the values of a large set of handcrafted features, followed by a ﬁne-tuning stage that learns the breast mass classiﬁer. Using the publicly available INbreast dataset, we show that the proposed method produces better classiﬁcation results, compared with the machine learning model using hand-crafted features and with deep learning method trained directly for the classiﬁcation stage without the pre-training stage. We also show that the proposed method produces the current state-of-the-art breast mass classiﬁcation results for the INbreast dataset. Finally, we integrate the proposed classiﬁer into a fully automated breast mass detection and segmentation, which shows promising results.},
	language = {en},
	urldate = {2023-05-24},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2016},
	publisher = {Springer International Publishing},
	author = {Dhungel, Neeraj and Carneiro, Gustavo and Bradley, Andrew P.},
	editor = {Ourselin, Sebastien and Joskowicz, Leo and Sabuncu, Mert R. and Unal, Gozde and Wells, William},
	year = {2016},
	doi = {10.1007/978-3-319-46723-8_13},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {106--114},
	file = {Dhungel et al. - 2016 - The Automated Learning of Deep Features for Breast.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NK377GQS\\Dhungel et al. - 2016 - The Automated Learning of Deep Features for Breast.pdf:application/pdf},
}

@misc{yinTransferringLearnedMicrocalcification2016,
	title = {Transferring {Learned} {Microcalcification} {Group} {Detection} from {2D} {Mammography} to {3D} {Digital} {Breast} {Tomosynthesis} {Using} a {Hierarchical} {Model} and {Scope}-based {Normalization} {Features}},
	url = {http://arxiv.org/abs/1603.05955},
	abstract = {A novel hierarchical model is introduced to solve a general problem of detecting groups of similar objects. Under this model, detection of groups is performed in hierarchically organized layers while each layer represents a scope for target objects. The processing of these layers involves sequential extraction of appearance features for an individual object, consistency measurement features for nearby objects, and ﬁnally the distribution features for all objects within the group. Using the concept of scope-based normalization, the extracted features not only enhance local contrast of an individual object, but also provide consistent characterization for all related objects. As an example, a microcalciﬁcation group detection system for 2D mammography was developed, and then the learned model was transferred to 3D digital breast tomosynthesis without any retraining or ﬁne-tuning. The detection system demonstrated state-of-the-art performance and detected 96\% of cancerous lesions at the rate of 1.2 false positives per volume as measured on an independent tomosynthesis test set.},
	language = {en},
	urldate = {2023-05-24},
	publisher = {arXiv},
	author = {Yin, Yin and Fotin, Sergei V. and Haldankar, Hrishikesh and Hoffmeister, Jeffrey W. and Periaswamy, Senthil},
	month = mar,
	year = {2016},
	note = {arXiv:1603.05955 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Yin et al. - 2016 - Transferring Learned Microcalcification Group Dete.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CWU3IX23\\Yin et al. - 2016 - Transferring Learned Microcalcification Group Dete.pdf:application/pdf},
}

@inproceedings{bekkerMultiviewDeepLearning2016,
	address = {Prague, Czech Republic},
	title = {A multi-view deep learning architecture for classification of breast microcalcifications},
	isbn = {978-1-4799-2349-6},
	url = {http://ieeexplore.ieee.org/document/7493369/},
	doi = {10.1109/ISBI.2016.7493369},
	abstract = {In this paper we address the problem of differentiating between malignant and benign tumors based on their appearance in the CC and MLO mammography views. Classiﬁcation of clustered breast microcalciﬁcations into benign and malignant categories is an extremely challenging task for computerized algorithms and expert radiologists alike. We describe a deep-learning classiﬁcation method that is based on two view-level decisions, implemented by two neural networks, followed by a single-neuron layer that combines the viewlevel decisions into a global decision that mimics the biopsy results. Our method is evaluated on a large multi-view dataset extracted from the standardized digital database for screening mammography (DDSM). Experimental results show that our network structure signiﬁcantly improves on previously suggested methods.},
	language = {en},
	urldate = {2023-05-24},
	booktitle = {2016 {IEEE} 13th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI})},
	publisher = {IEEE},
	author = {Bekker, Alan Joseph and Greenspan, Hayit and Goldberger, Jacob},
	month = apr,
	year = {2016},
	pages = {726--730},
	file = {Bekker et al. - 2016 - A multi-view deep learning architecture for classi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EVCNPU2S\\Bekker et al. - 2016 - A multi-view deep learning architecture for classi.pdf:application/pdf},
}

@inproceedings{wuBreastDensityClassification2018,
	address = {Calgary, AB},
	title = {Breast {Density} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	isbn = {978-1-5386-4658-8},
	url = {https://ieeexplore.ieee.org/document/8462671/},
	doi = {10.1109/ICASSP.2018.8462671},
	abstract = {Breast density classiﬁcation is an essential part of breast cancer screening. Although a lot of prior work considered this problem as a task for learning algorithms, to our knowledge, all of them used small and not clinically realistic data both for training and evaluation of their models. In this work, we explore the limits of this task with a data set coming from over 200,000 breast cancer screening exams. We use this data to train and evaluate a strong convolutional neural network classiﬁer. In a reader study, we ﬁnd that our model can perform this task comparably to a human expert.},
	language = {en},
	urldate = {2023-05-24},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Wu, Nan and Geras, Krzysztof J. and Shen, Yiqiu and Su, Jingyi and Kim, S. Gene and Kim, Eric and Wolfson, Stacey and Moy, Linda and Cho, Kyunghyun},
	month = apr,
	year = {2018},
	pages = {6682--6686},
	file = {Wu et al. - 2018 - Breast Density Classification with Deep Convolutio.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Z9IIL7GU\\Wu et al. - 2018 - Breast Density Classification with Deep Convolutio.pdf:application/pdf},
}

@article{liDeepLearningBreast2017,
	title = {Deep learning in breast cancer risk assessment: evaluation of convolutional neural networks on a clinical dataset of full-field digital mammograms},
	volume = {4},
	issn = {2329-4302},
	shorttitle = {Deep learning in breast cancer risk assessment},
	url = {https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-4/issue-04/041304/Deep-learning-in-breast-cancer-risk-assessment--evaluation-of/10.1117/1.JMI.4.4.041304.full},
	doi = {10.1117/1.JMI.4.4.041304},
	language = {en},
	number = {04},
	urldate = {2023-05-24},
	journal = {Journal of Medical Imaging},
	author = {Li, Hui and Giger, Maryellen L. and Huynh, Benjamin Q. and Antropova, Natalia O.},
	month = sep,
	year = {2017},
	pages = {1},
	file = {Li et al. - 2017 - Deep learning in breast cancer risk assessment ev.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\F8RJGTCV\\Li et al. - 2017 - Deep learning in breast cancer risk assessment ev.pdf:application/pdf},
}

@article{mohamedDeepLearningMethod2018,
	title = {A deep learning method for classifying mammographic breast density categories},
	volume = {45},
	issn = {00942405},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/mp.12683},
	doi = {10.1002/mp.12683},
	abstract = {Purpose: Mammographic breast density is an established risk marker for breast cancer and is visually assessed by radiologists in routine mammogram image reading, using four qualitative Breast Imaging and Reporting Data System (BI-RADS) breast density categories. It is particularly difficult for radiologists to consistently distinguish the two most common and most variably assigned BIRADS categories, i.e., “scattered density” and “heterogeneously dense”. The aim of this work was to investigate a deep learning-based breast density classifier to consistently distinguish these two categories, aiming at providing a potential computerized tool to assist radiologists in assigning a BIRADS category in current clinical workflow.
Methods: In this study, we constructed a convolutional neural network (CNN)-based model coupled with a large (i.e., 22,000 images) digital mammogram imaging dataset to evaluate the classification performance between the two aforementioned breast density categories. All images were collected from a cohort of 1,427 women who underwent standard digital mammography screening from 2005 to 2016 at our institution. The truths of the density categories were based on standard clinical assessment made by board-certified breast imaging radiologists. Effects of direct training from scratch solely using digital mammogram images and transfer learning of a pretrained model on a large nonmedical imaging dataset were evaluated for the specific task of breast density classification. In order to measure the classification performance, the CNN classifier was also tested on a refined version of the mammogram image dataset by removing some potentially inaccurately labeled images. Receiver operating characteristic (ROC) curves and the area under the curve (AUC) were used to measure the accuracy of the classifier.
Results: The AUC was 0.9421 when the CNN-model was trained from scratch on our own mammogram images, and the accuracy increased gradually along with an increased size of training samples. Using the pretrained model followed by a fine-tuning process with as few as 500 mammogram images led to an AUC of 0.9265. After removing the potentially inaccurately labeled images, AUC was increased to 0.9882 and 0.9857 for without and with the pretrained model, respectively, both significantly higher (P {\textless} 0.001) than when using the full imaging dataset.
Conclusions: Our study demonstrated high classification accuracies between two difficult to distinguish breast density categories that are routinely assessed by radiologists. We anticipate that our approach will help enhance current clinical assessment of breast density and better support consistent density notification to patients in breast cancer screening. © 2017 American Association of Physicists in Medicine [https://doi.org/10.1002/mp.12683]},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {Medical Physics},
	author = {Mohamed, Aly A. and Berg, Wendie A. and Peng, Hong and Luo, Yahong and Jankowitz, Rachel C. and Wu, Shandong},
	month = jan,
	year = {2018},
	pages = {314--321},
	file = {Mohamed et al. - 2018 - A deep learning method for classifying mammographi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\U7P863ZZ\\Mohamed et al. - 2018 - A deep learning method for classifying mammographi.pdf:application/pdf},
}

@article{chougradDeepConvolutionalNeural2018,
	title = {Deep {Convolutional} {Neural} {Networks} for breast cancer screening},
	volume = {157},
	issn = {01692607},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169260717301451},
	doi = {10.1016/j.cmpb.2018.01.011},
	abstract = {Background and objective: Radiologists often have a hard time classifying mammography mass lesions which leads to unnecessary breast biopsies to remove suspicions and this ends up adding exorbitant expenses to an already burdened patient and health care system.
Methods: In this paper we developed a Computer-aided Diagnosis (CAD) system based on deep Convolutional Neural Networks (CNN) that aims to help the radiologist classify mammography mass lesions. Deep learning usually requires large datasets to train networks of a certain depth from scratch. Transfer learning is an effective method to deal with relatively small datasets as in the case of medical images, although it can be tricky as we can easily start overﬁtting.
Results: In this work, we explore the importance of transfer learning and we experimentally determine the best ﬁne-tuning strategy to adopt when training a CNN model. We were able to successfully ﬁne-tune some of the recent, most powerful CNNs and achieved better results compared to other state-of-the-art methods which classiﬁed the same public datasets. For instance we achieved 97.35\% accuracy and 0.98 AUC on the DDSM database, 95.50\% accuracy and 0.97 AUC on the INbreast database and 96.67\% accuracy and 0.96 AUC on the BCDR database. Furthermore, after pre-processing and normalizing all the extracted Regions of Interest (ROIs) from the full mammograms, we merged all the datasets to build one large set of images and used it to ﬁne-tune our CNNs. The CNN model which achieved the best results, a 98.94\% accuracy, was used as a baseline to build the Breast Cancer Screening Framework. To evaluate the proposed CAD system and its eﬃciency to classify new images, we tested it on an independent database (MIAS) and got 98.23\% accuracy and 0.99 AUC.
Conclusion: The results obtained demonstrate that the proposed framework is performant and can indeed be used to predict if the mass lesions are benign or malignant. © 2018 Elsevier B.V. All rights reserved.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Chougrad, Hiba and Zouaki, Hamid and Alheyane, Omar},
	month = apr,
	year = {2018},
	pages = {19--30},
	file = {Chougrad et al. - 2018 - Deep Convolutional Neural Networks for breast canc.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PLJABT2I\\Chougrad et al. - 2018 - Deep Convolutional Neural Networks for breast canc.pdf:application/pdf},
}

@article{wangNovelFullyAutomated2017,
	title = {A novel and fully automated mammographic texture analysis for risk prediction: results from two case-control studies},
	volume = {19},
	issn = {1465-542X},
	shorttitle = {A novel and fully automated mammographic texture analysis for risk prediction},
	url = {http://breast-cancer-research.biomedcentral.com/articles/10.1186/s13058-017-0906-6},
	doi = {10.1186/s13058-017-0906-6},
	abstract = {Background: The percentage of mammographic dense tissue (PD) is an important risk factor for breast cancer, and there is some evidence that texture features may further improve predictive ability. However, relatively little work has assessed or validated textural feature algorithms using raw full field digital mammograms (FFDM).
Method: A case-control study nested within a screening cohort (age 46–73 years) from Manchester UK was used to develop a texture feature risk score (264 cases diagnosed at the same time as mammogram of the contralateral breast, 787 controls) using the least absolute shrinkage and selection operator (LASSO) method for 112 features, and validated in a second case-control study from the same cohort but with cases diagnosed after the index mammogram (317 cases, 931 controls). Predictive ability was assessed using deviance and matched concordance index (mC). The ability to improve risk estimation beyond percent volumetric density (Volpara) was evaluated using conditional logistic regression.
Results: The strongest features identified in the training set were “sum average” based on the grey-level co-occurrence matrix at low image resolutions (original resolution 10.628 pixels per mm; downsized by factors of 16, 32 and 64), which had a better deviance and mC than volumetric PD. In the validation study, the risk score combining the three sum average features achieved a better deviance than volumetric PD (Δχ2 = 10.55 or 6.95 if logarithm PD) and a similar mC to volumetric PD (0.58 and 0.57, respectively). The risk score added independent information to volumetric PD (Δχ2 = 14.38, p = 0.0008).
Conclusion: Textural features based on digital mammograms improve risk assessment beyond volumetric percentage density. The features and risk score developed need further investigation in other settings.},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {Breast Cancer Research},
	author = {Wang, Chao and Brentnall, Adam R. and Cuzick, Jack and Harkness, Elaine F. and Evans, D. Gareth and Astley, Susan},
	month = dec,
	year = {2017},
	pages = {114},
	file = {Wang et al. - 2017 - A novel and fully automated mammographic texture a.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9GEXAGEM\\Wang et al. - 2017 - A novel and fully automated mammographic texture a.pdf:application/pdf},
}

@misc{gerasHighResolutionBreastCancer2018,
	title = {High-{Resolution} {Breast} {Cancer} {Screening} with {Multi}-{View} {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1703.07047},
	abstract = {Advances in deep learning for natural images have prompted a surge of interest in applying similar techniques to medical images. The majority of the initial attempts focused on replacing the input of a deep convolutional neural network with a medical image, which does not take into consideration the fundamental differences between these two types of images. Speciﬁcally, ﬁne details are necessary for detection in medical images, unlike in natural images where coarse structures matter most. This difference makes it inadequate to use the existing network architectures developed for natural images, because they work on heavily downscaled images to reduce the memory requirements. This hides details necessary to make accurate predictions. Additionally, a single exam in medical imaging often comes with a set of views which must be fused in order to reach a correct conclusion. In our work, we propose to use a multi-view deep convolutional neural network that handles a set of high-resolution medical images. We evaluate it on largescale mammography-based breast cancer screening (BI-RADS prediction) using 886,000 images. We focus on investigating the impact of the training set size and image size on the prediction accuracy. Our results highlight that performance increases with the size of training set, and that the best performance can only be achieved using the original resolution. In the reader study, performed on a random subset of the test set, we conﬁrmed the efﬁcacy of our model, which achieved performance comparable to a committee of radiologists when presented with the same data.},
	language = {en},
	urldate = {2023-05-24},
	publisher = {arXiv},
	author = {Geras, Krzysztof J. and Wolfson, Stacey and Shen, Yiqiu and Wu, Nan and Kim, S. Gene and Kim, Eric and Heacock, Laura and Parikh, Ujas and Moy, Linda and Cho, Kyunghyun},
	month = jun,
	year = {2018},
	note = {arXiv:1703.07047 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Geras et al. - 2018 - High-Resolution Breast Cancer Screening with Multi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MBYA9D6Z\\Geras et al. - 2018 - High-Resolution Breast Cancer Screening with Multi.pdf:application/pdf},
}

@article{al-masniSimultaneousDetectionClassification2018,
	title = {Simultaneous detection and classification of breast masses in digital mammograms via a deep learning {YOLO}-based {CAD} system},
	volume = {157},
	issn = {01692607},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169260717314980},
	doi = {10.1016/j.cmpb.2018.01.017},
	abstract = {Background and objective: Automatic detection and classiﬁcation of the masses in mammograms are still a big challenge and play a crucial role to assist radiologists for accurate diagnosis. In this paper, we propose a novel Computer-Aided Diagnosis (CAD) system based on one of the regional deep learning techniques, a ROI-based Convolutional Neural Network (CNN) which is called You Only Look Once (YOLO). Although most previous studies only deal with classiﬁcation of masses, our proposed YOLO-based CAD system can handle detection and classiﬁcation simultaneously in one framework.
Methods: The proposed CAD system contains four main stages: preprocessing of mammograms, feature extraction utilizing deep convolutional networks, mass detection with conﬁdence, and ﬁnally mass classiﬁcation using Fully Connected Neural Networks (FC-NNs). In this study, we utilized original 600 mammograms from Digital Database for Screening Mammography (DDSM) and their augmented mammograms of 2,400 with the information of the masses and their types in training and testing our CAD. The trained YOLO-based CAD system detects the masses and then classiﬁes their types into benign or malignant.
Results: Our results with ﬁve-fold cross validation tests show that the proposed CAD system detects the mass location with an overall accuracy of 99.7\%. The system also distinguishes between benign and malignant lesions with an overall accuracy of 97\%.
Conclusions: Our proposed system even works on some challenging breast cancer cases where the masses exist over the pectoral muscles or dense regions.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Al-masni, Mohammed A. and Al-antari, Mugahed A. and Park, Jeong-Min and Gi, Geon and Kim, Tae-Yeon and Rivera, Patricio and Valarezo, Edwin and Choi, Mun-Taek and Han, Seung-Moo and Kim, Tae-Seong},
	month = apr,
	year = {2018},
	pages = {85--94},
	file = {Al-masni et al. - 2018 - Simultaneous detection and classification of breas.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BXNAQ98K\\Al-masni et al. - 2018 - Simultaneous detection and classification of breas.pdf:application/pdf},
}

@misc{mehtaYNetJointSegmentation2018,
	title = {Y-{Net}: {Joint} {Segmentation} and {Classification} for {Diagnosis} of {Breast} {Biopsy} {Images}},
	shorttitle = {Y-{Net}},
	url = {http://arxiv.org/abs/1806.01313},
	abstract = {In this paper, we introduce a conceptually simple network for generating discriminative tissue-level segmentation masks for the purpose of breast cancer diagnosis. Our method eﬃciently segments diﬀerent types of tissues in breast biopsy images while simultaneously predicting a discriminative map for identifying important areas in an image. Our network, Y-Net, extends and generalizes U-Net by adding a parallel branch for discriminative map generation and by supporting convolutional block modularity, which allows the user to adjust network eﬃciency without altering the network topology. Y-Net delivers state-of-the-art segmentation accuracy while learning 6.6× fewer parameters than its closest competitors. The addition of descriptive power from Y-Net’s discriminative segmentation masks improve diagnostic classiﬁcation accuracy by 7\% over state-of-the-art methods for diagnostic classiﬁcation. Source code is available at: https://sacmehta.github.io/YNet.},
	language = {en},
	urldate = {2023-05-24},
	publisher = {arXiv},
	author = {Mehta, Sachin and Mercan, Ezgi and Bartlett, Jamen and Weave, Donald and Elmore, Joann G. and Shapiro, Linda},
	month = jun,
	year = {2018},
	note = {arXiv:1806.01313 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Mehta et al. - 2018 - Y-Net Joint Segmentation and Classification for D.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\E5FV3858\\Mehta et al. - 2018 - Y-Net Joint Segmentation and Classification for D.pdf:application/pdf},
}

@article{tsochatzidisDeepLearningBreast2019,
	title = {Deep {Learning} for {Breast} {Cancer} {Diagnosis} from {Mammograms}—{A} {Comparative} {Study}},
	volume = {5},
	issn = {2313-433X},
	url = {https://www.mdpi.com/2313-433X/5/3/37},
	doi = {10.3390/jimaging5030037},
	abstract = {Deep convolutional neural networks (CNNs) are investigated in the context of computer-aided diagnosis (CADx) of breast cancer. State-of-the-art CNNs are trained and evaluated on two mammographic datasets, consisting of ROIs depicting benign or malignant mass lesions. The performance evaluation of each examined network is addressed in two training scenarios: the ﬁrst involves initializing the network with pre-trained weights, while for the second the networks are initialized in a random fashion. Extensive experimental results show the superior performance achieved in the case of ﬁne-tuning a pretrained network compared to training from scratch.},
	language = {en},
	number = {3},
	urldate = {2023-05-24},
	journal = {Journal of Imaging},
	author = {Tsochatzidis, Lazaros and Costaridou, Lena and Pratikakis, Ioannis},
	month = mar,
	year = {2019},
	pages = {37},
	file = {Tsochatzidis et al. - 2019 - Deep Learning for Breast Cancer Diagnosis from Mam.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\AKTGEMMG\\Tsochatzidis et al. - 2019 - Deep Learning for Breast Cancer Diagnosis from Mam.pdf:application/pdf},
}

@article{caiBreastMicrocalcificationDiagnosis2019,
	title = {Breast {Microcalcification} {Diagnosis} {Using} {Deep} {Convolutional} {Neural} {Network} from {Digital} {Mammograms}},
	volume = {2019},
	issn = {1748-670X, 1748-6718},
	url = {https://www.hindawi.com/journals/cmmm/2019/2717454/},
	doi = {10.1155/2019/2717454},
	abstract = {Mammography is successfully used as an effective screening tool for cancer diagnosis. A calcification cluster on mammography is a primary sign of cancer. Early researches have proved the diagnostic value of the calcification, yet their performance is highly dependent on handcrafted image descriptors. Characterizing the calcification mammography in an automatic and robust way remains a challenge. In this paper, the calcification was characterized by descriptors obtained from deep learning and handcrafted descriptors. We compared the performances of different image feature sets on digital mammograms. The feature sets included the deep features alone, the handcrafted features, their combination, and the filtered deep features. Experimental results have demonstrated that the deep features outperform handcrafted features, but the handcrafted features can provide complementary information for deep features. We achieved a classification precision of 89.32\% and sensitivity of 86.89\% using the filtered deep features, which is the best performance among all the feature sets.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Computational and Mathematical Methods in Medicine},
	author = {Cai, Hongmin and Huang, Qinjian and Rong, Wentao and Song, Yan and Li, Jiao and Wang, Jinhua and Chen, Jiazhou and Li, Li},
	month = mar,
	year = {2019},
	pages = {1--10},
	file = {Cai et al. - 2019 - Breast Microcalcification Diagnosis Using Deep Con.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SZQGHZIM\\Cai et al. - 2019 - Breast Microcalcification Diagnosis Using Deep Con.pdf:application/pdf},
}

@inproceedings{guanBreastCancerDetection2018,
	address = {Atlanta, United States},
	title = {Breast cancer detection using synthetic mammograms from generative adversarial networks in convolutional neural networks},
	isbn = {978-1-5106-2007-0 978-1-5106-2008-7},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10718/2318100/Breast-cancer-detection-using-synthetic-mammograms-from-generative-adversarial-networks/10.1117/12.2318100.full},
	doi = {10.1117/12.2318100},
	language = {en},
	urldate = {2023-05-24},
	booktitle = {14th {International} {Workshop} on {Breast} {Imaging} ({IWBI} 2018)},
	publisher = {SPIE},
	author = {Guan, Shuyue and Loew, Murray},
	editor = {Krupinski, Elizabeth A.},
	month = jul,
	year = {2018},
	pages = {43},
	file = {Guan and Loew - 2018 - Breast cancer detection using synthetic mammograms.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\K9JTEH94\\Guan and Loew - 2018 - Breast cancer detection using synthetic mammograms.pdf:application/pdf},
}

@article{ciritsisDeterminationMammographicBreast2018,
	title = {Determination of mammographic breast density using a deep convolutional neural network},
	issn = {0007-1285, 1748-880X},
	url = {https://www.birpublications.org/doi/10.1259/bjr.20180691},
	doi = {10.1259/bjr.20180691},
	language = {en},
	urldate = {2023-05-24},
	journal = {The British Journal of Radiology},
	author = {Ciritsis, Alexander and Rossi, Cristina and Vittoria De Martini, Ilaria and Eberhard, Matthias and Marcon, Magda and Becker, Anton S. and Berger, Nicole and Boss, Andreas},
	month = oct,
	year = {2018},
	pages = {20180691},
	file = {Ciritsis et al. - 2018 - Determination of mammographic breast density using.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KUAC24C9\\Ciritsis et al. - 2018 - Determination of mammographic breast density using.pdf:application/pdf},
}

@article{shenDeepLearningImprove2019a,
	title = {Deep {Learning} to {Improve} {Breast} {Cancer} {Detection} on {Screening} {Mammography}},
	volume = {9},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-019-48995-4},
	doi = {10.1038/s41598-019-48995-4},
	abstract = {Abstract
            
              The rapid development of deep learning, a family of machine learning techniques, has spurred much interest in its application to medical imaging problems. Here, we develop a deep learning algorithm that can accurately detect breast cancer on screening mammograms using an “end-to-end” training approach that efficiently leverages training datasets with either complete clinical annotation or only the cancer status (label) of the whole image. In this approach, lesion annotations are required only in the initial training stage, and subsequent stages require only image-level labels, eliminating the reliance on rarely available lesion annotations. Our all convolutional network method for classifying screening mammograms attained excellent performance in comparison with previous methods. On an independent test set of digitized film mammograms from the Digital Database for Screening Mammography (CBIS-DDSM), the best single model achieved a per-image AUC of 0.88, and four-model averaging improved the AUC to 0.91 (sensitivity: 86.1\%, specificity: 80.1\%). On an independent test set of full-field digital mammography (FFDM) images from the INbreast database, the best single model achieved a per-image AUC of 0.95, and four-model averaging improved the AUC to 0.98 (sensitivity: 86.7\%, specificity: 96.1\%). We also demonstrate that a whole image classifier trained using our end-to-end approach on the CBIS-DDSM digitized film mammograms can be transferred to INbreast FFDM images using only a subset of the INbreast data for fine-tuning and without further reliance on the availability of lesion annotations. These findings show that automatic deep learning methods can be readily trained to attain high accuracy on heterogeneous mammography platforms, and hold tremendous promise for improving clinical tools to reduce false positive and false negative screening mammography results. Code and model available at:
              https://github.com/lishen/end2end-all-conv
              .},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {Scientific Reports},
	author = {Shen, Li and Margolies, Laurie R. and Rothstein, Joseph H. and Fluder, Eugene and McBride, Russell and Sieh, Weiva},
	month = aug,
	year = {2019},
	pages = {12495},
	file = {Shen et al. - 2019 - Deep Learning to Improve Breast Cancer Detection o.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\N5X6I43T\\Shen et al. - 2019 - Deep Learning to Improve Breast Cancer Detection o.pdf:application/pdf},
}

@article{arefanDeepLearningModeling2020,
	title = {Deep learning modeling using normal mammograms for predicting breast cancer risk},
	volume = {47},
	issn = {0094-2405, 2473-4209},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/mp.13886},
	doi = {10.1002/mp.13886},
	abstract = {Purpose: To investigate two deep learning-based modeling schemes for predicting short-term risk of developing breast cancer using prior normal screening digital mammograms in a case-control setting.
Methods: We conducted a retrospective Institutional Review Board-approved study on a case-control cohort of 226 patients (including 113 women diagnosed with breast cancer and 113 controls) who underwent general population breast cancer screening. For each patient, a prior normal (i.e., with negative or benign findings) digital mammogram examination [including mediolateral oblique (MLO) view and craniocaudal (CC) view two images] was collected. Thus, a total of 452 normal images (226 MLO view images and 226 CC view images) of this case-control cohort were analyzed to predict the outcome, i.e., developing breast cancer (cancer cases) or remaining breast cancer-free (controls) within the follow-up period. We implemented an end-to-end deep learning model and a GoogLeNet-LDA model and compared their effects in several experimental settings using two mammographic view images and inputting two different subregions of the images to the models. The proposed models were also compared to logistic regression modeling of mammographic breast density. Area under the receiver operating characteristic curve (AUC) was used as the model performance metric.
Results: The highest AUC was 0.73 [95\% Confidence Interval (CI): 0.68–0.78; GoogLeNet-LDA model on CC view] when using the whole-breast and was 0.72 (95\% CI: 0.67–0.76; GoogLeNetLDA model on MLO + CC view) when using the dense tissue, respectively, as the model input. The GoogleNet-LDA model significantly (all P {\textless} 0.05) outperformed the end-to-end GoogLeNet model in all experiments. CC view was consistently more predictive than MLO view in both deep learning models, regardless of the input subregions. Both models exhibited superior performance than the percent breast density (AUC = 0.54; 95\% CI: 0.49–0.59).
Conclusions: The proposed deep learning modeling approach can predict short-term breast cancer risk using normal screening mammogram images. Larger studies are needed to further reveal the promise of deep learning in enhancing imaging-based breast cancer risk assessment. © 2019 American Association of Physicists in Medicine [https://doi.org/10.1002/mp.13886]},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {Medical Physics},
	author = {Arefan, Dooman and Mohamed, Aly A. and Berg, Wendie A. and Zuley, Margarita L. and Sumkin, Jules H. and Wu, Shandong},
	month = jan,
	year = {2020},
	pages = {110--118},
	file = {Arefan et al. - 2020 - Deep learning modeling using normal mammograms for.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3XIZ3FA8\\Arefan et al. - 2020 - Deep learning modeling using normal mammograms for.pdf:application/pdf},
}

@article{wuDeepNeuralNetworks2020a,
	title = {Deep {Neural} {Networks} {Improve} {Radiologists}’ {Performance} in {Breast} {Cancer} {Screening}},
	volume = {39},
	issn = {0278-0062, 1558-254X},
	url = {https://ieeexplore.ieee.org/document/8861376/},
	doi = {10.1109/TMI.2019.2945514},
	language = {en},
	number = {4},
	urldate = {2023-05-24},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Wu, Nan and Phang, Jason and Park, Jungkyu and Shen, Yiqiu and Huang, Zhe and Zorin, Masha and Jastrzebski, Stanislaw and Fevry, Thibault and Katsnelson, Joe and Kim, Eric and Wolfson, Stacey and Parikh, Ujas and Gaddam, Sushma and Lin, Leng Leng Young and Ho, Kara and Weinstein, Joshua D. and Reig, Beatriu and Gao, Yiming and Toth, Hildegard and Pysarenko, Kristine and Lewin, Alana and Lee, Jiyon and Airola, Krystal and Mema, Eralda and Chung, Stephanie and Hwang, Esther and Samreen, Naziya and Kim, S. Gene and Heacock, Laura and Moy, Linda and Cho, Kyunghyun and Geras, Krzysztof J.},
	month = apr,
	year = {2020},
	pages = {1184--1194},
	file = {Wu et al. - 2020 - Deep Neural Networks Improve Radiologists’ Perform.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FYG3SIXX\\Wu et al. - 2020 - Deep Neural Networks Improve Radiologists’ Perform.pdf:application/pdf},
}

@misc{liangJoint2D3DBreast2020,
	title = {Joint {2D}-{3D} {Breast} {Cancer} {Classification}},
	url = {http://arxiv.org/abs/2002.12392},
	abstract = {Breast cancer is the malignant tumor that causes the highest number of cancer deaths in females. Digital mammograms (DM or 2D mammogram) and digital breast tomosynthesis (DBT or 3D mammogram) are the two types of mammography imagery that are used in clinical practice for breast cancer detection and diagnosis. Radiologists usually read both imaging modalities in combination; however, existing computer-aided diagnosis tools are designed using only one imaging modality. Inspired by clinical practice, we propose an innovative convolutional neural network (CNN) architecture for breast cancer classiﬁcation, which uses both 2D and 3D mammograms, simultaneously. Our experiment shows that the proposed method signiﬁcantly improves the performance of breast cancer classiﬁcation. By assembling three CNN classiﬁers, the proposed model achieves 0.97 AUC, which is 34.72\% higher than the methods using only one imaging modality.},
	language = {en},
	urldate = {2023-05-24},
	publisher = {arXiv},
	author = {Liang, Gongbo and Wang, Xiaoqin and Zhang, Yu and Xing, Xin and Blanton, Hunter and Salem, Tawfiq and Jacobs, Nathan},
	month = feb,
	year = {2020},
	note = {arXiv:2002.12392 [cs, eess, q-bio]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Quantitative Biology - Quantitative Methods, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Liang et al. - 2020 - Joint 2D-3D Breast Cancer Classification.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IW9VUSP5\\Liang et al. - 2020 - Joint 2D-3D Breast Cancer Classification.pdf:application/pdf},
}

@incollection{al-antariDeepLearningComputerAided2020,
	address = {Cham},
	title = {Deep {Learning} {Computer}-{Aided} {Diagnosis} for {Breast} {Lesion} in {Digital} {Mammogram}},
	volume = {1213},
	isbn = {978-3-030-33127-6 978-3-030-33128-3},
	url = {http://link.springer.com/10.1007/978-3-030-33128-3_4},
	language = {en},
	urldate = {2023-05-24},
	booktitle = {Deep {Learning} in {Medical} {Image} {Analysis}},
	publisher = {Springer International Publishing},
	author = {Al-antari, Mugahed A. and Al-masni, Mohammed A. and Kim, Tae-Seong},
	editor = {Lee, Gobert and Fujita, Hiroshi},
	year = {2020},
	doi = {10.1007/978-3-030-33128-3_4},
	note = {Series Title: Advances in Experimental Medicine and Biology},
	pages = {59--72},
	file = {Al-antari et al. - 2020 - Deep Learning Computer-Aided Diagnosis for Breast .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\B9RGSFQP\\Al-antari et al. - 2020 - Deep Learning Computer-Aided Diagnosis for Breast .pdf:application/pdf},
}

@article{shuDeepNeuralNetworks2020,
	title = {Deep {Neural} {Networks} {With} {Region}-{Based} {Pooling} {Structures} for {Mammographic} {Image} {Classification}},
	volume = {39},
	issn = {0278-0062, 1558-254X},
	url = {https://ieeexplore.ieee.org/document/8964266/},
	doi = {10.1109/TMI.2020.2968397},
	abstract = {Breast cancer is one of the most frequently diagnosed solid cancers. Mammography is the most commonly used screening technology for detecting breast cancer. Traditional machine learning methods of mammographic image classiﬁcation or segmentation using manual features require a great quantity of manual segmentation annotation data to train the model and test the results. But manual labeling is expensive, time-consuming, and laborious, and greatly increases the cost of system construction. To reduce this cost and the workload of radiologists, an end-to-end full-image mammogram classiﬁcation method based on deep neural networks was proposed for classiﬁer building, which can be constructed without bounding boxes or mask ground truth label of training data. The only label required in this method is the classiﬁcation of mammographic images, which can be relatively easy to collect from diagnostic reports. Because breast lesions usually take up a fraction of the total area visualized in the mammographic image, we propose different pooling structures for convolutional neural networks(CNNs) instead of the common pooling methods, which divide the image into regions and select the few with high probability of malignancy as the representation of the whole mammographic image. The proposed pooling structures can be applied on most CNN-based models, which may greatly improve the models’ performance on mammographic image data with the same input. Experimental results on the publicly available INbreast dataset and CBIS dataset indicate that the proposed pooling structures perform satisfactorily on mammographic image data compared with previous stateof-the-art mammographic image classiﬁers and detection algorithm using segmentation annotations.},
	language = {en},
	number = {6},
	urldate = {2023-05-24},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Shu, Xin and Zhang, Lei and Wang, Zizhou and Lv, Qing and Yi, Zhang},
	month = jun,
	year = {2020},
	pages = {2246--2255},
	file = {Shu et al. - 2020 - Deep Neural Networks With Region-Based Pooling Str.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CYCBS8J9\\Shu et al. - 2020 - Deep Neural Networks With Region-Based Pooling Str.pdf:application/pdf},
}

@article{liClassificationBreastMass2021,
	title = {Classification of breast mass in two‐view mammograms via deep learning},
	volume = {15},
	issn = {1751-9659, 1751-9667},
	url = {https://onlinelibrary.wiley.com/doi/10.1049/ipr2.12035},
	doi = {10.1049/ipr2.12035},
	abstract = {Breast cancer is the second deadliest cancer among women. Mammography is an important method for physicians to diagnose breast cancer. The main purpose of this study is to use deep learning to automatically classify breast masses in mammograms into benign and malignant. This study proposes a two-view mammograms classiﬁcation model consisting of convolutional neural network (CNN) and recurrent neural network (RNN), which is used to classify benign and malignant breast masses. The model is composed of two branch networks, and two modiﬁed ResNet are used to extract breast-mass features of mammograms from craniocaudal (CC) view and mediolateral oblique (MLO) view, respectively. In order to effectively utilise the spatial relationship of the two-view mammograms, gate recurrent unit (GRU) structures of RNN is used to fuse the features of the breast mass from the two-view. The digital database for screening mammography (DDSM) be used for training and testing our model. The experimental results show that the classiﬁcation accuracy, recall and area under curve (AUC) of our method reach 0.947, 0.941 and 0.968, respectively. Compared with previous studies, our method has signiﬁcantly improved the performance of benign and malignant classiﬁcation.},
	language = {en},
	number = {2},
	urldate = {2023-05-24},
	journal = {IET Image Processing},
	author = {Li, Hua and Niu, Jing and Li, Dengao and Zhang, Chen},
	month = feb,
	year = {2021},
	pages = {454--467},
	file = {Li et al. - 2021 - Classification of breast mass in two‐view mammogra.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QCRV5ZBX\\Li et al. - 2021 - Classification of breast mass in two‐view mammogra.pdf:application/pdf},
}

@article{zeiserSegmentationMassesMammograms2020,
	title = {Segmentation of {Masses} on {Mammograms} {Using} {Data} {Augmentation} and {Deep} {Learning}},
	volume = {33},
	issn = {0897-1889, 1618-727X},
	url = {http://link.springer.com/10.1007/s10278-020-00330-4},
	doi = {10.1007/s10278-020-00330-4},
	abstract = {The diagnosis of breast cancer in early stage is essential for successful treatment. Detection can be performed in several ways, the most common being through mammograms. The projections acquired by this type of examination are directly affected by the composition of the breast, which density can be similar to the suspicious masses, being a challenge the identification of malignant lesions. In this article, we propose a computer-aided detection (CAD) system to aid in the diagnosis of masses in digitized mammograms using a model based in the U-Net, allowing specialists to monitor the lesion over time. Unlike most of the studies, we propose the use of an entire base of digitized mammograms using normal, benign, and malignant cases. Our research is divided into four stages: (1) pre-processing, with the removal of irrelevant information, enhancement of the contrast of 7989 images of the Digital Database for Screening Mammography (DDSM), and obtaining regions of interest. (2) Data augmentation, with horizontal mirroring, zooming, and resizing of images; (3) training, with tests of six-based U-Net models, with different characteristics; (4) testing, evaluating four metrics, accuracy, sensitivity, specificity, and Dice Index. The tested models obtained different results regarding the assessed parameters. The best model achieved a sensitivity of 92.32\%, specificity of 80.47\%, accuracy of 85.95\% Dice Index of 79.39\%, and AUC of 86.40\%. Even using a full base without case selection bias, the results obtained demonstrate that the use of a complete database can provide knowledge to the CAD expert.},
	language = {en},
	number = {4},
	urldate = {2023-05-24},
	journal = {Journal of Digital Imaging},
	author = {Zeiser, Felipe André and Da Costa, Cristiano André and Zonta, Tiago and Marques, Nuno M. C. and Roehe, Adriana Vial and Moreno, Marcelo and Da Rosa Righi, Rodrigo},
	month = aug,
	year = {2020},
	pages = {858--868},
	file = {Zeiser et al. - 2020 - Segmentation of Masses on Mammograms Using Data Au.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4TLHXLMS\\Zeiser et al. - 2020 - Segmentation of Masses on Mammograms Using Data Au.pdf:application/pdf},
}

@article{ramadanUsingConvolutionalNeural2020,
	title = {Using {Convolutional} {Neural} {Network} with {Cheat} {Sheet} and {Data} {Augmentation} to {Detect} {Breast} {Cancer} in {Mammograms}},
	volume = {2020},
	issn = {1748-6718, 1748-670X},
	url = {https://www.hindawi.com/journals/cmmm/2020/9523404/},
	doi = {10.1155/2020/9523404},
	abstract = {The American Cancer Society expected to diagnose 276,480 new cases of invasive breast cancer in the USA and 48,530 new cases of noninvasive breast cancer among women in 2020. Early detection of breast cancer, followed by appropriate treatment, can reduce the risk of death from this disease. DL through CNN can assist imaging specialists in classifying the mammograms accurately. Accurate classification of mammograms using CNN needs a well-trained CNN by a large number of labeled mammograms. Unfortunately, a large number of labeled mammograms are not always available. In this study, a novel procedure to aid imaging specialists in detecting normal and abnormal mammograms has been proposed. The procedure supplied the designed CNN with a cheat sheet for some classical attributes extracted from the ROI and an extra number of labeled mammograms through data augmentation. The cheat sheet aided the CNN through encoding easy-to-recognize artificial patterns in the mammogram before passing it to the CNN, and the data augmentation supported the CNN with more labeled data points. Fifteen runs of 4 different modified datasets taken from the MIAS dataset were conducted and analyzed. The results showed that the cheat sheet, along with data augmentation, enhanced CNN’s accuracy by at least 12.2\% and enhanced the precision of the CNN by at least 2.2. The mean accuracy, sensitivity, and specificity obtained using the proposed procedure were 92.1, 91.4, and 96.8, respectively, while the average area under the ROC curve was 94.9.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Computational and Mathematical Methods in Medicine},
	author = {Ramadan, Saleem Z.},
	editor = {Fantacci, Maria E.},
	month = oct,
	year = {2020},
	pages = {1--9},
	file = {Ramadan - 2020 - Using Convolutional Neural Network with Cheat Shee.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ICERBTYT\\Ramadan - 2020 - Using Convolutional Neural Network with Cheat Shee.pdf:application/pdf},
}

@article{sunAUNetAttentionguidedDenseupsampling2020a,
	title = {{AUNet}: attention-guided dense-upsampling networks for breast mass segmentation in whole mammograms},
	volume = {65},
	issn = {1361-6560},
	shorttitle = {{AUNet}},
	url = {https://iopscience.iop.org/article/10.1088/1361-6560/ab5745},
	doi = {10.1088/1361-6560/ab5745},
	abstract = {Mammography is one of the most commonly applied tools for early breast cancer screening. Automatic segmentation of breast masses in mammograms is essential but challenging due to the low signal-to-noise ratio and the wide variety of mass shapes and sizes. Existing methods deal with these challenges mainly by extracting mass-centered image patches manually or automatically. However, manual patch extraction is time-consuming and automatic patch extraction brings errors that could not be compensated in the following segmentation step. In this study, we propose a novel attention-guided dense-upsampling network (AUNet) for accurate breast mass segmentation in whole mammograms directly. In AUNet, we employ an asymmetrical encoder–decoder structure and propose an effective upsampling block, attention-guided dense-upsampling block (AU block). Especially, the AU block is designed to have three merits. Firstly, it compensates the information loss of bilinear upsampling by dense upsampling. Secondly, it designs a more effective method to fuse high- and low-level features. Thirdly, it includes a channel-attention function to highlight rich-information channels. We evaluated the proposed method on two publicly available datasets, CBIS-DDSM and INbreast. Compared to three state-of-the-art fully convolutional networks, AUNet achieved the best performances with an average Dice similarity coefficient of 81.8\% for CBIS-DDSM and 79.1\% for INbreast.},
	language = {en},
	number = {5},
	urldate = {2023-05-24},
	journal = {Physics in Medicine \& Biology},
	author = {Sun, Hui and Li, Cheng and Liu, Boqiang and Liu, Zaiyi and Wang, Meiyun and Zheng, Hairong and Dagan Feng, David and Wang, Shanshan},
	month = feb,
	year = {2020},
	pages = {055005},
	file = {Sun et al. - 2020 - AUNet attention-guided dense-upsampling networks .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WAYUQ33V\\Sun et al. - 2020 - AUNet attention-guided dense-upsampling networks .pdf:application/pdf},
}

@article{wuDeepNeuralNetworks2020b,
	title = {Deep {Neural} {Networks} {Improve} {Radiologists}’ {Performance} in {Breast} {Cancer} {Screening}},
	volume = {39},
	issn = {0278-0062, 1558-254X},
	url = {https://ieeexplore.ieee.org/document/8861376/},
	doi = {10.1109/TMI.2019.2945514},
	language = {en},
	number = {4},
	urldate = {2023-05-24},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Wu, Nan and Phang, Jason and Park, Jungkyu and Shen, Yiqiu and Huang, Zhe and Zorin, Masha and Jastrzebski, Stanislaw and Fevry, Thibault and Katsnelson, Joe and Kim, Eric and Wolfson, Stacey and Parikh, Ujas and Gaddam, Sushma and Lin, Leng Leng Young and Ho, Kara and Weinstein, Joshua D. and Reig, Beatriu and Gao, Yiming and Toth, Hildegard and Pysarenko, Kristine and Lewin, Alana and Lee, Jiyon and Airola, Krystal and Mema, Eralda and Chung, Stephanie and Hwang, Esther and Samreen, Naziya and Kim, S. Gene and Heacock, Laura and Moy, Linda and Cho, Kyunghyun and Geras, Krzysztof J.},
	month = apr,
	year = {2020},
	pages = {1184--1194},
	file = {Wu et al. - 2020 - Deep Neural Networks Improve Radiologists’ Perform.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\AKNVM34B\\Wu et al. - 2020 - Deep Neural Networks Improve Radiologists’ Perform.pdf:application/pdf},
}

@incollection{al-antariDeepLearningComputerAided2020a,
	address = {Cham},
	title = {Deep {Learning} {Computer}-{Aided} {Diagnosis} for {Breast} {Lesion} in {Digital} {Mammogram}},
	volume = {1213},
	isbn = {978-3-030-33127-6 978-3-030-33128-3},
	url = {http://link.springer.com/10.1007/978-3-030-33128-3_4},
	language = {en},
	urldate = {2023-05-24},
	booktitle = {Deep {Learning} in {Medical} {Image} {Analysis}},
	publisher = {Springer International Publishing},
	author = {Al-antari, Mugahed A. and Al-masni, Mohammed A. and Kim, Tae-Seong},
	editor = {Lee, Gobert and Fujita, Hiroshi},
	year = {2020},
	doi = {10.1007/978-3-030-33128-3_4},
	note = {Series Title: Advances in Experimental Medicine and Biology},
	pages = {59--72},
	file = {Al-antari et al. - 2020 - Deep Learning Computer-Aided Diagnosis for Breast .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IKJUY8ZJ\\Al-antari et al. - 2020 - Deep Learning Computer-Aided Diagnosis for Breast .pdf:application/pdf},
}

@inproceedings{zhaoCrossViewAttentionNetwork2020a,
	address = {Barcelona, Spain},
	title = {Cross-{View} {Attention} {Network} for {Breast} {Cancer} {Screening} from {Multi}-{View} {Mammograms}},
	isbn = {978-1-5090-6631-5},
	url = {https://ieeexplore.ieee.org/document/9054612/},
	doi = {10.1109/ICASSP40776.2020.9054612},
	abstract = {In this paper, we address the problem of breast caner detection from multi-view mammograms. We present a novel cross-view attention module (CvAM) which implicitly learns to focus on the cancerrelated local abnormal regions and highlighting salient features by exploring cross-view information among four views of a screening mammography exam, e.g. asymmetries between left and right breasts and lesion correspondence between two views of the same breast. More speciﬁcally, the proposed CvAM calculates spatial attention maps based on the same view of different breasts to enhance bilateral asymmetric regions, and channel attention maps based on two different views of the same breast to enhance the feature channels corresponding to the same lesion in a single breast. CvAMs can be easily integrated into standard convolutional neural networks (CNN) architectures such as ResNet to form a multi-view classiﬁcation model. Experiments are conducted on DDSM dataset, and results show that CvAMs can not only provide better classiﬁcation accuracy over non-attention and single-view attention models, but also demonstrate better abnormality localization power using CNN visualization tools.},
	language = {en},
	urldate = {2023-05-24},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Zhao, Xuran and Yu, Luyang and Wang, Xun},
	month = may,
	year = {2020},
	pages = {1050--1054},
	file = {Zhao et al. - 2020 - Cross-View Attention Network for Breast Cancer Scr.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UMN8BK8W\\Zhao et al. - 2020 - Cross-View Attention Network for Breast Cancer Scr.pdf:application/pdf},
}

@misc{liangJoint2D3DBreast2020a,
	title = {Joint {2D}-{3D} {Breast} {Cancer} {Classification}},
	url = {http://arxiv.org/abs/2002.12392},
	abstract = {Breast cancer is the malignant tumor that causes the highest number of cancer deaths in females. Digital mammograms (DM or 2D mammogram) and digital breast tomosynthesis (DBT or 3D mammogram) are the two types of mammography imagery that are used in clinical practice for breast cancer detection and diagnosis. Radiologists usually read both imaging modalities in combination; however, existing computer-aided diagnosis tools are designed using only one imaging modality. Inspired by clinical practice, we propose an innovative convolutional neural network (CNN) architecture for breast cancer classiﬁcation, which uses both 2D and 3D mammograms, simultaneously. Our experiment shows that the proposed method signiﬁcantly improves the performance of breast cancer classiﬁcation. By assembling three CNN classiﬁers, the proposed model achieves 0.97 AUC, which is 34.72\% higher than the methods using only one imaging modality.},
	language = {en},
	urldate = {2023-05-24},
	publisher = {arXiv},
	author = {Liang, Gongbo and Wang, Xiaoqin and Zhang, Yu and Xing, Xin and Blanton, Hunter and Salem, Tawfiq and Jacobs, Nathan},
	month = feb,
	year = {2020},
	note = {arXiv:2002.12392 [cs, eess, q-bio]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Quantitative Biology - Quantitative Methods, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Liang et al. - 2020 - Joint 2D-3D Breast Cancer Classification.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QUPBDQQH\\Liang et al. - 2020 - Joint 2D-3D Breast Cancer Classification.pdf:application/pdf},
}

@misc{wuSynthesizingLesionsUsing2020,
	title = {Synthesizing lesions using contextual {GANs} improves breast cancer classification on mammograms},
	url = {http://arxiv.org/abs/2006.00086},
	abstract = {Data scarcity and class imbalance are two fundamental challenges in many machine learning applications to healthcare. Breast cancer classiﬁcation in mammography exempliﬁes these challenges, with a malignancy rate of around 0.5\% in a screening population, which is compounded by the relatively small size of lesions (∼1\% of the image) in malignant cases. Simultaneously, the prevalence of screening mammography creates a potential abundance of non-cancer exams to use for training. Altogether, these characteristics lead to overﬁtting on cancer cases, while under-utilizing non-cancer data. Here, we present a novel generative adversarial network (GAN) model for data augmentation that can realistically synthesize and remove lesions on mammograms. With self-attention and semi-supervised learning components, the U-net-based architecture can generate high resolution (256x256px) outputs, as necessary for mammography. When augmenting the original training set with the GAN-generated samples, we ﬁnd a signiﬁcant improvement in malignancy classiﬁcation performance on a test set of real mammogram patches. Overall, the empirical results of our algorithm and the relevance to other medical imaging paradigms point to potentially fruitful further applications.},
	language = {en},
	urldate = {2023-05-24},
	publisher = {arXiv},
	author = {Wu, Eric and Wu, Kevin and Lotter, William},
	month = may,
	year = {2020},
	note = {arXiv:2006.00086 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Wu et al. - 2020 - Synthesizing lesions using contextual GANs improve.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\S7GTBN6E\\Wu et al. - 2020 - Synthesizing lesions using contextual GANs improve.pdf:application/pdf},
}

@article{chaEvaluationDataAugmentation2019,
	title = {Evaluation of data augmentation via synthetic images for improved breast mass detection on mammograms using deep learning},
	volume = {7},
	issn = {2329-4302},
	url = {https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-7/issue-01/012703/Evaluation-of-data-augmentation-via-synthetic-images-for-improved-breast/10.1117/1.JMI.7.1.012703.full},
	doi = {10.1117/1.JMI.7.1.012703},
	language = {en},
	number = {01},
	urldate = {2023-05-24},
	journal = {Journal of Medical Imaging},
	author = {Cha, Kenny H. and Petrick, Nicholas and Pezeshk, Aria and Graff, Christian G. and Sharma, Diksha and Badal, Andreu and Sahiner, Berkman},
	month = nov,
	year = {2019},
	pages = {1},
	file = {Cha et al. - 2019 - Evaluation of data augmentation via synthetic imag.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VUNQV8CS\\Cha et al. - 2019 - Evaluation of data augmentation via synthetic imag.pdf:application/pdf},
}

@article{sonPredictionBreastCancer2020,
	title = {Prediction of breast cancer molecular subtypes using radiomics signatures of synthetic mammography from digital breast tomosynthesis},
	volume = {10},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-020-78681-9},
	doi = {10.1038/s41598-020-78681-9},
	abstract = {Abstract
            We aimed to predict molecular subtypes of breast cancer using radiomics signatures extracted from synthetic mammography reconstructed from digital breast tomosynthesis (DBT). A total of 365 patients with invasive breast cancer with three different molecular subtypes (luminal A + B, luminal; HER2-positive, HER2; triple-negative, TN) were assigned to the training set and temporally independent validation cohort. A total of 129 radiomics features were extracted from synthetic mammograms. The radiomics signature was built using the elastic-net approach. Clinical features included patient age, lesion size and image features assessed by radiologists. In the validation cohort, the radiomics signature yielded an AUC of 0.838, 0.556, and 0.645 for the TN, HER2 and luminal subtypes, respectively. In a multivariate analysis, the radiomics signature was the only independent predictor of the molecular subtype. The combination of the radiomics signature and clinical features showed significantly higher AUC values than clinical features only for distinguishing the TN subtype. In conclusion, the radiomics signature showed high performance for distinguishing TN breast cancer. Radiomics signatures may serve as biomarkers for TN breast cancer and may help to determine the direction of treatment for these patients.},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {Scientific Reports},
	author = {Son, Jinwoo and Lee, Si Eun and Kim, Eun-Kyung and Kim, Sungwon},
	month = dec,
	year = {2020},
	pages = {21566},
	file = {Son et al. - 2020 - Prediction of breast cancer molecular subtypes usi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\HM5UUZMD\\Son et al. - 2020 - Prediction of breast cancer molecular subtypes usi.pdf:application/pdf},
}

@article{ragabFrameworkBreastCancer2021,
	title = {A framework for breast cancer classification using {Multi}-{DCNNs}},
	volume = {131},
	issn = {00104825},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010482521000391},
	doi = {10.1016/j.compbiomed.2021.104245},
	abstract = {Background: Deep learning (DL) is the fastest-growing field of machine learning (ML). Deep convolutional neural networks (DCNN) are currently the main tool used for image analysis and classification purposes. There are several DCNN architectures among them AlexNet, GoogleNet, and residual networks (ResNet).},
	language = {en},
	urldate = {2023-05-24},
	journal = {Computers in Biology and Medicine},
	author = {Ragab, Dina A. and Attallah, Omneya and Sharkas, Maha and Ren, Jinchang and Marshall, Stephen},
	month = apr,
	year = {2021},
	pages = {104245},
	file = {Ragab et al. - 2021 - A framework for breast cancer classification using.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MNVAEFYN\\Ragab et al. - 2021 - A framework for breast cancer classification using.pdf:application/pdf},
}

@article{liSAPCGANAdversarial2021,
	title = {{SAP}‐{cGAN}: {Adversarial} learning for breast mass segmentation in digital mammogram based on superpixel average pooling},
	volume = {48},
	issn = {0094-2405, 2473-4209},
	shorttitle = {{SAP}‐{cGAN}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/mp.14671},
	doi = {10.1002/mp.14671},
	abstract = {Purpose: Breast mass segmentation is a prerequisite step in the use of computer-aided tools designed for breast cancer diagnosis and treatment planning. However, mass segmentation remains challenging due to the low contrast, irregular shapes, and fuzzy boundaries of masses. In this work, we propose a mammography mass segmentation model for improving segmentation performance.
Methods: We propose a mammography mass segmentation model called SAP-cGAN, which is based on an improved conditional generative adversarial network (cGAN). We introduce a superpixel average pooling layer into the cGAN decoder, which utilizes superpixels as a pooling layout to improve boundary segmentation. In addition, we adopt a multiscale input strategy to enable the network to learn scale-invariant features with increased robustness. The performance of the model is evaluated with two public datasets: CBIS-DDSM and INbreast. Moreover, ablation analysis is conducted to evaluate further the individual contribution of each block to the performance of the network.
Results: Dice and Jaccard scores of 93.37\% and 87.57\%, respectively, are obtained for the CBISDDSM dataset. The Dice and Jaccard scores for the INbreast dataset are 91.54\% and 84.40\%, respectively. These results indicate that our proposed model outperforms current state-of-the-art breast mass segmentation methods. The superpixel average pooling layer and multiscale input strategy has improved the Dice and Jaccard scores of the original cGAN by 7.8\% and 12.79\%, respectively.
Conclusions: Adversarial learning with the addition of a superpixel average pooling layer and multiscale input strategy can encourage the Generator network to generate masks with increased realism and improve breast mass segmentation performance through the minimax game between the Generator network and Discriminator network. © 2020 American Association of Physicists in Medicine [https://doi.org/10.1002/mp.14671]},
	language = {en},
	number = {3},
	urldate = {2023-05-24},
	journal = {Medical Physics},
	author = {Li, Yamei and Zhao, Guohua and Zhang, Qian and Lin, Yusong and Wang, Meiyun},
	month = mar,
	year = {2021},
	pages = {1157--1167},
	file = {Li et al. - 2021 - SAP‐cGAN Adversarial learning for breast mass seg.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZTNYQPWU\\Li et al. - 2021 - SAP‐cGAN Adversarial learning for breast mass seg.pdf:application/pdf},
}

@article{oyeladeDeepLearningModel2021,
	title = {A deep learning model using data augmentation for detection of architectural distortion in whole and patches of images},
	volume = {65},
	issn = {17468094},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1746809420304730},
	doi = {10.1016/j.bspc.2020.102366},
	abstract = {Breast cancer is now widely known to be the second most lethal disease among women. Computer-aided detection (CAD) systems, deep learning (DL) in particular, have continued to provide significant computa­ tional solution in early detection and diagnosis of this disease. Research efforts are advancing novel approaches to improve the performance of DL-based models. Techniques such as data augmentation, varying depth of model, image quality enhancement, and choice of classifier have been proposed to improve performance in the char­ acterization of abnormalities in mammograms. However, no significant progress has been made in applying deep learning techniques to the detection of architectural distortion – a form of abnormalities in breast images. In this research, we propose a novel convolution neural network (CNN) model for the detection of architectural distortion by enhancing its performance using data augmentation technique. We also investigate the performance of the proposed model on different operations of image augmentation. Furthermore, the new model was adapted to detect images presenting the right and left breast presented in MLO and CC views. Similarly, we investigate the performance of our model under the fixed-size region of interests (ROIs) and multi-size whole images inputs. Our method was trained on 5136 ROIs from MIAS, 410 whole images from INbreast, 322 whole images from MIAS, and 55,890 ROIs from DDSM + CBS databases. Performance evaluation of the proposed model in comparison with other state-of-the-art techniques revealed that the model achieved 93.75 \% accuracy. This study has, therefore, strengthened the need to leverage data augmentation techniques to enhance the detection of archi­ tectural distortion, thereby reducing the rate of advanced cases of breast cancer.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Biomedical Signal Processing and Control},
	author = {Oyelade, Olaide N. and Ezugwu, Absalom E.},
	month = mar,
	year = {2021},
	pages = {102366},
	file = {Oyelade and Ezugwu - 2021 - A deep learning model using data augmentation for .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FKA8QKDK\\Oyelade and Ezugwu - 2021 - A deep learning model using data augmentation for .pdf:application/pdf},
}

@article{baccoucheConnectedUNetsDeepLearning2021,
	title = {Connected-{UNets}: a deep learning architecture for breast mass segmentation},
	volume = {7},
	issn = {2374-4677},
	shorttitle = {Connected-{UNets}},
	url = {https://www.nature.com/articles/s41523-021-00358-x},
	doi = {10.1038/s41523-021-00358-x},
	abstract = {Abstract
            Breast cancer analysis implies that radiologists inspect mammograms to detect suspicious breast lesions and identify mass tumors. Artificial intelligence techniques offer automatic systems for breast mass segmentation to assist radiologists in their diagnosis. With the rapid development of deep learning and its application to medical imaging challenges, UNet and its variations is one of the state-of-the-art models for medical image segmentation that showed promising performance on mammography. In this paper, we propose an architecture, called Connected-UNets, which connects two UNets using additional modified skip connections. We integrate Atrous Spatial Pyramid Pooling (ASPP) in the two standard UNets to emphasize the contextual information within the encoder–decoder network architecture. We also apply the proposed architecture on the Attention UNet (AUNet) and the Residual UNet (ResUNet). We evaluated the proposed architectures on two publically available datasets, the Curated Breast Imaging Subset of Digital Database for Screening Mammography (CBIS-DDSM) and INbreast, and additionally on a private dataset. Experiments were also conducted using additional synthetic data using the cycle-consistent Generative Adversarial Network (CycleGAN) model between two unpaired datasets to augment and enhance the images. Qualitative and quantitative results show that the proposed architecture can achieve better automatic mass segmentation with a high Dice score of 89.52\%, 95.28\%, and 95.88\% and Intersection over Union (IoU) score of 80.02\%, 91.03\%, and 92.27\%, respectively, on CBIS-DDSM, INbreast, and the private dataset.},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {npj Breast Cancer},
	author = {Baccouche, Asma and Garcia-Zapirain, Begonya and Castillo Olea, Cristian and Elmaghraby, Adel S.},
	month = dec,
	year = {2021},
	pages = {151},
	file = {Baccouche et al. - 2021 - Connected-UNets a deep learning architecture for .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JVTMT3WS\\Baccouche et al. - 2021 - Connected-UNets a deep learning architecture for .pdf:application/pdf},
}

@article{zhaoComputeraidedDiagnosticSystem2022,
	title = {A computer-aided diagnostic system for mammograms based on {YOLOv3}},
	volume = {81},
	issn = {1380-7501, 1573-7721},
	url = {https://link.springer.com/10.1007/s11042-021-10505-y},
	doi = {10.1007/s11042-021-10505-y},
	abstract = {Due to a large amount of noise in medical images, the task of detecting and classifying the lesions of mammograms remains a huge challenge. Based on the existing deep learning methods, focusing on the diversity of breast cancer lesion types, this paper proposes a computer-aided diagnosis system based on YOLOv3 (You Only Look Once version 3) convolutional neural network for mammograms. In this system, we integrate detection and multi-classification problems of breast lesions into a regression problem, thereby simultaneously accomplish the two tasks in one framework. The proposed computer-aided diagnosis system is mainly divided into three components: preprocessing part of the original mammograms, deep convolutional neural network based on YOLOv3, processing and evaluation of the network output. We use the dataset from CBIS-DDSM to train three models: general model, mass model and microcalcification model. These trained models can detect the position of the input mammograms in different situations, and then classify them into mass, microcalcification, benign, malignant, and other categories. After evaluating the performance by using test set images, the accuracy rates of the general model, mass model, and microcalcification model trained by our system reach 93.667 \%, 97.767 \%, 96.870 \% in the detection task, and 93.927 \%, 98.121 \%, 97.045 \% in the classification task. The computer-aided diagnosis system performs well in lesion detection and classification tasks with high-noise mammograms, reflecting well robustness.},
	language = {en},
	number = {14},
	urldate = {2023-05-24},
	journal = {Multimedia Tools and Applications},
	author = {Zhao, Jianhui and Chen, Tianquan and Cai, Bo},
	month = jun,
	year = {2022},
	pages = {19257--19281},
	file = {Zhao et al. - 2022 - A computer-aided diagnostic system for mammograms .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6WUXX8B3\\Zhao et al. - 2022 - A computer-aided diagnostic system for mammograms .pdf:application/pdf},
}

@article{baccoucheBreastLesionsDetection2021,
	title = {Breast {Lesions} {Detection} and {Classification} via {YOLO}-{Based} {Fusion} {Models}},
	volume = {69},
	issn = {1546-2226},
	url = {https://www.techscience.com/cmc/v69n1/42797},
	doi = {10.32604/cmc.2021.018461},
	abstract = {With recent breakthroughs in artificial intelligence, the use of deep learning models achieved remarkable advances in computer vision, ecommerce, cybersecurity, and healthcare. Particularly, numerous applications provided efficient solutions to assist radiologists for medical imaging analysis. For instance, automatic lesion detection and classification in mammograms is still considered a crucial task that requires more accurate diagnosis and precise analysis of abnormal lesions. In this paper, we propose an end-to-end system, which is based on You-Only-Look-Once (YOLO) model, to simultaneously localize and classify suspicious breast lesions from entire mammograms. The proposed system first preprocesses the raw images, then recognizes abnormal regions as breast lesions and determines their pathology classification as either mass or calcification. We evaluated the model on two publicly available datasets, with 2907 mammograms from the Curated Breast Imaging Subset of Digital Database for Screening Mammography (CBIS-DDSM) and 235 mammograms from INbreast database. We also used a privately collected dataset with 487 mammograms. Furthermore, we suggested a fusion models approach to report more precise detection and accurate classification. Our best results reached a detection accuracy rate of 95.7\%, 98.1\% and 98\% for mass lesions and 74.4\%, 71.8\% and 73.2\% for calcification lesions, respectively on CBIS-DDSM, INbreast and the private dataset.},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {Computers, Materials \& Continua},
	author = {Baccouche, Asma and Garcia-Zapirain, Begonya and Castillo Olea, Cristian and S. Elmaghraby, Adel},
	year = {2021},
	pages = {1407--1425},
	file = {Baccouche et al. - 2021 - Breast Lesions Detection and Classification via YO.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KZZWUE5A\\Baccouche et al. - 2021 - Breast Lesions Detection and Classification via YO.pdf:application/pdf},
}

@article{shenInterpretableClassifierHighresolution2021,
	title = {An interpretable classifier for high-resolution breast cancer screening images utilizing weakly supervised localization},
	volume = {68},
	issn = {13618415},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841520302723},
	doi = {10.1016/j.media.2020.101908},
	language = {en},
	urldate = {2023-05-24},
	journal = {Medical Image Analysis},
	author = {Shen, Yiqiu and Wu, Nan and Phang, Jason and Park, Jungkyu and Liu, Kangning and Tyagi, Sudarshini and Heacock, Laura and Kim, S. Gene and Moy, Linda and Cho, Kyunghyun and Geras, Krzysztof J.},
	month = feb,
	year = {2021},
	pages = {101908},
	file = {Shen et al. - 2021 - An interpretable classifier for high-resolution br.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\W4QPBDAQ\\Shen et al. - 2021 - An interpretable classifier for high-resolution br.pdf:application/pdf},
}

@article{xuARFNetAdaptiveReceptive2022,
	title = {{ARF}-{Net}: {An} {Adaptive} {Receptive} {Field} {Network} for breast mass segmentation in whole mammograms and ultrasound images},
	volume = {71},
	issn = {17468094},
	shorttitle = {{ARF}-{Net}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1746809421007758},
	doi = {10.1016/j.bspc.2021.103178},
	abstract = {UNet adopting an encoder-decoder structure has been used widely in medical image segmentation tasks for its outstanding performance. However, in our work, we find that UNet has the worse segmentation performance of small masses. The reason behind this is that the sizes of receptive fields are limited. In this work, to address this issue, we develop a novel end-to-end model, Adaptive Receptive Field Network (ARF-Net), for the precise breast mass segmentation in whole mammographic images and ultrasound images. ARF-Net composes of an encoder network and a corresponding decoder network, followed by a pixel-wise classifier. In ARF-Net, a Selective Receptive Filed Module (SRFM) is proposed to allocate the suitable sizes of receptive fields to the breast masses of different sizes. SRFM consists of a Multiple Receptive Field Module (MRFM) for generating multiple receptive fields of different sizes and a Multi-Scale Selection Module (MSSM) for selecting the suitable sizes of receptive fields based on the objects’ size. The proposed ARF-Net achieves the dice index of 86.1\%, 85.75\%, and 88.12\% on the two mammographic databases (INbreast and CBIS-DDSM) and one ultrasonic database (UDIAT), respectively. Moreover, extensive ablation experiments show that ARF-Net transcends several state-of-the-art segmentation networks, and the developed MSSM exceeds several counterparts.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Biomedical Signal Processing and Control},
	author = {Xu, Chunbo and Qi, Yunliang and Wang, Yiming and Lou, Meng and Pi, Jiande and Ma, Yide},
	month = jan,
	year = {2022},
	pages = {103178},
	file = {Xu et al. - 2022 - ARF-Net An Adaptive Receptive Field Network for b.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\I2ENBNHB\\Xu et al. - 2022 - ARF-Net An Adaptive Receptive Field Network for b.pdf:application/pdf},
}

@article{niuMultiScaleAttention2021a,
	title = {Multi‐scale attention‐based convolutional neural network for classification of breast masses in mammograms},
	volume = {48},
	issn = {0094-2405, 2473-4209},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/mp.14942},
	doi = {10.1002/mp.14942},
	abstract = {Purpose: Breast cancer is the cancer with the highest incidence in women, and early detection can effectively improve the survival rate of patients. Mammography is an important method for physicians to screening breast cancer, but the diagnosis of mammograms by physicians depends largely on clinical practice experience. Studies have shown that using computer-aided diagnosis techniques can help doctors diagnose breast cancer.
Methods: In this paper, the method of convolutional neural network is mainly used to classify benign and malignant breast masses in the mammograms. First, we use multi-scale residual networks and densely connected networks as backbone networks to extract the features of global image patches and local image patches. Second, we use the attention module named convolutional block attention module (CBAM) to improve the two feature extraction networks to enhance the network’s feature expression ability. Finally, we fuse the features of multi-scale image patches to achieve the classification of benign and malignant breast masses.
Results: In the digital database for screening mammography (DDSM) database, the accuracy, sensitivity, AUC value and corresponding standard deviation of our method are 0.9626 Æ 0.0110, 0.9719 Æ 0.0126, and 0.9576 Æ 0.0064, respectively. Compared with the commonly used ResNet (AUC = 0.8823 Æ 0.0112) and DenseNet (AUC = 0.9141 Æ 0.0085), the performance of our method has improved. In addition, we also used the INbreast database to train and validate the proposed method. The accuracy, sensitivity, AUC and corresponding standard deviations are 0.9554 Æ 0.0296, 0.9605 Æ 0.0228, and 0.9468 Æ 0.0085, respectively.
Conclusions: Compared with the previous work, our proposed method uses multi-scale image features, has better classification performance in breast mass patches classification tasks, and can effectively assist physicians in breast cancer diagnosis. © 2021 American Association of Physicists in Medicine [https://doi.org/10.1002/mp.14942]},
	language = {en},
	number = {7},
	urldate = {2023-05-24},
	journal = {Medical Physics},
	author = {Niu, Jing and Li, Hua and Zhang, Chen and Li, Dengao},
	month = jul,
	year = {2021},
	pages = {3878--3892},
	file = {Niu et al. - 2021 - Multi‐scale attention‐based convolutional neural n.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JCHYYBG5\\Niu et al. - 2021 - Multi‐scale attention‐based convolutional neural n.pdf:application/pdf},
}

@article{caoAutoDenseUNetSearchableNeural2022a,
	title = {Auto-{DenseUNet}: {Searchable} neural network architecture for mass segmentation in {3D} automated breast ultrasound},
	volume = {82},
	issn = {13618415},
	shorttitle = {Auto-{DenseUNet}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841522002250},
	doi = {10.1016/j.media.2022.102589},
	abstract = {Accurate segmentation of breast mass in 3D automated breast ultrasound (ABUS) plays an important role in breast cancer analysis. Deep convolutional networks have become a promising approach in segmenting ABUS images. However, designing an effective network architecture is time-consuming, and highly relies on specialist’s experience and prior knowledge. To address this issue, we introduce a searchable segmentation network (denoted as Auto-DenseUNet) based on the neural architecture search (NAS) to search the optimal architecture automatically for the ABUS mass segmentation task. Concretely, a novel search space is designed based on a densely connected structure to enhance the gradient and information flows throughout the network. Then, to encourage multiscale information fusion, a set of searchable multiscale aggregation nodes between the down-sampling and up-sampling parts of the network are further designed. Thus, all the operators within the dense connection structure or between any two aggregation nodes can be searched to find the optimal structure. Finally, a novel decoupled search training strategy during architecture search is also introduced to alleviate the memory limitation caused by continuous relaxation in NAS. The proposed Auto-DeseUNet method has been evaluated on our ABUS dataset with 170 volumes (from 107 patients), including 120 training volumes and 50 testing volumes split at patient level. Experimental results on testing volumes show that our searched architecture performed better than several human-designed segmentation models on the 3D ABUS mass segmentation task, indicating the effectiveness of our proposed method.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Medical Image Analysis},
	author = {Cao, Xuyang and Chen, Houjin and Li, Yanfeng and Peng, Yahui and Zhou, Yue and Cheng, Lin and Liu, Tianming and Shen, Dinggang},
	month = nov,
	year = {2022},
	pages = {102589},
	file = {Cao et al. - 2022 - Auto-DenseUNet Searchable neural network architec.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KR9UJSBJ\\Cao et al. - 2022 - Auto-DenseUNet Searchable neural network architec.pdf:application/pdf},
}

@article{baccoucheBreastLesionsDetection2021a,
	title = {Breast {Lesions} {Detection} and {Classification} via {YOLO}-{Based} {Fusion} {Models}},
	volume = {69},
	issn = {1546-2226},
	url = {https://www.techscience.com/cmc/v69n1/42797},
	doi = {10.32604/cmc.2021.018461},
	abstract = {With recent breakthroughs in artificial intelligence, the use of deep learning models achieved remarkable advances in computer vision, ecommerce, cybersecurity, and healthcare. Particularly, numerous applications provided efficient solutions to assist radiologists for medical imaging analysis. For instance, automatic lesion detection and classification in mammograms is still considered a crucial task that requires more accurate diagnosis and precise analysis of abnormal lesions. In this paper, we propose an end-to-end system, which is based on You-Only-Look-Once (YOLO) model, to simultaneously localize and classify suspicious breast lesions from entire mammograms. The proposed system first preprocesses the raw images, then recognizes abnormal regions as breast lesions and determines their pathology classification as either mass or calcification. We evaluated the model on two publicly available datasets, with 2907 mammograms from the Curated Breast Imaging Subset of Digital Database for Screening Mammography (CBIS-DDSM) and 235 mammograms from INbreast database. We also used a privately collected dataset with 487 mammograms. Furthermore, we suggested a fusion models approach to report more precise detection and accurate classification. Our best results reached a detection accuracy rate of 95.7\%, 98.1\% and 98\% for mass lesions and 74.4\%, 71.8\% and 73.2\% for calcification lesions, respectively on CBIS-DDSM, INbreast and the private dataset.},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {Computers, Materials \& Continua},
	author = {Baccouche, Asma and Garcia-Zapirain, Begonya and Castillo Olea, Cristian and S. Elmaghraby, Adel},
	year = {2021},
	pages = {1407--1425},
	file = {Baccouche et al. - 2021 - Breast Lesions Detection and Classification via YO.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\J983NZ92\\Baccouche et al. - 2021 - Breast Lesions Detection and Classification via YO.pdf:application/pdf},
}

@article{suYOLOLOGOTransformerbasedYOLO2022,
	title = {{YOLO}-{LOGO}: {A} transformer-based {YOLO} segmentation model for breast mass detection and segmentation in digital mammograms},
	volume = {221},
	issn = {01692607},
	shorttitle = {{YOLO}-{LOGO}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169260722002851},
	doi = {10.1016/j.cmpb.2022.106903},
	abstract = {Background and objective: Both mass detection and segmentation in digital mammograms play a crucial role in early breast cancer detection and treatment. Furthermore, clinical experience has shown that they are the upstream tasks of pathological classiﬁcation of breast lesions. Recent advancements in deep learning have made the analyses faster and more accurate. This study aims to develop a deep learning model architecture for breast cancer mass detection and segmentation using the mammography.
Methods: In this work we proposed a double shot model for mass detection and segmentation simultaneously using a combination of YOLO (You Only Look Once) and LOGO (Local-Global) architectures. Firstly, we adopted YoloV5L6, the state-of-the-art object detection model, to position and crop the breast mass in mammograms with a high resolution; Secondly, to balance training eﬃciency and segmentation performance, we modiﬁed the LOGO training strategy to train the whole images and cropped images on the global and local transformer branches separately. The two branches were then merged to form the ﬁnal segmentation decision.
Results: The proposed YOLO-LOGO model was tested on two independent mammography datasets (CBISDDSM and INBreast). The proposed model performs signiﬁcantly better than previous works. It achieves true positive rate 95.7\% and mean average precision 65.0\% for mass detection on CBIS-DDSM dataset. Its performance for mass segmentation on CBIS-DDSM dataset is F1-score=74.5\% and IoU=64.0\%. The similar performance trend is observed in another independent dataset INBreast as well.
Conclusions: The proposed model has a higher eﬃciency and better performance, reduces computational requirements, and improves the versatility and accuracy of computer-aided breast cancer diagnosis. Hence it has the potential to enable more assistance for doctors in early breast cancer detection and treatment, thereby reducing mortality.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Su, Yongye and Liu, Qian and Xie, Wentao and Hu, Pingzhao},
	month = jun,
	year = {2022},
	pages = {106903},
	file = {Su et al. - 2022 - YOLO-LOGO A transformer-based YOLO segmentation m.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\645MPP6Z\\Su et al. - 2022 - YOLO-LOGO A transformer-based YOLO segmentation m.pdf:application/pdf},
}

@article{haqBTSGANComputeraidedSegmentation2022a,
	title = {{BTS}-{GAN}: {Computer}-aided segmentation system for breast tumor using {MRI} and conditional adversarial networks},
	volume = {36},
	issn = {22150986},
	shorttitle = {{BTS}-{GAN}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2215098622000623},
	doi = {10.1016/j.jestch.2022.101154},
	abstract = {Breast tumor is one of the most prominent indicators for the diagnosis of breast cancer. The precise segmentation of tumors is crucial for enhancing the accuracy of breast cancer detection. A physician’s assessment of the MRI scan is time-consuming and require a lot of human effort and expertise. Furthermore, traditional medical segmentation approaches frequently need prior information or manual feature extraction, resulting in a subjective diagnosis. Therefore, the development of an automated image segmentation approach is essential for clinical applications. This work presents BTS-GAN, an automatic breast tumor segmentation process using conditional GAN (cGAN) in Magnetic Resonance Imaging (MRI) scans. First, we used an encoder-decoder deep network with skip connections between encoder and decoder for the generator to increase the localization efﬁciency. Second, we utilized a parallel dilated convolution (PDC) module to retain the features of various sizes of masses and to effectively extract information about the masses’ edges and interior texture. Third, an extra classiﬁcation-related constraint is included to the loss function of the cGAN for mitigating the hard-to-converge challenge in image-toimage (I2I) translation tasks based on classiﬁcation. The generator side of our proposed model learns to detect the tumor and construct a binary mask, while the discriminator learns to distinguish between ground truth and synthetic masks, driving the generator to produce masks as genuine as possible. The experimental results demonstrate that our BTS-GAN is more efﬁcient and reliable for breast tumor segmentation and outperform other segmentation techniques in terms of the IoU and Dice coefﬁcient on the publicly available RIDER breast cancer MRI dataset. Our proposed model achieved an average IoU and Dice scores of 77\% and 85\% respectively.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Engineering Science and Technology, an International Journal},
	author = {Haq, Imran Ul and Ali, Haider and Wang, Hong Yu and Cui, Lei and Feng, Jun},
	month = dec,
	year = {2022},
	pages = {101154},
	file = {Haq et al. - 2022 - BTS-GAN Computer-aided segmentation system for br.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6QTLHKRP\\Haq et al. - 2022 - BTS-GAN Computer-aided segmentation system for br.pdf:application/pdf},
}

@article{baccoucheEarlyDetectionClassification2022,
	title = {Early detection and classification of abnormality in prior mammograms using image-to-image translation and {YOLO} techniques},
	volume = {221},
	issn = {01692607},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169260722002668},
	doi = {10.1016/j.cmpb.2022.106884},
	abstract = {Background and Objective: Computer-aided-detection (CAD) systems have been developed to assist radiologists on ﬁnding suspicious lesions in mammogram. Deep Learning technology have recently succeeded to increase the chance of recognizing abnormality at an early stage in order to avoid unnecessary biopsies and decrease the mortality rate. In this study, we investigated the effectiveness of an end-to-end fusion model based on You-Only-Look-Once (YOLO) architecture, to simultaneously detect and classify suspicious breast lesions on digital mammograms. Four categories of cases were included: Mass, Calciﬁcation, Architectural Distortions, and Normal from a private digital mammographic database including 413 cases. For all cases, Prior mammograms (typically scanned 1 year before) were all reported as Normal, while Current mammograms were diagnosed as cancerous (conﬁrmed by biopsies) or healthy.
Methods: We propose to apply the YOLO-based fusion model to the Current mammograms for breast lesions detection and classiﬁcation. Then apply the same model retrospectively to synthetic mammograms for an early cancer prediction, where the synthetic mammograms were generated from the Prior mammograms by using the image-to-image translation models, CycleGAN and Pix2Pix.
Results: Evaluation results showed that our methodology could signiﬁcantly detect and classify breast lesions on Current mammograms with a highest rate of 93\% ± 0.118 for Mass lesions, 88\% ± 0.09 for Calciﬁcation lesions, and 95\% ± 0.06 for Architectural Distortion lesions. In addition, we reported evaluation results on Prior mammograms with a highest rate of 36\% ± 0.01 for Mass lesions, 14\% ± 0.01 for Calciﬁcation lesions, and 50\% ± 0.02 for Architectural Distortion lesions. Normal mammograms were accordingly classiﬁed with an accuracy rate of 92\% ± 0.09 and 90\% ± 0.06 respectively on Current and Prior exams.
Conclusions: Our proposed framework was ﬁrst developed to help detecting and identifying suspicious breast lesions in X-ray mammograms on their Current screening. The work was also suggested to reduce the temporal changes between pairs of Prior and follow-up screenings for early predicting the location and type of abnormalities in Prior mammogram screening. The paper presented a CAD method to assist doctors and experts to identify the risk of breast cancer presence. Overall, the proposed CAD method incorporates the advances of image processing, deep learning and image-to-image translation for a biomedical application.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Baccouche, Asma and Garcia-Zapirain, Begonya and Zheng, Yufeng and Elmaghraby, Adel S.},
	month = jun,
	year = {2022},
	pages = {106884},
	file = {Baccouche et al. - 2022 - Early detection and classification of abnormality .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MPCSJ2BI\\Baccouche et al. - 2022 - Early detection and classification of abnormality .pdf:application/pdf},
}

@article{petriniBreastCancerDiagnosis2022a,
	title = {Breast {Cancer} {Diagnosis} in {Two}-{View} {Mammography} {Using} {End}-to-{End} {Trained} {EfficientNet}-{Based} {Convolutional} {Network}},
	volume = {10},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9837037/},
	doi = {10.1109/ACCESS.2022.3193250},
	language = {en},
	urldate = {2023-05-24},
	journal = {IEEE Access},
	author = {Petrini, Daniel G. P. and Shimizu, Carlos and Roela, Rosimeire A. and Valente, Gabriel Vansuita and Folgueira, Maria Aparecida Azevedo Koike and Kim, Hae Yong},
	year = {2022},
	pages = {77723--77731},
	file = {Petrini et al. - 2022 - Breast Cancer Diagnosis in Two-View Mammography Us.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EEXDJNR4\\Petrini et al. - 2022 - Breast Cancer Diagnosis in Two-View Mammography Us.pdf:application/pdf},
}

@article{baccoucheIntegratedFrameworkBreast2022,
	title = {An integrated framework for breast mass classification and diagnosis using stacked ensemble of residual neural networks},
	volume = {12},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-022-15632-6},
	doi = {10.1038/s41598-022-15632-6},
	abstract = {Abstract
            A computer-aided diagnosis (CAD) system requires automated stages of tumor detection, segmentation, and classification that are integrated sequentially into one framework to assist the radiologists with a final diagnosis decision. In this paper, we introduce the final step of breast mass classification and diagnosis using a stacked ensemble of residual neural network (ResNet) models (i.e. ResNet50V2, ResNet101V2, and ResNet152V2). The work presents the task of classifying the detected and segmented breast masses into malignant or benign, and diagnosing the Breast Imaging Reporting and Data System (BI-RADS) assessment category with a score from 2 to 6 and the shape as oval, round, lobulated, or irregular. The proposed methodology was evaluated on two publicly available datasets, the Curated Breast Imaging Subset of Digital Database for Screening Mammography (CBIS-DDSM) and INbreast, and additionally on a private dataset. Comparative experiments were conducted on the individual models and an average ensemble of models with an XGBoost classifier. Qualitative and quantitative results show that the proposed model achieved better performance for (1) Pathology classification with an accuracy of 95.13\%, 99.20\%, and 95.88\%; (2) BI-RADS category classification with an accuracy of 85.38\%, 99\%, and 96.08\% respectively on CBIS-DDSM, INbreast, and the private dataset; and (3) shape classification with 90.02\% on the CBIS-DDSM dataset. Our results demonstrate that our proposed integrated framework could benefit from all automated stages to outperform the latest deep learning methodologies.},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {Scientific Reports},
	author = {Baccouche, Asma and Garcia-Zapirain, Begonya and Elmaghraby, Adel S.},
	month = jul,
	year = {2022},
	pages = {12259},
	file = {Baccouche et al. - 2022 - An integrated framework for breast mass classifica.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\L5RKM6KV\\Baccouche et al. - 2022 - An integrated framework for breast mass classifica.pdf:application/pdf},
}

@article{leeIdentifyingWomenMammographically2022,
	title = {Identifying {Women} {With} {Mammographically}- {Occult} {Breast} {Cancer} {Leveraging} {GAN}-{Simulated} {Mammograms}},
	volume = {41},
	issn = {0278-0062, 1558-254X},
	url = {https://ieeexplore.ieee.org/document/9525406/},
	doi = {10.1109/TMI.2021.3108949},
	abstract = {Our objective is to show the feasibility of using simulated mammograms to detect mammographicallyoccult (MO) cancer in women with dense breasts and a normal screening mammogram who could be triaged for additional screening with magnetic resonance imaging (MRI) or ultrasound. We developed a Conditional Generative Adversarial Network (CGAN) to simulate a mammogram with normal appearance using the opposite mammogram as the condition. We used a Convolutional Neural Network (CNN) trained on Radon Cumulative Distribution Transform (RCDT) processed mammograms to detect MO cancer. For training CGAN, we used screening mammograms of 1366 women. For MO cancer detection, we used screening mammograms of 333 women (97 MO cancer) with dense breasts. We simulated the right mammogram for normal controls and the cancer side for MO cancer cases. We created two RCDT images, one from a real mammogram pair and another from a real-simulated mammogram pair. We ﬁnetuned a VGG16 on resulting RCDT images to classify the women with MO cancer. We compared the classiﬁcation performance of the CNN trained on fused RCDT images, CNNFused to that of trained only on real RCDT images, CNNReal, and to that of trained only on simulated RCDT images, CNNSimulated. The test AUC for CNNFused was 0.77 with a 95\% conﬁdence interval (95CI) of [0.71, 0.83], which was statistically better (p-value {\textless} 0.02) than the CNNReal AUC of 0.70 with a 95CI of [0.64, 0.77] and CNNSimulated AUC of 0.68 with a 95CI of [0.62, 0.75]. It showed that CGAN simulated mammograms can help MO cancer detection.},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Lee, Juhun and Nishikawa, Robert M.},
	month = jan,
	year = {2022},
	pages = {225--236},
	file = {Lee and Nishikawa - 2022 - Identifying Women With Mammographically- Occult Br.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\N3VKFYTG\\Lee and Nishikawa - 2022 - Identifying Women With Mammographically- Occult Br.pdf:application/pdf},
}

@misc{lopezHypercomplexNeuralArchitectures2022,
	title = {Hypercomplex {Neural} {Architectures} for {Multi}-{View} {Breast} {Cancer} {Classification}},
	url = {http://arxiv.org/abs/2204.05798},
	abstract = {Traditionally, deep learning-based methods for breast cancer classiﬁcation perform a single-view analysis. However, radiologists simultaneously analyze all four views that compose a mammography exam, owing to the correlations contained in mammography views, which present crucial information for identifying tumors. In light of this, some studies have started to propose multi-view methods. Nevertheless, in such existing architectures, mammogram views are processed as independent images by separate convolutional branches, thus losing correlations among them. To overcome such limitations, in this paper we propose a novel approach for multi-view breast cancer classiﬁcation based on parameterized hypercomplex neural networks. Thanks to hypercomplex algebra properties, our networks are able to model, and thus leverage, existing correlations between the different views that comprise a mammogram exam, thus mimicking the reading process performed by clinicians. As a consequence, the proposed method is able to handle the information of a patient altogether without breaking the multi-view nature of the exam. Starting from the proposed hypercomplex approach, we deﬁne architectures designed to process two-view exams, namely PHResNets, and four-view exams, i.e., PHYSEnet and PHYSBOnet, with the ability to grasp inter-view correlations in a wide range of clinical use cases. Through an extensive experimental evaluation conducted with two publicly available datasets, CBIS-DDSM and INbreast, we demonstrate that our parameterized hypercomplex models clearly outperform realvalued counterparts and also state-of-the-art methods, proving that breast cancer classiﬁcation beneﬁts from the proposed multi-view architecture. Full code and pretrained models for complete reproducibility of our experiments are freely available at https://github.com/ispamm/PHBreast.},
	language = {en},
	urldate = {2023-05-24},
	publisher = {arXiv},
	author = {Lopez, Eleonora and Grassucci, Eleonora and Valleriani, Martina and Comminiello, Danilo},
	month = dec,
	year = {2022},
	note = {arXiv:2204.05798 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Lopez et al. - 2022 - Hypercomplex Neural Architectures for Multi-View B.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9ZPM5XH2\\Lopez et al. - 2022 - Hypercomplex Neural Architectures for Multi-View B.pdf:application/pdf},
}

@article{honjoVisualQuantitativeEvaluation2022a,
	title = {Visual and quantitative evaluation of microcalcifications in mammograms with deep learning-based super-resolution},
	volume = {154},
	issn = {0720048X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0720048X22002832},
	doi = {10.1016/j.ejrad.2022.110433},
	abstract = {Purpose: To evaluate visually and quantitatively the performance of a deep-learning-based super-resolution (SR) model for microcalcifications in digital mammography.
Method: Mammograms were consecutively collected from 5080 patients who underwent breast cancer screening from January 2015 to March 2017. Of these, 93 patients (136 breasts, mean age, 50 ± 7 years) had micro­ calcifications in their breasts on mammograms. We applied an artificial intelligence model known as a fast SR convolutional neural network to the mammograms. SR and original mammograms were visually evaluated by four breast radiologists using a 5-point scale (1: original mammograms are strongly preferred, 5: SR mammo­ grams are strongly preferred) for the detection, diagnostic quality, contrast, sharpness, and noise of micro­ calcifications. Mammograms were quantitatively evaluated using a perception-based image-quality evaluator (PIQE).
Results: All radiologists rated the SR mammograms better than the original ones in terms of detection, diagnostic quality, contrast, and sharpness of microcalcifications. These ratings were significantly different according to the Wilcoxon signed-rank test (p {\textless}.001), while the noise score of the three radiologists was significantly lower (p {\textless}.001). According to PIQE, SR mammograms were rated better than the original mammograms, showing a significant difference by paired t-test (p {\textless}.001).
Conclusion: An SR model based on deep learning can improve the visibility of microcalcifications in mammog­ raphy and help detect and diagnose them in mammograms.},
	language = {en},
	urldate = {2023-05-24},
	journal = {European Journal of Radiology},
	author = {Honjo, Takashi and Ueda, Daiju and Katayama, Yutaka and Shimazaki, Akitoshi and Jogo, Atsushi and Kageyama, Ken and Murai, Kazuki and Tatekawa, Hiroyuki and Fukumoto, Shinya and Yamamoto, Akira and Miki, Yukio},
	month = sep,
	year = {2022},
	pages = {110433},
	file = {Honjo et al. - 2022 - Visual and quantitative evaluation of microcalcifi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8HCPRVFM\\Honjo et al. - 2022 - Visual and quantitative evaluation of microcalcifi.pdf:application/pdf},
}

@article{suNoncalcifiedDuctalCarcinoma2017,
	title = {Non-calcified ductal carcinoma in situ of the breast: comparison of diagnostic accuracy of digital breast tomosynthesis, digital mammography, and ultrasonography},
	volume = {24},
	issn = {1340-6868, 1880-4233},
	shorttitle = {Non-calcified ductal carcinoma in situ of the breast},
	url = {http://link.springer.com/10.1007/s12282-016-0739-7},
	doi = {10.1007/s12282-016-0739-7},
	abstract = {Background To retrospectively compare the diagnostic accuracy of digital breast tomosynthesis (DBT), digital mammography (DM), and ultrasonography (US) in noncalciﬁed ductal carcinoma in situ (DCIS, include DCIS with micro-invasion).},
	language = {en},
	number = {4},
	urldate = {2023-05-24},
	journal = {Breast Cancer},
	author = {Su, Xiaohui and Lin, Qing and Cui, Chunxiao and Xu, Wenjian and Wei, Zhimin and Fei, Jie and Li, Lili},
	month = jul,
	year = {2017},
	pages = {562--570},
	file = {Su et al. - 2017 - Non-calcified ductal carcinoma in situ of the brea.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\C628E44X\\Su et al. - 2017 - Non-calcified ductal carcinoma in situ of the brea.pdf:application/pdf},
}

@article{petriniBreastCancerDiagnosis2022b,
	title = {Breast {Cancer} {Diagnosis} in {Two}-{View} {Mammography} {Using} {End}-to-{End} {Trained} {EfficientNet}-{Based} {Convolutional} {Network}},
	volume = {10},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9837037/},
	doi = {10.1109/ACCESS.2022.3193250},
	language = {en},
	urldate = {2023-05-24},
	journal = {IEEE Access},
	author = {Petrini, Daniel G. P. and Shimizu, Carlos and Roela, Rosimeire A. and Valente, Gabriel Vansuita and Folgueira, Maria Aparecida Azevedo Koike and Kim, Hae Yong},
	year = {2022},
	pages = {77723--77731},
	file = {Petrini et al. - 2022 - Breast Cancer Diagnosis in Two-View Mammography Us.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\W8CRDLAN\\Petrini et al. - 2022 - Breast Cancer Diagnosis in Two-View Mammography Us.pdf:application/pdf},
}

@article{chenTransformersImproveBreast2022,
	title = {Transformers {Improve} {Breast} {Cancer} {Diagnosis} from {Unregistered} {Multi}-{View} {Mammograms}},
	volume = {12},
	issn = {2075-4418},
	url = {https://www.mdpi.com/2075-4418/12/7/1549},
	doi = {10.3390/diagnostics12071549},
	abstract = {Deep convolutional neural networks (CNNs) have been widely used in various medical imaging tasks. However, due to the intrinsic locality of convolution operations, CNNs generally cannot model long-range dependencies well, which are important for accurately identifying or mapping corresponding breast lesion features computed from unregistered multiple mammograms. This motivated us to leverage the architecture of Multi-view Vision Transformers to capture longrange relationships of multiple mammograms from the same patient in one examination. For this purpose, we employed local transformer blocks to separately learn patch relationships within four mammograms acquired from two-view (CC/MLO) of two-side (right/left) breasts. The outputs from different views and sides were concatenated and fed into global transformer blocks, to jointly learn patch relationships between four images representing two different views of the left and right breasts. To evaluate the proposed model, we retrospectively assembled a dataset involving 949 sets of mammograms, which included 470 malignant cases and 479 normal or benign cases. We trained and evaluated the model using a ﬁve-fold cross-validation method. Without any arduous preprocessing steps (e.g., optimal window cropping, chest wall or pectoral muscle removal, two-view image registration, etc.), our four-image (two-view-two-side) transformer-based model achieves case classiﬁcation performance with an area under ROC curve (AUC = 0.818 ± 0.039), which signiﬁcantly outperforms AUC = 0.784 ± 0.016 achieved by the state-of-the-art multi-view CNNs (p = 0.009). It also outperforms two one-view-two-side models that achieve AUC of 0.724 ± 0.013 (CC view) and 0.769 ± 0.036 (MLO view), respectively. The study demonstrates the potential of using transformers to develop high-performing computer-aided diagnosis schemes that combine four mammograms.},
	language = {en},
	number = {7},
	urldate = {2023-05-24},
	journal = {Diagnostics},
	author = {Chen, Xuxin and Zhang, Ke and Abdoli, Neman and Gilley, Patrik W. and Wang, Ximin and Liu, Hong and Zheng, Bin and Qiu, Yuchen},
	month = jun,
	year = {2022},
	pages = {1549},
	file = {Chen et al. - 2022 - Transformers Improve Breast Cancer Diagnosis from .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\L39SGFH4\\Chen et al. - 2022 - Transformers Improve Breast Cancer Diagnosis from .pdf:application/pdf},
}

@article{jananRICEMethodQuantitative2021,
	title = {{RICE}: {A} method for quantitative mammographic image enhancement},
	volume = {71},
	issn = {13618415},
	shorttitle = {{RICE}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S136184152100089X},
	doi = {10.1016/j.media.2021.102043},
	language = {en},
	urldate = {2023-05-24},
	journal = {Medical Image Analysis},
	author = {Janan, Faraz and Brady, Michael},
	month = jul,
	year = {2021},
	pages = {102043},
	file = {Janan and Brady - 2021 - RICE A method for quantitative mammographic image.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\P93PL5V6\\Janan and Brady - 2021 - RICE A method for quantitative mammographic image.pdf:application/pdf},
}

@article{iranmakaniReviewVariousModalities2020,
	title = {A review of various modalities in breast imaging: technical aspects and clinical outcomes},
	volume = {51},
	issn = {2090-4762},
	shorttitle = {A review of various modalities in breast imaging},
	url = {https://ejrnm.springeropen.com/articles/10.1186/s43055-020-00175-5},
	doi = {10.1186/s43055-020-00175-5},
	abstract = {Background: Nowadays, breast cancer is the second cause of death after cardiovascular diseases. In general, about one out of eight women (about 12\%) suffer from this disease during their life in the USA and European countries. If breast cancer is detected at an early stage, its survival rate will be very high. Several methods have been introduced to diagnose breast cancer with their clinical advantages and disadvantages. Main text: In this review, various methods of breast imaging have been introduced. Furthermore, the sensitivity and specificity of each of these methods have been investigated. For each of the imaging methods, articles that were relevant to the past 10 years were selected through electronic search engines, and then the most relevant papers were selected. Finally, about 40 articles were studied and their results were categorized and presented in the form of a report as follows. Various breast cancer imaging techniques were extracted as follows: mammography, contrast-enhanced mammography, digital tomosynthesis, sonography, sonoelastography, magnetic resonance imaging, magnetic elastography, diffusion-weighted imaging, magnetic spectroscopy, nuclear medicine, optical imaging, and microwave imaging. Conclusion: The choice of these methods depends on the patient’s state and stage, the age of the individual and the density of the breast tissue. Hybrid imaging techniques appear to be an acceptable way to improve detection of breast cancer. This review article can be useful in choosing the right method for imaging in people suspected of breast cancer.},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {Egyptian Journal of Radiology and Nuclear Medicine},
	author = {Iranmakani, Sepideh and Mortezazadeh, Tohid and Sajadian, Fakhrossadat and Ghaziani, Mona Fazel and Ghafari, Ali and Khezerloo, Davood and Musa, Ahmed Eleojo},
	month = dec,
	year = {2020},
	pages = {57},
	file = {Iranmakani et al. - 2020 - A review of various modalities in breast imaging .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MS5HFWNH\\Iranmakani et al. - 2020 - A review of various modalities in breast imaging .pdf:application/pdf},
}

@article{murtazaDeepLearningbasedBreast2020,
	title = {Deep learning-based breast cancer classification through medical imaging modalities: state of the art and research challenges},
	volume = {53},
	issn = {0269-2821, 1573-7462},
	shorttitle = {Deep learning-based breast cancer classification through medical imaging modalities},
	url = {http://link.springer.com/10.1007/s10462-019-09716-5},
	doi = {10.1007/s10462-019-09716-5},
	language = {en},
	number = {3},
	urldate = {2023-05-24},
	journal = {Artificial Intelligence Review},
	author = {Murtaza, Ghulam and Shuib, Liyana and Abdul Wahab, Ainuddin Wahid and Mujtaba, Ghulam and Mujtaba, Ghulam and Nweke, Henry Friday and Al-garadi, Mohammed Ali and Zulfiqar, Fariha and Raza, Ghulam and Azmi, Nor Aniza},
	month = mar,
	year = {2020},
	pages = {1655--1720},
	file = {Murtaza et al. - 2020 - Deep learning-based breast cancer classification t.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\B5EJQHWZ\\Murtaza et al. - 2020 - Deep learning-based breast cancer classification t.pdf:application/pdf},
}

@article{yassinMachineLearningTechniques2018,
	title = {Machine learning techniques for breast cancer computer aided diagnosis using different image modalities: {A} systematic review},
	volume = {156},
	issn = {01692607},
	shorttitle = {Machine learning techniques for breast cancer computer aided diagnosis using different image modalities},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169260717306405},
	doi = {10.1016/j.cmpb.2017.12.012},
	language = {en},
	urldate = {2023-05-24},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Yassin, Nisreen I.R. and Omran, Shaimaa and El Houby, Enas M.F. and Allam, Hemat},
	month = mar,
	year = {2018},
	pages = {25--45},
	file = {Yassin et al. - 2018 - Machine learning techniques for breast cancer comp.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JBZXT49V\\Yassin et al. - 2018 - Machine learning techniques for breast cancer comp.pdf:application/pdf},
}

@article{raghavendraComputeraidedDiagnosisIdentification2019,
	title = {Computer-aided diagnosis for the identification of breast cancer using thermogram images: {A} comprehensive review},
	volume = {102},
	issn = {13504495},
	shorttitle = {Computer-aided diagnosis for the identification of breast cancer using thermogram images},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1350449519304608},
	doi = {10.1016/j.infrared.2019.103041},
	abstract = {Breast cancer is a cancer that can form in the cells of breasts. It is much more common in females than in males. The typical periods of cancer development are during puberty, pregnancy, and breastfeeding. Thermography can be utilized for breast analysis, and provides useful data on the location of hyperthermia and the vascular state of the tissue. Computer-aided diagnosis is an algorithmic approach which can be assistive during routine screening, so that human error in breast analysis for cancer detection is reduced. In early-stage cancer, the accuracy of the assessment then increases, enabling clinicians to make an improved diagnosis of benign versus malignant classification. Herein, we have reviewed thermogram-based computer-aided diagnostic systems developed during the last two decades for breast cancer screening and analysis. We explore the quantitative and qualitative performances of machine learning based approaches, which include segmentation based and feature extraction based methods, dimensionality reduction, and various classification schemes, as proposed in the literature. We also describe the limitations, as well as future requirements to improve current techniques, which can help researchers and clinicians to be apprised of quantitative developments and to plan for the future.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Infrared Physics \& Technology},
	author = {Raghavendra, U. and Gudigar, Anjan and Rao, Tejaswi N. and Ciaccio, Edward J. and Ng, E.Y.K. and Rajendra Acharya, U.},
	month = nov,
	year = {2019},
	pages = {103041},
	file = {Raghavendra et al. - 2019 - Computer-aided diagnosis for the identification of.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8H77XG3C\\Raghavendra et al. - 2019 - Computer-aided diagnosis for the identification of.pdf:application/pdf},
}

@article{borcharttBreastThermographyImage2013,
	title = {Breast thermography from an image processing viewpoint: {A} survey},
	volume = {93},
	issn = {01651684},
	shorttitle = {Breast thermography from an image processing viewpoint},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0165168412002794},
	doi = {10.1016/j.sigpro.2012.08.012},
	abstract = {Breast cancer is the leading cause of death among women. This fact justiﬁes researches to reach early diagnosis, improving patients’ life expectancy. Moreover, there are other pathologies, such as cysts and benign neoplasms that deserve investigation. In the last ten years, the infrared thermography has shown to be a promising technique to early diagnosis of breast pathologies. Works on this subject presented results that justify the thermography as a complementary exam to detect breast diseases. Several papers on the use of infrared imaging for breast screening can be found in the current medical literature. This survey explores and analyses these works in the light of their applications in computer vision. Consequently, the comments are organized according to the main steps of pattern recognition systems. These include: image acquisition protocols, exams storage, segmentation methods, feature extraction, classiﬁcation or diagnostic and computer modelling. Main contributions of discussed papers are summarized in tables to provide a structured vision of the aspects involved in breast thermography.},
	language = {en},
	number = {10},
	urldate = {2023-05-24},
	journal = {Signal Processing},
	author = {Borchartt, Tiago B. and Conci, Aura and Lima, Rita C.F. and Resmini, Roger and Sanchez, Angel},
	month = oct,
	year = {2013},
	pages = {2785--2803},
	file = {Borchartt et al. - 2013 - Breast thermography from an image processing viewp.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WBP2BQW2\\Borchartt et al. - 2013 - Breast thermography from an image processing viewp.pdf:application/pdf},
}

@article{roslidarReviewRecentProgress2020,
	title = {A {Review} on {Recent} {Progress} in {Thermal} {Imaging} and {Deep} {Learning} {Approaches} for {Breast} {Cancer} {Detection}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9121984/},
	doi = {10.1109/ACCESS.2020.3004056},
	abstract = {Developing a breast cancer screening method is very important to facilitate early breast cancer detection and treatment. Building a screening method using medical imaging modality that does not cause body tissue damage (non-invasive) and does not involve physical touch is challenging. Thermography, a non-invasive and non-contact cancer screening method, can detect tumors at an early stage even under precancerous conditions by observing temperature distribution in both breasts. The thermograms obtained on thermography can be interpreted using deep learning models such as convolutional neural networks (CNNs). CNNs can automatically classify breast thermograms into categories such as normal and abnormal. Despite their demostrated utility, CNNs have not been widely used in breast thermogram classiﬁcation. In this study, we aimed to summarize the current work and progress in breast cancer detection based on thermography and CNNs. We ﬁrst discuss of breast thermography potential in early breast cancer detection, providing an overview of the availability of breast thermal datasets together with publicly accessible. We also discuss characteristics of breast thermograms and the differences between healthy and cancerous thermographic patterns. Breast thermogram classiﬁcation using a CNN model is described step by step including a simulation example illustrating feature learning. We cover most research related to the implementation of deep neural networks for breast thermogram classiﬁcation and propose future research directions for developing representative datasets, feeding the segmented image, assigning a good kernel, and building a lightweight CNN model to improve CNN performance.},
	language = {en},
	urldate = {2023-05-24},
	journal = {IEEE Access},
	author = {Roslidar, Roslidar and Rahman, Aulia and Muharar, Rusdha and Syahputra, Muhammad Rizky and Arnia, Fitri and Syukri, Maimun and Pradhan, Biswajeet and Munadi, Khairul},
	year = {2020},
	pages = {116176--116194},
	file = {Roslidar et al. - 2020 - A Review on Recent Progress in Thermal Imaging and.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QVYFPC42\\Roslidar et al. - 2020 - A Review on Recent Progress in Thermal Imaging and.pdf:application/pdf},
}

@article{samalaMassDetectionDigital2016,
	title = {Mass detection in digital breast tomosynthesis: {Deep} convolutional neural network with transfer learning from mammography: {DBT} mass detection using deep convolutional neural network},
	volume = {43},
	issn = {00942405},
	shorttitle = {Mass detection in digital breast tomosynthesis},
	url = {http://doi.wiley.com/10.1118/1.4967345},
	doi = {10.1118/1.4967345},
	abstract = {Purpose: Develop a computer-aided detection (CAD) system for masses in digital breast tomosynthesis (DBT) volume using a deep convolutional neural network (DCNN) with transfer learning from mammograms.
Methods: A data set containing 2282 digitized film and digital mammograms and 324 DBT volumes were collected with IRB approval. The mass of interest on the images was marked by an experienced breast radiologist as reference standard. The data set was partitioned into a training set (2282 mammograms with 2461 masses and 230 DBT views with 228 masses) and an independent test set (94 DBT views with 89 masses). For DCNN training, the region of interest (ROI) containing the mass (true positive) was extracted from each image. False positive (FP) ROIs were identified at prescreening by their previously developed CAD systems. After data augmentation, a total of 45 072 mammographic ROIs and 37 450 DBT ROIs were obtained. Data normalization and reduction of non-uniformity in the ROIs across heterogeneous data was achieved using a background correction method applied to each ROI. A DCNN with four convolutional layers and three fully connected (FC) layers was first trained on the mammography data. Jittering and dropout techniques were used to reduce overfitting. After training with the mammographic ROIs, all weights in the first three convolutional layers were frozen, and only the last convolution layer and the FC layers were randomly initialized again and trained using the DBT training ROIs. The authors compared the performances of two CAD systems for mass detection in DBT: one used the DCNN-based approach and the other used their previously developed feature-based approach for FP reduction. The prescreening stage was identical in both systems, passing the same set of mass candidates to the FP reduction stage. For the feature-based CAD system, 3D clustering and active contour method was used for segmentation; morphological, gray level, and texture features were extracted and merged with a linear discriminant classifier to score the detected masses. For the DCNN-based CAD system, ROIs from five consecutive slices centered at each candidate were passed through the trained DCNN and a mass likelihood score was generated. The performances of the CAD systems were evaluated using free-response ROC curves and the performance difference was analyzed using a non-parametric method.
Results: Before transfer learning, the DCNN trained only on mammograms with an AUC of 0.99 classified DBT masses with an AUC of 0.81 in the DBT training set. After transfer learning with DBT, the AUC improved to 0.90. For breast-based CAD detection in the test set, the sensitivity for the feature-based and the DCNN-based CAD systems was 83\% and 91\%, respectively, at 1 FP/DBT volume. The difference between the performances for the two systems was statistically significant (p-value {\textless} 0.05).
Conclusions: The image patterns learned from the mammograms were transferred to the mass detection on DBT slices through the DCNN. This study demonstrated that large data sets collected from mammography are useful for developing new CAD systems for DBT, alleviating the problem and effort of collecting entirely new large data sets for the new modality. C 2016 American Association of Physicists in Medicine. [http://dx.doi.org/10.1118/1.4967345]},
	language = {en},
	number = {12},
	urldate = {2023-05-24},
	journal = {Medical Physics},
	author = {Samala, Ravi K. and Chan, Heang-Ping and Hadjiiski, Lubomir and Helvie, Mark A. and Wei, Jun and Cha, Kenny},
	month = nov,
	year = {2016},
	pages = {6654--6666},
	file = {Samala et al. - 2016 - Mass detection in digital breast tomosynthesis De.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\37U2CXNI\\Samala et al. - 2016 - Mass detection in digital breast tomosynthesis De.pdf:application/pdf},
}

@article{fanMassDetectionSegmentation2020a,
	title = {Mass {Detection} and {Segmentation} in {Digital} {Breast} {Tomosynthesis} {Using} {3D}-{Mask} {Region}-{Based} {Convolutional} {Neural} {Network}: {A} {Comparative} {Analysis}},
	volume = {7},
	issn = {2296-889X},
	shorttitle = {Mass {Detection} and {Segmentation} in {Digital} {Breast} {Tomosynthesis} {Using} {3D}-{Mask} {Region}-{Based} {Convolutional} {Neural} {Network}},
	url = {https://www.frontiersin.org/articles/10.3389/fmolb.2020.599333/full},
	doi = {10.3389/fmolb.2020.599333},
	abstract = {Digital breast tomosynthesis (DBT) is an emerging breast cancer screening and diagnostic modality that uses quasi-three-dimensional breast images to provide detailed assessments of the dense tissue within the breast. In this study, a framework of a 3D-Mask region-based convolutional neural network (3D-Mask RCNN) computeraided diagnosis (CAD) system was developed for mass detection and segmentation with a comparative analysis of performance on patient subgroups with different clinicopathological characteristics. To this end, 364 samples of DBT data were used and separated into a training dataset (n = 201) and a testing dataset (n = 163). The detection and segmentation results were evaluated on the testing set and on subgroups of patients with different characteristics, including different age ranges, lesion sizes, histological types, lesion shapes and breast densities. The results of our 3D-Mask RCNN framework were compared with those of the 2D-Mask RCNN and Faster RCNN methods. For lesion-based mass detection, the sensitivity of 3D-Mask RCNN-based CAD was 90\% with 0.8 false positives (FPs) per lesion, whereas the sensitivity of the 2D-Mask RCNN- and Faster RCNN-based CAD was 90\% at 1.3 and 2.37 FPs/lesion, respectively. For breast-based mass detection, the 3D-Mask RCNN generated a sensitivity of 90\% at 0.83 FPs/breast, and this framework is better than the 2D-Mask RCNN and Faster RCNN, which generated a sensitivity of 90\% with 1.24 and 2.38 FPs/breast, respectively. Additionally, the 3D-Mask RCNN achieved signiﬁcantly (p {\textless} 0.05) better performance than the 2D methods on subgroups of samples with characteristics of ages ranged from 40 to 49 years, malignant tumors, spiculate and irregular masses and dense breast, respectively. Lesion segmentation using the 3DMask RCNN achieved an average precision (AP) of 0.934 and a false negative rate (FNR) of 0.053, which are better than those achieved by the 2D methods. The results suggest that the 3D-Mask RCNN CAD framework has advantages over 2D-based mass detection on both the whole data and subgroups with different characteristics.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Frontiers in Molecular Biosciences},
	author = {Fan, Ming and Zheng, Huizhong and Zheng, Shuo and You, Chao and Gu, Yajia and Gao, Xin and Peng, Weijun and Li, Lihua},
	month = nov,
	year = {2020},
	pages = {599333},
	file = {Fan et al. - 2020 - Mass Detection and Segmentation in Digital Breast .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\84IURZBM\\Fan et al. - 2020 - Mass Detection and Segmentation in Digital Breast .pdf:application/pdf},
}

@article{gerasArtificialIntelligenceMammography2019b,
	title = {Artificial {Intelligence} for {Mammography} and {Digital} {Breast} {Tomosynthesis}: {Current} {Concepts} and {Future} {Perspectives}},
	volume = {293},
	issn = {0033-8419, 1527-1315},
	shorttitle = {Artificial {Intelligence} for {Mammography} and {Digital} {Breast} {Tomosynthesis}},
	url = {http://pubs.rsna.org/doi/10.1148/radiol.2019182627},
	doi = {10.1148/radiol.2019182627},
	abstract = {Because of the advances in deep learning, the quality of artificial intelligence is rapidly improving for breast imaging and it will likely play an important role for mammography and digital breast tomosynthesis in all steps—from image generation and denoising to risk prediction, cancer detection, and, ultimately, therapy selection and outcome prediction.},
	language = {en},
	number = {2},
	urldate = {2023-05-24},
	journal = {Radiology},
	author = {Geras, Krzysztof J. and Mann, Ritse M. and Moy, Linda},
	month = nov,
	year = {2019},
	pages = {246--259},
	file = {Geras et al. - 2019 - Artificial Intelligence for Mammography and Digita.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QPXXXH2N\\Geras et al. - 2019 - Artificial Intelligence for Mammography and Digita.pdf:application/pdf},
}

@article{sechopoulosArtificialIntelligenceBreast2021b,
	title = {Artificial intelligence for breast cancer detection in mammography and digital breast tomosynthesis: {State} of the art},
	volume = {72},
	issn = {1044579X},
	shorttitle = {Artificial intelligence for breast cancer detection in mammography and digital breast tomosynthesis},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1044579X20301358},
	doi = {10.1016/j.semcancer.2020.06.002},
	abstract = {Screening for breast cancer with mammography has been introduced in various countries over the last 30 years, initially using analog screen-film-based systems and, over the last 20 years, transitioning to the use of fully digital systems. With the introduction of digitization, the computer interpretation of images has been a subject of intense interest, resulting in the introduction of computer-aided detection (CADe) and diagnosis (CADx) algo­ rithms in the early 2000′s. Although they were introduced with high expectations, the potential improvement in the clinical realm failed to materialize, mostly due to the high number of false positive marks per analyzed image. In the last five years, the artificial intelligence (AI) revolution in computing, driven mostly by deep learning and convolutional neural networks, has also pervaded the field of automated breast cancer detection in digital mammography and digital breast tomosynthesis. Research in this area first involved comparison of its capabil­ ities to that of conventional CADe/CADx methods, which quickly demonstrated the potential of this new tech­ nology. In the last couple of years, more mature and some commercial products have been developed, and studies of their performance compared to that of experienced breast radiologists are showing that these algorithms are on par with human-performance levels in retrospective data sets. Although additional studies, especially pro­ spective evaluations performed in the real screening environment, are needed, it is becoming clear that AI will have an important role in the future breast cancer screening realm. Exactly how this new player will shape this field remains to be determined, but recent studies are already evaluating different options for implementation of this technology.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Seminars in Cancer Biology},
	author = {Sechopoulos, Ioannis and Teuwen, Jonas and Mann, Ritse},
	month = jul,
	year = {2021},
	pages = {214--225},
	file = {Sechopoulos et al. - 2021 - Artificial intelligence for breast cancer detectio.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JBX5EXLP\\Sechopoulos et al. - 2021 - Artificial intelligence for breast cancer detectio.pdf:application/pdf},
}

@article{baiApplyingDeepLearning2021,
	title = {Applying deep learning in digital breast tomosynthesis for automatic breast cancer detection: {A} review},
	volume = {71},
	issn = {13618415},
	shorttitle = {Applying deep learning in digital breast tomosynthesis for automatic breast cancer detection},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841521000955},
	doi = {10.1016/j.media.2021.102049},
	language = {en},
	urldate = {2023-05-24},
	journal = {Medical Image Analysis},
	author = {Bai, Jun and Posner, Russell and Wang, Tianyu and Yang, Clifford and Nabavi, Sheida},
	month = jul,
	year = {2021},
	pages = {102049},
	file = {Bai et al. - 2021 - Applying deep learning in digital breast tomosynth.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9PN8EC5H\\Bai et al. - 2021 - Applying deep learning in digital breast tomosynth.pdf:application/pdf},
}

@article{jaglanAutomaticEfficientTechnique2021,
	title = {An automatic and efficient technique for tumor location identification and classification through breast {MR} images},
	volume = {185},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417421009829},
	doi = {10.1016/j.eswa.2021.115580},
	abstract = {Aim: The basic objective of this paper is to develop a single structured algorithm to classify the breast tissues into normal or abnormal. Material \& Methodology: For this study, the breast MR images dataset of 448 images collected from Healthmap diagnostics centre, PGIMS, Rohtak, India. The proposed algorithm consists of several steps i.e. an integrated (Median Wiener \& Median) filtering technique is used for de-noising; breast boundary region extraction via selection of nipple and mid- sternum points to make the image rotation invariant; determined the tumor region intensity by using morphological operations \& hole filling; classify the normal and abnormal breast tissues by SVM using 14 texture features extracted through GLCM \& 13 morphological or kinetic features; evaluated the exact location as well as area of abnormal tissues.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Expert Systems with Applications},
	author = {Jaglan, Poonam and Dass, Rajeshwar and Duhan, Manoj},
	month = dec,
	year = {2021},
	pages = {115580},
	file = {Jaglan et al. - 2021 - An automatic and efficient technique for tumor loc.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DD2SD63C\\Jaglan et al. - 2021 - An automatic and efficient technique for tumor loc.pdf:application/pdf},
}

@article{dhamijaDigitalBreastTomosynthesis2021,
	title = {Digital {Breast} {Tomosynthesis}: an {Overview}},
	volume = {12},
	issn = {0975-7651, 0976-6952},
	shorttitle = {Digital {Breast} {Tomosynthesis}},
	url = {https://link.springer.com/10.1007/s13193-021-01310-y},
	doi = {10.1007/s13193-021-01310-y},
	abstract = {Breast cancer is emerging as the most common malignancy in Indian women. Mammography is one of the few screening modalities available to the modern world that has proved itself of much use by aiding early detection and treatment of nonpalpable, node-negative breast cancers. However, due to its two-dimensional nature, many cases of malignancies are still missed, to be detected at a later date or by an alternate modality. In 2011, FDA approved the supplemental use of digital breast tomosynthesis (DBT) in screening and diagnostic set ups. The acquisition of multiple low-dose projection images of the compressed parenchyma provided a ‘third’ dimension to the mammogram whereby the breast tissue could be seen layer by layer on the workstation. It improves cancer detection rate, and reduces recall rate and false-positive findings by improving lesion characterization. The current review discusses the principle of DBT with a comprehensive study of the literature.},
	language = {en},
	number = {2},
	urldate = {2023-05-24},
	journal = {Indian Journal of Surgical Oncology},
	author = {Dhamija, Ekta and Gulati, Malvika and Deo, S. V. S. and Gogia, Ajay and Hari, Smriti},
	month = jun,
	year = {2021},
	pages = {315--329},
	file = {Dhamija et al. - 2021 - Digital Breast Tomosynthesis an Overview.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TGWLDAJG\\Dhamija et al. - 2021 - Digital Breast Tomosynthesis an Overview.pdf:application/pdf},
}

@article{zhouComprehensiveReviewBreast2020,
	title = {A {Comprehensive} {Review} for {Breast} {Histopathology} {Image} {Analysis} {Using} {Classical} and {Deep} {Neural} {Networks}},
	volume = {8},
	issn = {2169-3536},
	url = {http://arxiv.org/abs/2003.12255},
	doi = {10.1109/ACCESS.2020.2993788},
	abstract = {Breast cancer is one of the most common and deadliest cancers among women. Since histopathological images contain sufﬁcient phenotypic information, they play an indispensable role in the diagnosis and treatment of breast cancers. To improve the accuracy and objectivity of Breast Histopathological Image Analysis (BHIA), Artiﬁcial Neural Network (ANN) approaches are widely used in the segmentation and classiﬁcation tasks of breast histopathological images. In this review, we present a comprehensive overview of the BHIA techniques based on ANNs. First of all, we categorize the BHIA systems into classical and deep neural networks for in-depth investigation. Then, the relevant studies based on BHIA systems are presented. After that, we analyze the existing models to discover the most suitable algorithms. Finally, publicly accessible datasets, along with their download links, are provided for the convenience of future researchers.},
	language = {en},
	urldate = {2023-05-24},
	journal = {IEEE Access},
	author = {Zhou, Xiaomin and Li, Chen and Rahaman, Md Mamunur and Yao, Yudong and Ai, Shiliang and Sun, Changhao and Li, Xiaoyan and Wang, Qian and Jiang, Tao},
	year = {2020},
	note = {arXiv:2003.12255 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	pages = {90931--90956},
	file = {Zhou et al. - 2020 - A Comprehensive Review for Breast Histopathology I.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SJAUQG4B\\Zhou et al. - 2020 - A Comprehensive Review for Breast Histopathology I.pdf:application/pdf},
}

@inproceedings{ferrariCanBilateralAsymmetry2008,
	address = {Campo Grande, Brazil},
	title = {Can {Bilateral} {Asymmetry} {Analysis} of {Breast} {MR} {Images} {Provide} {Additional} {Information} for {Detection} of {Breast} {Diseases}?},
	isbn = {978-0-7695-3358-2},
	url = {https://ieeexplore.ieee.org/document/4654150/},
	doi = {10.1109/SIBGRAPI.2008.10},
	abstract = {This paper presents a new method for bilateral asymmetry analysis of breast MR images that uses directional statistics of the breast parenchymal edges, obtained from a multiresolution local energy edge detector, and image texture information derived from local energy maps, obtained by using a bank of log-Gabor ﬁlters. Classiﬁcation of MRI scans into cancer and non-cancer categories was performed by linear discriminant analysis and the leave-oneout methodology. A total of 40 cases, 20 normal/benign (BI-RADS 1 and 2) and 20 malignant, taken from a high risk screening population, were used in this pilot study. Average classiﬁcation accuracy of 70\% (κ = 0.45 ± 0.14) with sensitivity and speciﬁcity of 75\% and 65\%, respectively, was achieved. The results obtained support the idea that bilateral asymmetry analysis of breast MR images can provide additional information for detection of breast tissue changes arising from diseases.},
	language = {en},
	urldate = {2023-05-24},
	booktitle = {2008 {XXI} {Brazilian} {Symposium} on {Computer} {Graphics} and {Image} {Processing}},
	publisher = {IEEE},
	author = {Ferrari, R.J. and Hill, K.A. and Plewes, D.B. and Martel, A.L.},
	month = oct,
	year = {2008},
	pages = {113--120},
	file = {Ferrari et al. - 2008 - Can Bilateral Asymmetry Analysis of Breast MR Imag.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\D98AV2YK\\Ferrari et al. - 2008 - Can Bilateral Asymmetry Analysis of Breast MR Imag.pdf:application/pdf},
}

@article{mannBreastMRIState2019,
	title = {Breast {MRI}: {State} of the {Art}},
	volume = {292},
	issn = {0033-8419, 1527-1315},
	shorttitle = {Breast {MRI}},
	url = {http://pubs.rsna.org/doi/10.1148/radiol.2019182947},
	doi = {10.1148/radiol.2019182947},
	abstract = {Indications for breast MRI are consolidating; MRI for screening leads to earlier cancer detection in virtually all evaluated populations; in the hands of experienced teams, MRI allows for improvement of surgical practice, reducing the number of re-excisions while preventing unnecessary mastectomies; and MRI allows for patient selection for neoadjuvant chemotherapy and is the technique of choice to support modification of therapeutic agents and for presurgical assessment of residual tumor size to determine breast conservation surgery candidacy. Essentials nn Breast MRI is a key imaging technique for breast imaging. nn Multiparametric breast MRI protocols can be adapted to the clinical indication. nn Translating preoperative MRI for extent of disease evaluation to better surgical outcomes requires experience in incorporating MRI findings for MRI-guided surgery, with lesion localization where appropriate. nn Screening with breast MRI leads to earlier cancer detection in all women. nn In the neoadjuvant setting, enhancement characteristics of lesions change, and assessment should be adapted to the clinical question that is to be answered.},
	language = {en},
	number = {3},
	urldate = {2023-05-24},
	journal = {Radiology},
	author = {Mann, Ritse M. and Cho, Nariya and Moy, Linda},
	month = sep,
	year = {2019},
	pages = {520--536},
	file = {Mann et al. - 2019 - Breast MRI State of the Art.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FL5MNW4F\\Mann et al. - 2019 - Breast MRI State of the Art.pdf:application/pdf},
}

@article{robertsonDigitalImageAnalysis2018,
	title = {Digital image analysis in breast pathology—from image processing techniques to artificial intelligence},
	volume = {194},
	issn = {19315244},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1931524417302955},
	doi = {10.1016/j.trsl.2017.10.010},
	language = {en},
	urldate = {2023-05-24},
	journal = {Translational Research},
	author = {Robertson, Stephanie and Azizpour, Hossein and Smith, Kevin and Hartman, Johan},
	month = apr,
	year = {2018},
	pages = {19--35},
	file = {Robertson et al. - 2018 - Digital image analysis in breast pathology—from im.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KGJEWMHH\\Robertson et al. - 2018 - Digital image analysis in breast pathology—from im.pdf:application/pdf},
}

@article{duggentoDeepComputationalPathology2021,
	title = {Deep computational pathology in breast cancer},
	volume = {72},
	issn = {1044579X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1044579X20301784},
	doi = {10.1016/j.semcancer.2020.08.006},
	abstract = {Deep Learning (DL) algorithms are a set of techniques that exploit large and/or complex real-world datasets for cross-domain and cross-discipline prediction and classification tasks. DL architectures excel in computer vision tasks, and in particular image processing and interpretation. This has prompted a wave of disruptingly innovative applications in medical imaging, where DL strategies have the potential to vastly outperform human experts. This is particularly relevant in the context of histopathology, where whole slide imaging (WSI) of stained tissue in conjuction with DL algorithms for their interpretation, selection and cancer staging are beginning to play an ever increasing role in supporting human operators in visual assessments. This has the potential to reduce everyday workload as well as to increase precision and reproducibility across observers, centers, staining techniques and even pathologies. In this paper we introduce the most common DL architectures used in image analysis, with a focus on histopathological image analysis in general and in breast histology in particular. We briefly review how, state-of-art DL architectures compare to human performance on across a number of critical tasks such as mitotic count, tubules analysis and nuclear pleomorphism analysis. Also, the development of DL algorithms specialized to pathology images have been enormously fueled by a number of world-wide challenges based on large, mul­ ticentric image databases which are now publicly available. In turn, this has allowed most recent efforts to shift more and more towards semi-supervised learning methods, which provide greater flexibility and applicability. We also review all major repositories of manually labelled pathology images in breast cancer and provide an indepth discussion of the challenges specific to training DL architectures to interpret WSI data, as well as a review of the state-of-the-art methods for interpretation of images generated from immunohistochemical analysis of breast lesions. We finally discuss the future challenges and opportunities which the adoption of DL paradigms is most likely to pose in the field of pathology for breast cancer detection, diagnosis, staging and prognosis. This review is intended as a comprehensive stepping stone into the field of modern computational pathology for a transdisciplinary readership across technical and medical disciplines.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Seminars in Cancer Biology},
	author = {Duggento, Andrea and Conti, Allegra and Mauriello, Alessandro and Guerrisi, Maria and Toschi, Nicola},
	month = jul,
	year = {2021},
	pages = {226--237},
	file = {Duggento et al. - 2021 - Deep computational pathology in breast cancer.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XRB52MSY\\Duggento et al. - 2021 - Deep computational pathology in breast cancer.pdf:application/pdf},
}

@article{hamidinekooDeepLearningMammography2018a,
	title = {Deep learning in mammography and breast histology, an overview and future trends},
	volume = {47},
	issn = {13618415},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841518300902},
	doi = {10.1016/j.media.2018.03.006},
	language = {en},
	urldate = {2023-05-24},
	journal = {Medical Image Analysis},
	author = {Hamidinekoo, Azam and Denton, Erika and Rampun, Andrik and Honnor, Kate and Zwiggelaar, Reyer},
	month = jul,
	year = {2018},
	pages = {45--67},
	file = {Hamidinekoo et al. - 2018 - Deep learning in mammography and breast histology,.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MP28UFFK\\Hamidinekoo et al. - 2018 - Deep learning in mammography and breast histology,.pdf:application/pdf},
}

@article{miDeepLearningBasedMultiClass2021,
	title = {Deep {Learning}-{Based} {Multi}-{Class} {Classification} of {Breast} {Digital} {Pathology} {Images}},
	volume = {Volume 13},
	issn = {1179-1322},
	url = {https://www.dovepress.com/deep-learning-based-multi-class-classification-of-breast-digital-patho-peer-reviewed-fulltext-article-CMAR},
	doi = {10.2147/CMAR.S312608},
	abstract = {Introduction: Breast cancer, one of the most common health threats to females worldwide, has always been a crucial topic in the medical field. With the rapid development of digital pathology, many scholars have used AI-based systems to classify breast cancer pathological images. However, most existing studies only stayed on the binary classification of breast lesions (normal vs tumor or benign vs malignant), far from meeting the clinical demand. Therefore, we established a multi-class classification system of breast digital pathology images based on AI, which is more clinically practical than the binary classification system.
Methods: In this paper, we adopted a two-stage architecture based on deep learning method and machine learning method for the multi-class classification (normal tissue, benign lesion, ductal carcinoma in situ, and invasive carcinoma) of breast digital pathological images.
Results: The proposed approach achieved an overall accuracy of 86.67\% at patch-level. At WSI-level, the overall accuracies of our classification system were 88.16\% on validation data and 90.43\% on test data. Additionally, we used two public datasets, the BreakHis and BACH, for independent verification. The accuracies our model obtained on these two datasets were comparable to related publications. Furthermore, our model could achieve accuracies of 85.19\% on multi-classification and 96.30\% on binary classification (nonmalignant vs malignant) using pathology images of frozen sections, which was proven to have good generalizability. Then, we used t-SNE for visualization of patch classification efficiency. Finally, we analyzed morphological characteristics of patches learned by the model.
Conclusion: The proposed two-stage model could be effectively applied to the multi-class classification task of breast pathology images and could be a very useful tool for assisting pathologists in diagnosing breast cancer.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Cancer Management and Research},
	author = {Mi, Weiming and Li, Junjie and Guo, Yucheng and Ren, Xinyu and Liang, Zhiyong and Zhang, Tao and Zou, Hao},
	month = jun,
	year = {2021},
	pages = {4605--4617},
	file = {Mi et al. - 2021 - Deep Learning-Based Multi-Class Classification of .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5QH3ENAV\\Mi et al. - 2021 - Deep Learning-Based Multi-Class Classification of .pdf:application/pdf},
}

@article{mostafaAutomatedBreastUltrasound2019,
	title = {Automated breast ultrasound ({ABUS}) as a screening tool: initial experience},
	volume = {50},
	issn = {2090-4762},
	shorttitle = {Automated breast ultrasound ({ABUS}) as a screening tool},
	url = {https://ejrnm.springeropen.com/articles/10.1186/s43055-019-0032-9},
	doi = {10.1186/s43055-019-0032-9},
	abstract = {Background: Breast cancer is a major health problem, being the most common cancer in women. Early detection of breast cancer aims to the reduction of mortality and morbidity rates. Conventional screening methods include mammography and ultrasonography; however, both modalities have their limitations. Automated breast ultrasound (ABUS) is a recent technological advancement in the field of breast imaging having the benefit of standardization of the scans and lack of operator dependence as in conventional handheld ultrasound scans. The aim of this work was to report our initial experience of the added value of ABUS as a breast screening tool. The study included 200 patients who had screening mammograms, ultrasound, and ABUS.
Results: A significant difference was found between the number of lesions detected by ABUS and conventional ultrasound. A significant difference was found between lesions detected by ABUS and mammography which was most evident in patients with dense breasts.
Conclusions: ABUS is a valuable tool in the screening of the breast with improved lesion detection, especially in patients with dense breasts.},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {Egyptian Journal of Radiology and Nuclear Medicine},
	author = {Mostafa, Amera Abd Elsalam and Eltomey, Mohamed Adel and Elaggan, Ashraf Mohammed and Hashish, Amel A.},
	month = dec,
	year = {2019},
	pages = {37},
	file = {Mostafa et al. - 2019 - Automated breast ultrasound (ABUS) as a screening .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\V4YVFRU8\\Mostafa et al. - 2019 - Automated breast ultrasound (ABUS) as a screening .pdf:application/pdf},
}

@article{rashmiBreastHistopathologicalImage2022,
	title = {Breast histopathological image analysis using image processing techniques for diagnostic purposes: {A} methodological review},
	volume = {46},
	issn = {0148-5598, 1573-689X},
	shorttitle = {Breast histopathological image analysis using image processing techniques for diagnostic purposes},
	url = {https://link.springer.com/10.1007/s10916-021-01786-9},
	doi = {10.1007/s10916-021-01786-9},
	abstract = {Breast cancer in women is the second most common cancer worldwide. Early detection of breast cancer can reduce the risk of human life. Non-invasive techniques such as mammograms and ultrasound imaging are popularly used to detect the tumour. However, histopathological analysis is necessary to determine the malignancy of the tumour as it analyses the image at the cellular level. Manual analysis of these slides is time consuming, tedious, subjective and are susceptible to human errors. Also, at times the interpretation of these images are inconsistent between laboratories. Hence, a Computer-Aided Diagnostic system that can act as a decision support system is need of the hour. Moreover, recent developments in computational power and memory capacity led to the application of computer tools and medical image processing techniques to process and analyze breast cancer histopathological images. This review paper summarizes various traditional and deep learning based methods developed to analyze breast cancer histopathological images. Initially, the characteristics of breast cancer histopathological images are discussed. A detailed discussion on the various potential regions of interest is presented which is crucial for the development of Computer-Aided Diagnostic systems. We summarize the recent trends and choices made during the selection of medical image processing techniques. Finally, a detailed discussion on the various challenges involved in the analysis of BCHI is presented along with the future scope.},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {Journal of Medical Systems},
	author = {Rashmi, R and Prasad, Keerthana and Udupa, Chethana Babu K},
	month = jan,
	year = {2022},
	pages = {7},
	file = {Rashmi et al. - 2022 - Breast histopathological image analysis using imag.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\V84H9N3Q\\Rashmi et al. - 2022 - Breast histopathological image analysis using imag.pdf:application/pdf},
}

@article{shenArtificialIntelligenceSystem2021,
	title = {Artificial intelligence system reduces false-positive findings in the interpretation of breast ultrasound exams},
	volume = {12},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-26023-2},
	doi = {10.1038/s41467-021-26023-2},
	abstract = {Abstract
            Though consistently shown to detect mammographically occult cancers, breast ultrasound has been noted to have high false-positive rates. In this work, we present an AI system that achieves radiologist-level accuracy in identifying breast cancer in ultrasound images. Developed on 288,767 exams, consisting of 5,442,907 B-mode and Color Doppler images, the AI achieves an area under the receiver operating characteristic curve (AUROC) of 0.976 on a test set consisting of 44,755 exams. In a retrospective reader study, the AI achieves a higher AUROC than the average of ten board-certified breast radiologists (AUROC: 0.962 AI, 0.924 ± 0.02 radiologists). With the help of the AI, radiologists decrease their false positive rates by 37.3\% and reduce requested biopsies by 27.8\%, while maintaining the same level of sensitivity. This highlights the potential of AI in improving the accuracy, consistency, and efficiency of breast ultrasound diagnosis.},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {Nature Communications},
	author = {Shen, Yiqiu and Shamout, Farah E. and Oliver, Jamie R. and Witowski, Jan and Kannan, Kawshik and Park, Jungkyu and Wu, Nan and Huddleston, Connor and Wolfson, Stacey and Millet, Alexandra and Ehrenpreis, Robin and Awal, Divya and Tyma, Cathy and Samreen, Naziya and Gao, Yiming and Chhor, Chloe and Gandhi, Stacey and Lee, Cindy and Kumari-Subaiya, Sheila and Leonard, Cindy and Mohammed, Reyhan and Moczulski, Christopher and Altabet, Jaime and Babb, James and Lewin, Alana and Reig, Beatriu and Moy, Linda and Heacock, Laura and Geras, Krzysztof J.},
	month = sep,
	year = {2021},
	pages = {5645},
	file = {Shen et al. - 2021 - Artificial intelligence system reduces false-posit.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DHKUYEHD\\Shen et al. - 2021 - Artificial intelligence system reduces false-posit.pdf:application/pdf},
}

@article{chengAutomatedBreastCancer2010,
	title = {Automated breast cancer detection and classification using ultrasound images: {A} survey},
	volume = {43},
	issn = {00313203},
	shorttitle = {Automated breast cancer detection and classification using ultrasound images},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320309002027},
	doi = {10.1016/j.patcog.2009.05.012},
	abstract = {Breast cancer is the second leading cause of death for women all over the world. Since the cause of the disease remains unknown, early detection and diagnosis is the key for breast cancer control, and it can increase the success of treatment, save lives and reduce cost. Ultrasound imaging is one of the most frequently used diagnosis tools to detect and classify abnormalities of the breast. In order to eliminate the operator dependency and improve the diagnostic accuracy, computer-aided diagnosis (CAD) system is a valuable and beneficial means for breast cancer detection and classification. Generally, a CAD system consists of four stages: preprocessing, segmentation, feature extraction and selection, and classification. In this paper, the approaches used in these stages are summarized and their advantages and disadvantages are discussed. The performance evaluation of CAD system is investigated as well.},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {Pattern Recognition},
	author = {Cheng, H.D. and Shan, Juan and Ju, Wen and Guo, Yanhui and Zhang, Ling},
	month = jan,
	year = {2010},
	pages = {299--317},
	file = {Cheng et al. - 2010 - Automated breast cancer detection and classificati.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SX3S8PD3\\Cheng et al. - 2010 - Automated breast cancer detection and classificati.pdf:application/pdf},
}

@article{nicosiaAutomaticBreastUltrasound2020,
	title = {Automatic breast ultrasound: state of the art and future perspectives},
	volume = {14},
	issn = {17546605},
	shorttitle = {Automatic breast ultrasound},
	url = {https://ecancer.org/en/journal/article/1062-automatic-breast-ultrasound-state-of-the-art-and-future-perspectives},
	doi = {10.3332/ecancer.2020.1062},
	abstract = {The three-dimensional automated breast ultrasound system (3D ABUS) is a new device which represents a huge innovation in the breast ultrasound field, with several application scenarios of great interest.},
	language = {en},
	urldate = {2023-05-24},
	journal = {ecancermedicalscience},
	author = {Nicosia, Luca and Ferrari, Federica and Bozzini, Anna Carla and Latronico, Antuono and Trentin, Chiara and Meneghetti, Lorenza and Pesapane, Filippo and Pizzamiglio, Maria and Balesetreri, Nicola and Cassano, Enrico},
	month = jun,
	year = {2020},
	file = {Nicosia et al. - 2020 - Automatic breast ultrasound state of the art and .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WZLZYGPM\\Nicosia et al. - 2020 - Automatic breast ultrasound state of the art and .pdf:application/pdf},
}

@article{mckinneyInternationalEvaluationAI2020,
	title = {International evaluation of an {AI} system for breast cancer screening},
	volume = {577},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-019-1799-6},
	doi = {10.1038/s41586-019-1799-6},
	language = {en},
	number = {7788},
	urldate = {2023-05-24},
	journal = {Nature},
	author = {McKinney, Scott Mayer and Sieniek, Marcin and Godbole, Varun and Godwin, Jonathan and Antropova, Natasha and Ashrafian, Hutan and Back, Trevor and Chesus, Mary and Corrado, Greg S. and Darzi, Ara and Etemadi, Mozziyar and Garcia-Vicente, Florencia and Gilbert, Fiona J. and Halling-Brown, Mark and Hassabis, Demis and Jansen, Sunny and Karthikesalingam, Alan and Kelly, Christopher J. and King, Dominic and Ledsam, Joseph R. and Melnick, David and Mostofi, Hormuz and Peng, Lily and Reicher, Joshua Jay and Romera-Paredes, Bernardino and Sidebottom, Richard and Suleyman, Mustafa and Tse, Daniel and Young, Kenneth C. and De Fauw, Jeffrey and Shetty, Shravya},
	month = jan,
	year = {2020},
	pages = {89--94},
	file = {McKinney et al. - 2020 - International evaluation of an AI system for breas.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\M9G78WQW\\McKinney et al. - 2020 - International evaluation of an AI system for breas.pdf:application/pdf},
}

@article{batchuReviewApplicationsMachine2021,
	title = {A {Review} of {Applications} of {Machine} {Learning} in {Mammography} and {Future} {Challenges}},
	volume = {99},
	issn = {0030-2414, 1423-0232},
	url = {https://www.karger.com/Article/FullText/515698},
	doi = {10.1159/000515698},
	abstract = {\textbf{\textit{Background:}} The aim of this study is to systematically review the literature to summarize the evidence surrounding the clinical utility of artificial intelligence (AI) in the field of mammography. Databases from PubMed, IEEE Xplore, and Scopus were searched for relevant literature. Studies evaluating AI models in the context of prediction and diagnosis of breast malignancies that also reported conventional performance metrics were deemed suitable for inclusion. From 90 unique citations, 21 studies were considered suitable for our examination. Data was not pooled due to heterogeneity in study evaluation methods. \textbf{\textit{Summary:}} Three studies showed the applicability of AI in reducing workload. Six studies demonstrated that AI can aid in diagnosis, with up to 69\% reduction in false positives and an increase in sensitivity ranging from 84 to 91\%. Five studies show how AI models can independently mark and classify suspicious findings on conventional scans, with abilities comparable with radiologists. Seven studies examined AI predictive potential for breast cancer and risk score calculation. \textbf{\textit{Key Messages:}} Despite limitations in the current evidence base and technical obstacles, this review suggests AI has marked potential for extensive use in mammography. Additional works, including large-scale prospective studies, are warranted to elucidate the clinical utility of AI.},
	language = {en},
	number = {8},
	urldate = {2023-05-24},
	journal = {Oncology},
	author = {Batchu, Sai and Liu, Fan and Amireh, Ahmad and Waller, Joseph and Umair, Muhammad},
	year = {2021},
	pages = {483--490},
	file = {Batchu et al. - 2021 - A Review of Applications of Machine Learning in Ma.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7P9HPSY7\\Batchu et al. - 2021 - A Review of Applications of Machine Learning in Ma.pdf:application/pdf},
}

@article{baughanPresentFutureMachine2022,
	title = {Past, {Present}, and {Future} of {Machine} {Learning} and {Artificial} {Intelligence} for {Breast} {Cancer} {Screening}},
	volume = {4},
	issn = {2631-6110, 2631-6129},
	url = {https://academic.oup.com/jbi/article/4/5/451/6697999},
	doi = {10.1093/jbi/wbac052},
	abstract = {Breast cancer screening has evolved substantially over the past few decades because of advancements in new image acquisition systems and novel artificial intelligence (AI) algorithms.This review provides a brief overview of the history, current state, and future of AI in breast cancer screening and diagnosis along with challenges involved in the development of AI systems. Although AI has been developing for interpretation tasks associated with breast cancer screening for decades, its potential to combat the subjective nature and improve the efficiency of human image interpretation is always expanding. The rapid advancement of computational power and deep learning has increased greatly in AI research, with promising performance in detection and classification tasks across imaging modalities. Most AI systems, based on human-engineered or deep learning methods, serve as concurrent or secondary readers, that is, as aids to radiologists for a specific, well-defined task. In the future, AI may be able to perform multiple integrated tasks, making decisions at the level of or surpassing the ability of humans. Artificial intelligence may also serve as a partial primary reader to streamline ancillary tasks, triaging cases or ruling out obvious normal cases. However, before AI is used as an independent, autonomous reader, various challenges need to be addressed, including explainability and interpretability, in addition to repeatability and generalizability, to ensure that AI will provide a significant clinical benefit to breast cancer screening across all populations.},
	language = {en},
	number = {5},
	urldate = {2023-05-24},
	journal = {Journal of Breast Imaging},
	author = {Baughan, Natalie and Douglas, Lindsay and Giger, Maryellen L},
	month = oct,
	year = {2022},
	pages = {451--459},
	file = {Baughan et al. - 2022 - Past, Present, and Future of Machine Learning and .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NERGBQY9\\Baughan et al. - 2022 - Past, Present, and Future of Machine Learning and .pdf:application/pdf},
}

@incollection{jimenezgaonaDenseNetBreastTumor2021,
	address = {Cham},
	title = {{DenseNet} for {Breast} {Tumor} {Classification} in {Mammographic} {Images}},
	volume = {12940},
	isbn = {978-3-030-88162-7 978-3-030-88163-4},
	url = {https://link.springer.com/10.1007/978-3-030-88163-4_16},
	abstract = {Breast cancer is the most common invasive cancer in women, and the second main cause of death. Breast cancer screening is an efficient method to detect indeterminate breast lesions early. The common approaches of screening for women are tomosynthesis and mammography images. However, the traditional manual diagnosis requires an intense workload by pathologists, who are prone to diagnostic errors. Thus, the aim of this study is to build a deep convolutional neural network method for automatic detection, segmentation, and classification of breast lesions in mammography images. Based on deep learning the Mask-CNN (RoIAlign) method was developed to features selection and extraction; and the classification was carried out by DenseNet architecture. Finally, the precision and accuracy of the model is evaluated by cross validation matrix and AUC curve. To summarize, the findings of this study may provide a helpful to improve the diagnosis and efficiency in the automatic tumor localization through the medical image classification.},
	language = {en},
	urldate = {2023-05-24},
	booktitle = {Bioengineering and {Biomedical} {Signal} and {Image} {Processing}},
	publisher = {Springer International Publishing},
	author = {Jiménez Gaona, Yuliana and Rodriguez-Alvarez, María José and Espino-Morato, Hector and Castillo Malla, Darwin and Lakshminarayanan, Vasudevan},
	editor = {Rojas, Ignacio and Castillo-Secilla, Daniel and Herrera, Luis Javier and Pomares, Héctor},
	year = {2021},
	doi = {10.1007/978-3-030-88163-4_16},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {166--176},
	file = {Jiménez Gaona et al. - 2021 - DenseNet for Breast Tumor Classification in Mammog.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\G6QIG4NT\\Jiménez Gaona et al. - 2021 - DenseNet for Breast Tumor Classification in Mammog.pdf:application/pdf},
}

@misc{kwongSurveyDeepLearning2022,
	title = {A survey on deep learning approaches for breast cancer diagnosis},
	url = {http://arxiv.org/abs/2109.08853},
	abstract = {Deep learning has introduced several learning-based methods to recognize breast tumours and presents high applicability in breast cancer diagnostics. It has presented itself as a practical installment in Computer-Aided Diagnostic (CAD) systems to further assist radiologists in diagnostics for different modalities. A deep learning network trained on images provided by hospitals or public databases can perform classiﬁcation, detection, and segmentation of lesion types. Signiﬁcant progress has been made in recognizing tumours on 2D images but recognizing 3D images remains a frontier so far. The interconnection of deep learning networks between different ﬁelds of study help propels discoveries for more efﬁcient, accurate, and robust networks. In this review paper, the following topics will be explored: (i) theory and application of deep learning, (ii) progress of 2D, 2.5D, and 3D CNN approaches in breast tumour recognition from a performance metric perspective, and (iii) challenges faced in CNN approaches.},
	language = {en},
	urldate = {2023-05-24},
	publisher = {arXiv},
	author = {Kwong, Timothy and Mazaheri, Samaneh},
	month = feb,
	year = {2022},
	note = {arXiv:2109.08853 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Kwong and Mazaheri - 2022 - A survey on deep learning approaches for breast ca.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\U67VTMM7\\Kwong and Mazaheri - 2022 - A survey on deep learning approaches for breast ca.pdf:application/pdf},
}

@article{petriniBreastCancerDiagnosis2022c,
	title = {Breast {Cancer} {Diagnosis} in {Two}-{View} {Mammography} {Using} {End}-to-{End} {Trained} {EfficientNet}-{Based} {Convolutional} {Network}},
	volume = {10},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9837037/},
	doi = {10.1109/ACCESS.2022.3193250},
	language = {en},
	urldate = {2023-05-24},
	journal = {IEEE Access},
	author = {Petrini, Daniel G. P. and Shimizu, Carlos and Roela, Rosimeire A. and Valente, Gabriel Vansuita and Folgueira, Maria Aparecida Azevedo Koike and Kim, Hae Yong},
	year = {2022},
	pages = {77723--77731},
	file = {Petrini et al. - 2022 - Breast Cancer Diagnosis in Two-View Mammography Us.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SNYGD9R5\\Petrini et al. - 2022 - Breast Cancer Diagnosis in Two-View Mammography Us.pdf:application/pdf},
}

@article{ragabBreastCancerDetection2019,
	title = {Breast cancer detection using deep convolutional neural networks and support vector machines},
	volume = {7},
	issn = {2167-8359},
	url = {https://peerj.com/articles/6201},
	doi = {10.7717/peerj.6201},
	abstract = {It is important to detect breast cancer as early as possible. In this manuscript, a new methodology for classifying breast cancer using deep learning and some segmentation techniques are introduced. A new computer aided detection (CAD) system is proposed for classifying benign and malignant mass tumors in breast mammography images. In this CAD system, two segmentation approaches are used. The first approach involves determining the region of interest (ROI) manually, while the second approach uses the technique of threshold and region based. The deep convolutional neural network (DCNN) is used for feature extraction. A well-known DCNN architecture named AlexNet is used and is fine-tuned to classify two classes instead of 1,000 classes. The last fully connected (fc) layer is connected to the support vector machine (SVM) classifier to obtain better accuracy. The results are obtained using the following publicly available datasets (1) the digital database for screening mammography (DDSM); and (2) the Curated Breast Imaging Subset of DDSM (CBIS-DDSM). Training on a large number of data gives high accuracy rate. Nevertheless, the biomedical datasets contain a relatively small number of samples due to limited patient volume. Accordingly, data augmentation is a method for increasing the size of the input data by generating new data from the original input data. There are many forms for the data augmentation; the one used here is the rotation. The accuracy of the new-trained DCNN architecture is 71.01\% when cropping the ROI manually from the mammogram. The highest area under the curve (AUC) achieved was 0.88 (88\%) for the samples obtained from both segmentation techniques. Moreover, when using the samples obtained from the CBISDDSM, the accuracy of the DCNN is increased to 73.6\%. Consequently, the SVM accuracy becomes 87.2\% with an AUC equaling to 0.94 (94\%). This is the highest AUC value compared to previous work using the same conditions.},
	language = {en},
	urldate = {2023-05-24},
	journal = {PeerJ},
	author = {Ragab, Dina A. and Sharkas, Maha and Marshall, Stephen and Ren, Jinchang},
	month = jan,
	year = {2019},
	pages = {e6201},
	file = {Ragab et al. - 2019 - Breast cancer detection using deep convolutional n.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GSP5WXU4\\Ragab et al. - 2019 - Breast cancer detection using deep convolutional n.pdf:application/pdf},
}

@article{pirouzbakhtAlgorithmDetectionBreast2017,
	title = {Algorithm for the {Detection} of {Breast} {Cancer} in {Digital} {Mammograms} {Using} {Deep} {Learning}},
	abstract = {Breast cancer is one of the most frequent malignant tumors in women worldwide, the detection of this disease in time increases the possibility of receiving a less aggressive treatment and increases the survival rate. In this paper, we developed a cancer detection system that could be beneﬁcial to help radiologists in cancer detection. To this end, we used a deep-learning network architecture. The proposed network consists of three convolutional layers followed each by pooling, and ﬁnally, four full connected layers provided the output of the network. Here, we also proposed to feed up the net with contrast-enhanced images to improve performance.},
	language = {en},
	author = {Pirouzbakht, Natalia and Mejıa, Jose},
	year = {2017},
	file = {Pirouzbakht and Mejıa - 2017 - Algorithm for the Detection of Breast Cancer in Di.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QWDZ3V7N\\Pirouzbakht and Mejıa - 2017 - Algorithm for the Detection of Breast Cancer in Di.pdf:application/pdf},
}

@article{nassifBreastCancerDetection2022,
	title = {Breast cancer detection using artificial intelligence techniques: {A} systematic literature review},
	volume = {127},
	issn = {09333657},
	shorttitle = {Breast cancer detection using artificial intelligence techniques},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0933365722000410},
	doi = {10.1016/j.artmed.2022.102276},
	abstract = {Cancer is one of the most dangerous diseases to humans, and yet no permanent cure has been developed for it. Breast cancer is one of the most common cancer types. According to the National Breast Cancer foundation, in 2020 alone, more than 276,000 new cases of invasive breast cancer and more than 48,000 non-invasive cases were diagnosed in the US. To put these figures in perspective, 64\% of these cases are diagnosed early in the disease’s cycle, giving patients a 99\% chance of survival. Artificial intelligence and machine learning have been used effectively in detection and treatment of several dangerous diseases, helping in early diagnosis and treatment, and thus increasing the patient’s chance of survival. Deep learning has been designed to analyze the most important features affecting detection and treatment of serious diseases. For example, breast cancer can be detected using genes or histopathological imaging. Analysis at the genetic level is very expensive, so histopathological imaging is the most common approach used to detect breast cancer. In this research work, we systematically reviewed previous work done on detection and treatment of breast cancer using genetic sequencing or histopathological imaging with the help of deep learning and machine learning. We also provide recommendations to researchers who will work in this field.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Artificial Intelligence in Medicine},
	author = {Nassif, Ali Bou and Talib, Manar Abu and Nasir, Qassim and Afadar, Yaman and Elgendy, Omar},
	month = may,
	year = {2022},
	pages = {102276},
	file = {Nassif et al. - 2022 - Breast cancer detection using artificial intellige.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KW4XHZNQ\\Nassif et al. - 2022 - Breast cancer detection using artificial intellige.pdf:application/pdf},
}

@article{abunasserBreastCancerDetection2022,
	title = {Breast {Cancer} {Detection} and {Classification} using {Deep} {Learning} {Xception} {Algorithm}},
	volume = {13},
	issn = {21565570, 2158107X},
	url = {http://thesai.org/Publications/ViewPaper?Volume=13&Issue=7&Code=IJACSA&SerialNo=29},
	doi = {10.14569/IJACSA.2022.0130729},
	abstract = {Breast Cancer (BC) is one of the leading cause of deaths worldwide. Approximately 10 million people pass away internationally from breast cancer in the year 2020. Breast Cancer is a fatal disease and very popular among women globally. It is ranked fourth among the fatal diseases of different cancers, for example colorectal cancer, cervical cancer, and brain tumors. Furthermore, the number of new cases of breast cancer is anticipated to upsurge by 70\% in the next twenty years. Consequently, early detection and precise diagnosis of breast cancer plays an essential part in enhancing the diagnosis and improving the breast cancer survival rate of patients from 30 to 50\%. Through the advances of technology in healthcare, deep learning takes a significant role in handling and inspecting a great number of X-ray, Magnetic Resonance Imaging (MRI), computed tomography (CT) images. The aim of this study is to propose a deep learning model to detect and classify breast cancers. Breast cancers has eight classes of cancers: benign adenosis, benign fibroadenoma, benign phyllodes tumor, benign tubular adenoma, malignant ductal carcinoma, malignant lobular carcinoma, malignant mucinous carcinoma, and malignant papillary carcinoma. The dataset was collected from Kaggle depository for breast cancer detection and classification. The measurement that was used in the evaluation of the proposed model includes: F1-score, recall, precision, accuracy. The proposed model was trained, validated and tested using the preprocessed dataset. The results showed that Precision was (97.60\%), Recall (97.60\%) and F1-Score (97.58\%). This indicates that deep learning models are suitable for detecting and classifying breast cancers precisely.},
	language = {en},
	number = {7},
	urldate = {2023-05-24},
	journal = {International Journal of Advanced Computer Science and Applications},
	author = {Abunasser, Basem S. and AL-Hiealy, Mohammed Rasheed J. and Zaqout, Ihab S. and Abu-Naser, Samy S.},
	year = {2022},
	file = {Abunasser et al. - 2022 - Breast Cancer Detection and Classification using D.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\H7SXYGHR\\Abunasser et al. - 2022 - Breast Cancer Detection and Classification using D.pdf:application/pdf},
}

@inproceedings{rashedDeepLearningApproach2019,
	address = {Cairo Egypt},
	title = {Deep learning approach for breast cancer diagnosis},
	isbn = {978-1-4503-6105-7},
	url = {https://dl.acm.org/doi/10.1145/3328833.3328867},
	doi = {10.1145/3328833.3328867},
	abstract = {Breast cancer is one of the leading fatal disease worldwide with high risk control if early discovered. Conventional method for breast screening is x-ray mammography, which is known to be challenging for early detection of cancer lesions. The dense breast structure produced due to the compression process during imaging lead to difficulties to recognize small size abnormalities. Also, inter- and intra-variations of breast tissues lead to significant difficulties to achieve high diagnosis accuracy using hand-crafted features. Deep learning is an emerging machine learning technology that requires a relatively high computation power. Yet, it proved to be very effective in several difficult tasks that requires decision making at the level of human intelligence. In this paper, we develop a new network architecture inspired by the U-net structure that can be used for effective and early detection of breast cancer. Results indicate a high rate of sensitivity and specificity that indicate potential usefulness of the proposed approach in clinical use.},
	language = {en},
	urldate = {2023-05-24},
	booktitle = {Proceedings of the 2019 8th {International} {Conference} on {Software} and {Information} {Engineering}},
	publisher = {ACM},
	author = {Rashed, Essam and El Seoud, M. Samir Abou},
	month = apr,
	year = {2019},
	pages = {243--247},
	file = {Rashed and El Seoud - 2019 - Deep learning approach for breast cancer diagnosis.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YLKICNXI\\Rashed and El Seoud - 2019 - Deep learning approach for breast cancer diagnosis.pdf:application/pdf},
}

@article{nasserDeepLearningBased2023,
	title = {Deep {Learning} {Based} {Methods} for {Breast} {Cancer} {Diagnosis}: {A} {Systematic} {Review} and {Future} {Direction}},
	volume = {13},
	issn = {2075-4418},
	shorttitle = {Deep {Learning} {Based} {Methods} for {Breast} {Cancer} {Diagnosis}},
	url = {https://www.mdpi.com/2075-4418/13/1/161},
	doi = {10.3390/diagnostics13010161},
	abstract = {Breast cancer is one of the precarious conditions that affect women, and a substantive cure has not yet been discovered for it. With the advent of Artiﬁcial intelligence (AI), recently, deep learning techniques have been used effectively in breast cancer detection, facilitating early diagnosis and therefore increasing the chances of patients’ survival. Compared to classical machine learning techniques, deep learning requires less human intervention for similar feature extraction. This study presents a systematic literature review on the deep learning-based methods for breast cancer detection that can guide practitioners and researchers in understanding the challenges and new trends in the ﬁeld. Particularly, different deep learning-based methods for breast cancer detection are investigated, focusing on the genomics and histopathological imaging data. The study speciﬁcally adopts the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA), which offer a detailed analysis and synthesis of the published articles. Several studies were searched and gathered, and after the eligibility screening and quality evaluation, 98 articles were identiﬁed. The results of the review indicated that the Convolutional Neural Network (CNN) is the most accurate and extensively used model for breast cancer detection, and the accuracy metrics are the most popular method used for performance evaluation. Moreover, datasets utilized for breast cancer detection and the evaluation metrics are also studied. Finally, the challenges and future research direction in breast cancer detection based on deep learning models are also investigated to help researchers and practitioners acquire in-depth knowledge of and insight into the area.},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {Diagnostics},
	author = {Nasser, Maged and Yusof, Umi Kalsom},
	month = jan,
	year = {2023},
	pages = {161},
	file = {Nasser and Yusof - 2023 - Deep Learning Based Methods for Breast Cancer Diag.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FBHXAJ3P\\Nasser and Yusof - 2023 - Deep Learning Based Methods for Breast Cancer Diag.pdf:application/pdf},
}

@article{mahoroApplyingDeepLearning2022,
	title = {Applying {Deep} {Learning} for {Breast} {Cancer} {Detection} in {Radiology}},
	volume = {29},
	issn = {1718-7729},
	url = {https://www.mdpi.com/1718-7729/29/11/690},
	doi = {10.3390/curroncol29110690},
	abstract = {Recent advances in deep learning have enhanced medical imaging research. Breast cancer is the most prevalent cancer among women, and many applications have been developed to improve its early detection. The purpose of this review is to examine how various deep learning methods can be applied to breast cancer screening workﬂows. We summarize deep learning methods, data availability and different screening methods for breast cancer including mammography, thermography, ultrasound and magnetic resonance imaging. In this review, we will explore deep learning in diagnostic breast imaging and describe the literature review. As a conclusion, we discuss some of the limitations and opportunities of integrating artiﬁcial intelligence into breast cancer clinical practice.},
	language = {en},
	number = {11},
	urldate = {2023-05-24},
	journal = {Current Oncology},
	author = {Mahoro, Ella and Akhloufi, Moulay A.},
	month = nov,
	year = {2022},
	pages = {8767--8793},
	file = {Mahoro and Akhloufi - 2022 - Applying Deep Learning for Breast Cancer Detection.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ERKFB7PK\\Mahoro and Akhloufi - 2022 - Applying Deep Learning for Breast Cancer Detection.pdf:application/pdf},
}

@article{shenDeepLearningImprove2019b,
	title = {Deep {Learning} to {Improve} {Breast} {Cancer} {Detection} on {Screening} {Mammography}},
	volume = {9},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-019-48995-4},
	doi = {10.1038/s41598-019-48995-4},
	abstract = {Abstract
            
              The rapid development of deep learning, a family of machine learning techniques, has spurred much interest in its application to medical imaging problems. Here, we develop a deep learning algorithm that can accurately detect breast cancer on screening mammograms using an “end-to-end” training approach that efficiently leverages training datasets with either complete clinical annotation or only the cancer status (label) of the whole image. In this approach, lesion annotations are required only in the initial training stage, and subsequent stages require only image-level labels, eliminating the reliance on rarely available lesion annotations. Our all convolutional network method for classifying screening mammograms attained excellent performance in comparison with previous methods. On an independent test set of digitized film mammograms from the Digital Database for Screening Mammography (CBIS-DDSM), the best single model achieved a per-image AUC of 0.88, and four-model averaging improved the AUC to 0.91 (sensitivity: 86.1\%, specificity: 80.1\%). On an independent test set of full-field digital mammography (FFDM) images from the INbreast database, the best single model achieved a per-image AUC of 0.95, and four-model averaging improved the AUC to 0.98 (sensitivity: 86.7\%, specificity: 96.1\%). We also demonstrate that a whole image classifier trained using our end-to-end approach on the CBIS-DDSM digitized film mammograms can be transferred to INbreast FFDM images using only a subset of the INbreast data for fine-tuning and without further reliance on the availability of lesion annotations. These findings show that automatic deep learning methods can be readily trained to attain high accuracy on heterogeneous mammography platforms, and hold tremendous promise for improving clinical tools to reduce false positive and false negative screening mammography results. Code and model available at:
              https://github.com/lishen/end2end-all-conv
              .},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {Scientific Reports},
	author = {Shen, Li and Margolies, Laurie R. and Rothstein, Joseph H. and Fluder, Eugene and McBride, Russell and Sieh, Weiva},
	month = aug,
	year = {2019},
	pages = {12495},
	file = {Shen et al. - 2019 - Deep Learning to Improve Breast Cancer Detection o.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Z7485HKJ\\Shen et al. - 2019 - Deep Learning to Improve Breast Cancer Detection o.pdf:application/pdf},
}

@article{mridhaComprehensiveSurveyDeepLearningBased2021a,
	title = {A {Comprehensive} {Survey} on {Deep}-{Learning}-{Based} {Breast} {Cancer} {Diagnosis}},
	volume = {13},
	issn = {2072-6694},
	url = {https://www.mdpi.com/2072-6694/13/23/6116},
	doi = {10.3390/cancers13236116},
	abstract = {Breast cancer is now the most frequently diagnosed cancer in women, and its percentage is gradually increasing. Optimistically, there is a good chance of recovery from breast cancer if identiﬁed and treated at an early stage. Therefore, several researchers have established deep-learningbased automated methods for their efﬁciency and accuracy in predicting the growth of cancer cells utilizing medical imaging modalities. As of yet, few review studies on breast cancer diagnosis are available that summarize some existing studies. However, these studies were unable to address emerging architectures and modalities in breast cancer diagnosis. This review focuses on the evolving architectures of deep learning for breast cancer detection. In what follows, this survey presents existing deep-learning-based architectures, analyzes the strengths and limitations of the existing studies, examines the used datasets, and reviews image pre-processing techniques. Furthermore, a concrete review of diverse imaging modalities, performance metrics and results, challenges, and research directions for future researchers is presented.},
	language = {en},
	number = {23},
	urldate = {2023-05-24},
	journal = {Cancers},
	author = {Mridha, Muhammad Firoz and Hamid, Md. Abdul and Monowar, Muhammad Mostafa and Keya, Ashfia Jannat and Ohi, Abu Quwsar and Islam, Md. Rashedul and Kim, Jong-Myon},
	month = dec,
	year = {2021},
	pages = {6116},
	file = {Mridha et al. - 2021 - A Comprehensive Survey on Deep-Learning-Based Brea.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CQDBZECW\\Mridha et al. - 2021 - A Comprehensive Survey on Deep-Learning-Based Brea.pdf:application/pdf},
}

@inproceedings{huBreastDensitySegmentation2022,
	address = {Amsterdam Netherlands},
	title = {Breast {Density} {Segmentation} in {Mammograms} {Based} on {Dual} {Attention} {Mechanism}},
	isbn = {978-1-4503-9844-2},
	url = {https://dl.acm.org/doi/10.1145/3570773.3570873},
	doi = {10.1145/3570773.3570873},
	abstract = {In response to the problem that poor segmentation accuracy results from artifacts in the mammogram, this paper proposes combining the U-Net with Coordinated Attention and Attention Gates to enhance target feature information and suppress irrelevant regions. First of all, the INbreast dataset is preprocessed to remove external artifacts. Second, in the contracting path, enhance features of the Region of Interest(ROI) of the mammogram through the Coordinate Attention Module. Finally, in the expansive path, the local feature enhancement can be achieved by Attention Gates is used instead to combine the shallow layer and upsampling feature maps directly. The experimental results show that the proposed algorithm has a good segmentation effect for the mammogram, and its the Dice Similarity Coefficient (DSC) and the Intersection of Union(IoU) are 91.8\% and 85.8\%, respectively. Furthermore, we obtained DSC and IoU of 98.4\%, 96.8\%, respectively, for women with high breast density. Compared with the conditional Generative Adversarial Networks (cGAN) algorithm, the DSC increased by 3.36\%, IoU increased by 5.91\%. The better segmentation achieved can help doctors accurately judge breast density categories.},
	language = {en},
	urldate = {2023-05-24},
	booktitle = {2022 3rd {International} {Symposium} on {Artificial} {Intelligence} for {Medicine} {Sciences}},
	publisher = {ACM},
	author = {Hu, Jingyu and Liu, Zhiqin and Wang, Qingfeng},
	month = oct,
	year = {2022},
	pages = {430--435},
	file = {Hu et al. - 2022 - Breast Density Segmentation in Mammograms Based on.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EANY3TA5\\Hu et al. - 2022 - Breast Density Segmentation in Mammograms Based on.pdf:application/pdf},
}

@inproceedings{aminDevelopmentSecureCloudbased2022,
	address = {Birmingham United Kingdom},
	title = {Development of a {Secure} {Cloud}-based {Breast} {Cancer} {Diagnosis} {System}},
	isbn = {978-1-4503-9657-8},
	url = {https://dl.acm.org/doi/10.1145/3555962.3555970},
	doi = {10.1145/3555962.3555970},
	abstract = {Breast cancer is one of the most common cancers that cause death in women. Breast cancer was the most common cancer in 2020, with 2.26 million new cases. Cancer mortalities can be reduced by early detection and treatment. There are two components of early detection: early diagnosis and screening. Our work is related to early diagnosis. Early diagnosis helps to start treatment early, which is more effective and less expensive. The lives of cancer patients can be improved significantly by early detection and avoiding treatment delays. This paper presents the design and development of a secure cloud-based breast cancer diagnosis system. The proposed system uses using convolutional neural network for breast mass classification. Fragile zero watermarking will be implemented to secure the transmission of mammographs. The proposed system will enable women living in remote areas to get the facility for the routine initial screening near their homes without making their identity and medical data vulnerable. It will reduce the cost and time by avoiding frequent visits of the patients to specialized health care centers. On the other hand, it will help enhance the quality of service at specialized healthcare centers by reducing the number of patients in the initial screening.},
	language = {en},
	urldate = {2023-05-24},
	booktitle = {Proceedings of the 2022 6th {International} {Conference} on {Cloud} and {Big} {Data} {Computing}},
	publisher = {ACM},
	author = {Amin, Fazal-E- and Hussain, Muhammad and Ali, Zulfiqar and Busaleh, Mariam and Al Sultan, Sarah A.},
	month = aug,
	year = {2022},
	pages = {42--48},
	file = {Amin et al. - 2022 - Development of a Secure Cloud-based Breast Cancer .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9QRATJTT\\Amin et al. - 2022 - Development of a Secure Cloud-based Breast Cancer .pdf:application/pdf},
}

@incollection{asaduzzamanImageAnalysisMachine2021,
	address = {Cham},
	title = {Image {Analysis} with {Machine} {Learning} {Algorithms} to {Assist} {Breast} {Cancer} {Treatment}},
	volume = {207},
	isbn = {978-3-030-75489-1 978-3-030-75490-7},
	url = {https://link.springer.com/10.1007/978-3-030-75490-7_12},
	language = {en},
	urldate = {2023-05-24},
	booktitle = {Vision, {Sensing} and {Analytics}: {Integrative} {Approaches}},
	publisher = {Springer International Publishing},
	author = {Asaduzzaman, Abu and Sibai, Fadi N. and Kanaya, Shigehiko and Altaf-Ul-Amin, Md. and Jashim Uddin, Md. and Chidella, Kishore K. and Mitra, Parthib},
	editor = {Ahad, Md Atiqur Rahman and Inoue, Atsushi},
	year = {2021},
	doi = {10.1007/978-3-030-75490-7_12},
	note = {Series Title: Intelligent Systems Reference Library},
	pages = {327--355},
	file = {Asaduzzaman et al. - 2021 - Image Analysis with Machine Learning Algorithms to.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\799NR4X4\\Asaduzzaman et al. - 2021 - Image Analysis with Machine Learning Algorithms to.pdf:application/pdf},
}

@article{omerPreprocessingDigitalMammogram2017,
	title = {Preprocessing of {Digital} {Mammogram} {Image} {Based} on {Otsu}’s {Threshold}},
	volume = {37},
	abstract = {Mammograms are difficult images to interpret. Hence, a preprocessing stage is very important to standardize the mammogram image along with the reduction of its size, and improve the quality of image in order to produce reliable image for CAD system. The proposed technique of preprocessing involves removal of unwanted parts from background of the mammogram, removal of pectoral muscle, and image enhancement. Binarization based on Otsu’s threshold is a main process in all preprocessing steps. Multi-level thresholding applied to segment the pectoral muscle, and level three shows perfect results of pectoral muscle segmentation. A propose method applied on 160 images from MIAS database. Using of level-three multi-thresholding technique, the success rate was 96\% in mammogram preprocessing stage.},
	language = {en},
	number = {1},
	author = {Omer, Ashgan M and Elfadil, Mohammed},
	year = {2017},
	file = {Omer and Elfadil - 2017 - Preprocessing of Digital Mammogram Image Based on .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\H5IXZLQE\\Omer and Elfadil - 2017 - Preprocessing of Digital Mammogram Image Based on .pdf:application/pdf},
}

@article{ittannavarComparativeAssessmentImage2020,
	title = {Comparative {Assessment} of {Image} {Processing} {Techniques} for the {Early} {Detection} of {Breast} {Cancer}: {A} {Review}},
	volume = {13},
	issn = {09746455, 23214007},
	shorttitle = {Comparative {Assessment} of {Image} {Processing} {Techniques} for the {Early} {Detection} of {Breast} {Cancer}},
	url = {https://bbrc.in/wp-content/uploads/2021/03/Spcial-Issue-13-13-12.pdf},
	doi = {10.21786/bbrc/13.13/12},
	abstract = {The most prevalence of breast disease in ladies is elevated in modern-day years. Some of the automatic feature extraction and classification strategies are used at some stage in the method of breast cancer analysis. Most usually used strategies in this discipline is primarily based on image processing. It is carried out by using mammograms, ultrasound, and MRI. This paper gives systematic evaluation on current image processing based breast most cancers detection techniques that are proposed in 2008 to 2018. The reason of this overview is to summarize and synthesize this evaluation on breast cancer genocide attention and measure the info towards work out capacity consequences for examine. Prospective evaluation lessons are referred to shape a numerous goal and economical CAD methods. Modern-day status of cad structures in line with the use of photograph visuals and also the classifiers works based on machine learning. Various machine learning techniques utilized for breast cancer detection was discussed. The performance of different CAD methods proposed during 2008 to 2018 were estimated and found that up to 99\% of accuracy was acquired by such CAD techniques. This study aimed to expose the best imaging technique for detecting the breast cancer more accurately and found that the MRI based CNN techniques achieved better results than other techniques in terms of accuracy, specificity, and sensitivity.},
	language = {en},
	number = {13},
	urldate = {2023-05-24},
	journal = {Bioscience Biotechnology Research Communications},
	author = {Ittannavar, S.S},
	month = dec,
	year = {2020},
	pages = {81--94},
	file = {Ittannavar - 2020 - Comparative Assessment of Image Processing Techniq.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SXFY544P\\Ittannavar - 2020 - Comparative Assessment of Image Processing Techniq.pdf:application/pdf},
}

@article{chartrandDeepLearningPrimer2017,
	title = {Deep {Learning}: {A} {Primer} for {Radiologists}},
	volume = {37},
	issn = {0271-5333, 1527-1323},
	shorttitle = {Deep {Learning}},
	url = {http://pubs.rsna.org/doi/10.1148/rg.2017170077},
	doi = {10.1148/rg.2017170077},
	language = {en},
	number = {7},
	urldate = {2023-05-24},
	journal = {RadioGraphics},
	author = {Chartrand, Gabriel and Cheng, Phillip M. and Vorontsov, Eugene and Drozdzal, Michal and Turcotte, Simon and Pal, Christopher J. and Kadoury, Samuel and Tang, An},
	month = nov,
	year = {2017},
	pages = {2113--2131},
	file = {Chartrand et al. - 2017 - Deep Learning A Primer for Radiologists.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LVS8NHLZ\\Chartrand et al. - 2017 - Deep Learning A Primer for Radiologists.pdf:application/pdf},
}

@article{leeCuratedMammographyData2017,
	title = {A curated mammography data set for use in computer-aided detection and diagnosis research},
	volume = {4},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/sdata2017177},
	doi = {10.1038/sdata.2017.177},
	abstract = {Abstract
            Published research results are difficult to replicate due to the lack of a standard evaluation data set in the area of decision support systems in mammography; most computer-aided diagnosis (CADx) and detection (CADe) algorithms for breast cancer in mammography are evaluated on private data sets or on unspecified subsets of public databases. This causes an inability to directly compare the performance of methods or to replicate prior results. We seek to resolve this substantial challenge by releasing an updated and standardized version of the Digital Database for Screening Mammography (DDSM) for evaluation of future CADx and CADe systems (sometimes referred to generally as CAD) research in mammography. Our data set, the CBIS-DDSM (Curated Breast Imaging Subset of DDSM), includes decompressed images, data selection and curation by trained mammographers, updated mass segmentation and bounding boxes, and pathologic diagnosis for training data, formatted similarly to modern computer vision data sets. The data set contains 753 calcification cases and 891 mass cases, providing a data-set size capable of analyzing decision support systems in mammography.},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {Scientific Data},
	author = {Lee, Rebecca Sawyer and Gimenez, Francisco and Hoogi, Assaf and Miyake, Kanae Kawai and Gorovoy, Mia and Rubin, Daniel L.},
	month = dec,
	year = {2017},
	pages = {170177},
	file = {Lee et al. - 2017 - A curated mammography data set for use in computer.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YJ6YRB6P\\Lee et al. - 2017 - A curated mammography data set for use in computer.pdf:application/pdf},
}

@article{moreiraINbreast2012,
	title = {{INbreast}},
	volume = {19},
	issn = {10766332},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S107663321100451X},
	doi = {10.1016/j.acra.2011.09.014},
	language = {en},
	number = {2},
	urldate = {2023-05-24},
	journal = {Academic Radiology},
	author = {Moreira, Inês C. and Amaral, Igor and Domingues, Inês and Cardoso, António and Cardoso, Maria João and Cardoso, Jaime S.},
	month = feb,
	year = {2012},
	pages = {236--248},
	file = {Moreira et al. - 2012 - INbreast.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JZZDPQ5Y\\Moreira et al. - 2012 - INbreast.pdf:application/pdf},
}

@article{shenDeepLearningMedical2017a,
	title = {Deep {Learning} in {Medical} {Image} {Analysis}},
	abstract = {This review covers computer-assisted analysis of images in the ﬁeld of medical imaging. Recent advances in machine learning, especially with regard to deep learning, are helping to identify, classify, and quantify patterns in medical images. At the core of these advances is the ability to exploit hierarchical feature representations learned solely from data, instead of features designed by hand according to domain-speciﬁc knowledge. Deep learning is rapidly becoming the state of the art, leading to enhanced performance in various medical applications. We introduce the fundamentals of deep learning methods and review their successes in image registration, detection of anatomical and cellular structures, tissue segmentation, computer-aided disease diagnosis and prognosis, and so on. We conclude by discussing research issues and suggesting future directions for further improvement.},
	language = {en},
	author = {Shen, Dinggang and Wu, Guorong and Suk, Heung-Il},
	year = {2017},
	file = {Shen et al. - 2017 - Deep Learning in Medical Image Analysis.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BM5UIUYH\\Shen et al. - 2017 - Deep Learning in Medical Image Analysis.pdf:application/pdf},
}

@article{anaya-isazaOverviewDeepLearning2021,
	title = {An overview of deep learning in medical imaging},
	volume = {26},
	issn = {23529148},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2352914821002033},
	doi = {10.1016/j.imu.2021.100723},
	abstract = {Deep learning (DL) is one of the branches of artificial intelligence that has seen exponential growth in recent years. The scientific community has focused its attention on DL due to its versatility, high performance, high generalization capacity, and multidisciplinary uses, among many other qualities. In addition, a large amount of medical data and the development of more powerful computers has also fostered an interest in this area. This paper presents an overview of current deep learning methods, starting from the most straightforward concept but accompanied by the mathematical models that are behind the functionality of this type of intelligence. In the first instance, the fundamental concept of artificial neural networks is introduced, progressively covering convolu­ tional structures, recurrent networks, attention models, up to the current structure known as the Transformer. Secondly, all the basic concepts involved in training and other common elements in the design of the archi­ tectures are introduced. Thirdly, some of the key elements in modern networks for medical image classification and segmentation are shown. Subsequently, a review of some applications realized in the last years is shown, where the main features related to DL are highlighted. Finally, the perspectives and future expectations of deep learning are presented.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Informatics in Medicine Unlocked},
	author = {Anaya-Isaza, Andrés and Mera-Jiménez, Leonel and Zequera-Diaz, Martha},
	year = {2021},
	pages = {100723},
	file = {Anaya-Isaza et al. - 2021 - An overview of deep learning in medical imaging.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\97CSIR88\\Anaya-Isaza et al. - 2021 - An overview of deep learning in medical imaging.pdf:application/pdf},
}

@article{pesapaneArtificialIntelligenceMedical2018,
	title = {Artificial intelligence in medical imaging: threat or opportunity? {Radiologists} again at the forefront of innovation in medicine},
	volume = {2},
	issn = {2509-9280},
	shorttitle = {Artificial intelligence in medical imaging},
	url = {https://eurradiolexp.springeropen.com/articles/10.1186/s41747-018-0061-6},
	doi = {10.1186/s41747-018-0061-6},
	abstract = {One of the most promising areas of health innovation is the application of artificial intelligence (AI), primarily in medical imaging. This article provides basic definitions of terms such as “machine/deep learning” and analyses the integration of AI into radiology. Publications on AI have drastically increased from about 100–150 per year in 2007–2008 to 700–800 per year in 2016–2017. Magnetic resonance imaging and computed tomography collectively account for more than 50\% of current articles. Neuroradiology appears in about one-third of the papers, followed by musculoskeletal, cardiovascular, breast, urogenital, lung/thorax, and abdomen, each representing 6–9\% of articles. With an irreversible increase in the amount of data and the possibility to use AI to identify findings either detectable or not by the human eye, radiology is now moving from a subjective perceptual skill to a more objective science. Radiologists, who were on the forefront of the digital era in medicine, can guide the introduction of AI into healthcare. Yet, they will not be replaced because radiology includes communication of diagnosis, consideration of patient’s values and preferences, medical judgment, quality assurance, education, policy-making, and interventional procedures. The higher efficiency provided by AI will allow radiologists to perform more value-added tasks, becoming more visible to patients and playing a vital role in multidisciplinary clinical teams.},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {European Radiology Experimental},
	author = {Pesapane, Filippo and Codari, Marina and Sardanelli, Francesco},
	month = dec,
	year = {2018},
	pages = {35},
	file = {Pesapane et al. - 2018 - Artificial intelligence in medical imaging threat.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MYBVLUEC\\Pesapane et al. - 2018 - Artificial intelligence in medical imaging threat.pdf:application/pdf},
}

@article{topolHighperformanceMedicineConvergence2019,
	title = {High-performance medicine: the convergence of human and artificial intelligence},
	volume = {25},
	issn = {1078-8956, 1546-170X},
	shorttitle = {High-performance medicine},
	url = {https://www.nature.com/articles/s41591-018-0300-7},
	doi = {10.1038/s41591-018-0300-7},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {Nature Medicine},
	author = {Topol, Eric J.},
	month = jan,
	year = {2019},
	pages = {44--56},
	file = {Topol - 2019 - High-performance medicine the convergence of huma.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\F4Q555VA\\Topol - 2019 - High-performance medicine the convergence of huma.pdf:application/pdf},
}

@article{lambinRadiomicsBridgeMedical2017,
	title = {Radiomics: the bridge between medical imaging and personalized medicine},
	volume = {14},
	issn = {1759-4774, 1759-4782},
	shorttitle = {Radiomics},
	url = {https://www.nature.com/articles/nrclinonc.2017.141},
	doi = {10.1038/nrclinonc.2017.141},
	abstract = {Radiomics, the high-throughput mining of quantitative image features from standard-of‑care medical imaging that enables data to be extracted and applied within clinical-decision support systems to improve diagnostic, prognostic, and predictive accuracy, is gaining importance in cancer research. Radiomic analysis exploits sophisticated image analysis tools and the rapid development and validation of medical imaging data that uses image-based signatures for precision diagnosis and treatment, providing a powerful tool in modern medicine. Herein, we describe the process of radiomics, its pitfalls, challenges, opportunities, and its capacity to improve clinical decision making, emphasizing the utility for patients with cancer. Currently, the field of radiomics lacks standardized evaluation of both the scientific integrity and the clinical relevance of the numerous published radiomics investigations resulting from the rapid growth of this area. Rigorous evaluation criteria and reporting guidelines need to be established in order for radiomics to mature as a discipline. Herein, we provide guidance for investigations to meet this urgent need in the field of radiomics.},
	language = {en},
	number = {12},
	urldate = {2023-05-24},
	journal = {Nature Reviews Clinical Oncology},
	author = {Lambin, Philippe and Leijenaar, Ralph T.H. and Deist, Timo M. and Peerlings, Jurgen and De Jong, Evelyn E.C. and Van Timmeren, Janita and Sanduleanu, Sebastian and Larue, Ruben T.H.M. and Even, Aniek J.G. and Jochems, Arthur and Van Wijk, Yvonka and Woodruff, Henry and Van Soest, Johan and Lustberg, Tim and Roelofs, Erik and Van Elmpt, Wouter and Dekker, Andre and Mottaghy, Felix M. and Wildberger, Joachim E. and Walsh, Sean},
	month = dec,
	year = {2017},
	pages = {749--762},
	file = {Lambin et al. - 2017 - Radiomics the bridge between medical imaging and .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\HVEXT2A5\\Lambin et al. - 2017 - Radiomics the bridge between medical imaging and .pdf:application/pdf},
}

@article{estevaDeepLearningenabledMedical2021,
	title = {Deep learning-enabled medical computer vision},
	volume = {4},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-020-00376-2},
	doi = {10.1038/s41746-020-00376-2},
	abstract = {Abstract
            A decade of unprecedented progress in artificial intelligence (AI) has demonstrated the potential for many fields—including medicine—to benefit from the insights that AI techniques can extract from data. Here we survey recent progress in the development of modern computer vision techniques—powered by deep learning—for medical applications, focusing on medical imaging, medical video, and clinical deployment. We start by briefly summarizing a decade of progress in convolutional neural networks, including the vision tasks they enable, in the context of healthcare. Next, we discuss several example medical imaging applications that stand to benefit—including cardiology, pathology, dermatology, ophthalmology–and propose new avenues for continued work. We then expand into general medical video, highlighting ways in which clinical workflows can integrate computer vision to enhance care. Finally, we discuss the challenges and hurdles required for real-world clinical deployment of these technologies.},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {npj Digital Medicine},
	author = {Esteva, Andre and Chou, Katherine and Yeung, Serena and Naik, Nikhil and Madani, Ali and Mottaghi, Ali and Liu, Yun and Topol, Eric and Dean, Jeff and Socher, Richard},
	month = jan,
	year = {2021},
	pages = {5},
	file = {Esteva et al. - 2021 - Deep learning-enabled medical computer vision.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CCZP2BVJ\\Esteva et al. - 2021 - Deep learning-enabled medical computer vision.pdf:application/pdf},
}

@article{gilliesRadiomicsImagesAre2016,
	title = {Radiomics: {Images} {Are} {More} than {Pictures}, {They} {Are} {Data}},
	volume = {278},
	issn = {0033-8419, 1527-1315},
	shorttitle = {Radiomics},
	url = {http://pubs.rsna.org/doi/10.1148/radiol.2015151169},
	doi = {10.1148/radiol.2015151169},
	language = {en},
	number = {2},
	urldate = {2023-05-24},
	journal = {Radiology},
	author = {Gillies, Robert J. and Kinahan, Paul E. and Hricak, Hedvig},
	month = feb,
	year = {2016},
	pages = {563--577},
	file = {Gillies et al. - 2016 - Radiomics Images Are More than Pictures, They Are.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WZDRRCZT\\Gillies et al. - 2016 - Radiomics Images Are More than Pictures, They Are.pdf:application/pdf},
}

@article{rajpurkarAIHealthMedicine2022,
	title = {{AI} in health and medicine},
	volume = {28},
	issn = {1078-8956, 1546-170X},
	url = {https://www.nature.com/articles/s41591-021-01614-0},
	doi = {10.1038/s41591-021-01614-0},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {Nature Medicine},
	author = {Rajpurkar, Pranav and Chen, Emma and Banerjee, Oishi and Topol, Eric J.},
	month = jan,
	year = {2022},
	pages = {31--38},
	file = {Rajpurkar et al. - 2022 - AI in health and medicine.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EAXHGKDZ\\Rajpurkar et al. - 2022 - AI in health and medicine.pdf:application/pdf},
}

@article{papanikolaouHowDevelopMeaningful2020,
	title = {How to develop a meaningful radiomic signature for clinical use in oncologic patients},
	volume = {20},
	issn = {1470-7330},
	url = {https://cancerimagingjournal.biomedcentral.com/articles/10.1186/s40644-020-00311-4},
	doi = {10.1186/s40644-020-00311-4},
	abstract = {During the last decade, there is an increasing usage of quantitative methods in Radiology in an effort to reduce the diagnostic variability associated with a subjective manner of radiological interpretation. Combined approaches where visual assessment made by the radiologist is augmented by quantitative imaging biomarkers are gaining attention. Advances in machine learning resulted in the rise of radiomics that is a new methodology referring to the extraction of quantitative information from medical images. Radiomics are based on the development of computational models, referred to as “Radiomic Signatures”, trying to address either unmet clinical needs, mostly in the field of oncologic imaging, or to compare radiomics performance with that of radiologists. However, to explore this new technology, initial publications did not consider best practices in the field of machine learning resulting in publications with questionable clinical value. In this paper, our effort was concentrated on how to avoid methodological mistakes and consider critical issues in the workflow of the development of clinically meaningful radiomic signatures.},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {Cancer Imaging},
	author = {Papanikolaou, Nikolaos and Matos, Celso and Koh, Dow Mu},
	month = dec,
	year = {2020},
	pages = {33},
	file = {Papanikolaou et al. - 2020 - How to develop a meaningful radiomic signature for.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ESFM4XSM\\Papanikolaou et al. - 2020 - How to develop a meaningful radiomic signature for.pdf:application/pdf},
}

@article{limkinPromisesChallengesImplementation2017,
	title = {Promises and challenges for the implementation of computational medical imaging (radiomics) in oncology},
	volume = {28},
	issn = {09237534},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0923753419324123},
	doi = {10.1093/annonc/mdx034},
	abstract = {Medical image processing and analysis (also known as Radiomics) is a rapidly growing discipline that maps digital medical images into quantitative data, with the end goal of generating imaging biomarkers as decision support tools for clinical practice. The use of imaging data from routine clinical work-up has tremendous potential in improving cancer care by heightening understanding of tumor biology and aiding in the implementation of precision medicine. As a noninvasive method of assessing the tumor and its microenvironment in their entirety, radiomics allows the evaluation and monitoring of tumor characteristics such as temporal and spatial heterogeneity. One can observe a rapid increase in the number of computational medical imaging publications—milestones that have highlighted the utility of imaging biomarkers in oncology. Nevertheless, the use of radiomics as clinical biomarkers still necessitates amelioration and standardization in order to achieve routine clinical adoption. This Review addresses the critical issues to ensure the proper development of radiomics as a biomarker and facilitate its implementation in clinical practice.},
	language = {en},
	number = {6},
	urldate = {2023-05-24},
	journal = {Annals of Oncology},
	author = {Limkin, E.J. and Sun, R. and Dercle, L. and Zacharaki, E.I. and Robert, C. and Reuzé, S. and Schernberg, A. and Paragios, N. and Deutsch, E. and Ferté, C.},
	month = jun,
	year = {2017},
	pages = {1191--1206},
	file = {Limkin et al. - 2017 - Promises and challenges for the implementation of .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NF9C4FU6\\Limkin et al. - 2017 - Promises and challenges for the implementation of .pdf:application/pdf},
}

@article{alghaibOverviewMammogramAnalysis2016,
	title = {An {Overview} of {Mammogram} {Analysis}},
	volume = {35},
	issn = {0278-6648},
	url = {http://ieeexplore.ieee.org/document/7743059/},
	doi = {10.1109/MPOT.2015.2396533},
	language = {en},
	number = {6},
	urldate = {2023-05-24},
	journal = {IEEE Potentials},
	author = {Alghaib, Huda A and Scott, Melanie and Adhami, Reza R},
	month = nov,
	year = {2016},
	pages = {21--28},
	file = {Alghaib et al. - 2016 - An Overview of Mammogram Analysis.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Y4JX8V5D\\Alghaib et al. - 2016 - An Overview of Mammogram Analysis.pdf:application/pdf},
}

@incollection{bozekSurveyImageProcessing2009,
	address = {Berlin, Heidelberg},
	title = {A {Survey} of {Image} {Processing} {Algorithms} in {Digital} {Mammography}},
	volume = {231},
	isbn = {978-3-642-02899-1 978-3-642-02900-4},
	url = {http://link.springer.com/10.1007/978-3-642-02900-4_24},
	abstract = {Mammography is at present the best available technique for early detection of breast cancer. The most common breast abnormalities that may indicate breast cancer are masses and calcifications. In some cases, subtle signs that can also lead to a breast cancer diagnosis, such as architectural distortion and bilateral asymmetry, are present. Breast abnormalities are defined with wide range of features and may be easily missed or misinterpreted by radiologists while reading large amount of mammographic images provided in screening programs. To help radiologists provide an accurate diagnosis, a computer-aided detection (CADe) and computer-aided diagnosis (CADx) algorithms are being developed. CADe and CADx algorithms help reducing the number of false positives and they assist radiologists in deciding between follow up and biopsy. This chapter gives a survey of image processing algorithms that have been developed for detection of masses and calcifications. An overview of algorithms in each step (segmentation step, feature extraction step, feature selection step, classification step) of the mass detection algorithms is given. Wavelet detection methods and other recently proposed methods for calcification detection are presented. An overview of contrast enhancement and noise equalization methods is given as well as an overview of calcification classification algorithms.},
	language = {en},
	urldate = {2023-05-24},
	booktitle = {Recent {Advances} in {Multimedia} {Signal} {Processing} and {Communications}},
	publisher = {Springer Berlin Heidelberg},
	author = {Bozek, Jelena and Mustra, Mario and Delac, Kresimir and Grgic, Mislav},
	editor = {Kacprzyk, Janusz and Grgic, Mislav and Delac, Kresimir and Ghanbari, Mohammed},
	year = {2009},
	doi = {10.1007/978-3-642-02900-4_24},
	note = {Series Title: Studies in Computational Intelligence},
	pages = {631--657},
	file = {Bozek et al. - 2009 - A Survey of Image Processing Algorithms in Digital.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IJQI6NZR\\Bozek et al. - 2009 - A Survey of Image Processing Algorithms in Digital.pdf:application/pdf},
}

@article{katzenReviewComputerAided2018,
	title = {A review of computer aided detection in mammography},
	volume = {52},
	issn = {08997071},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0899707118302341},
	doi = {10.1016/j.clinimag.2018.08.014},
	abstract = {Breast screening with mammography is widely recognized as the most eﬀective method of detecting early breast cancer and has consistently demonstrated a 20–40\% decrease in mortality among screened women. Despite this, the sensitivity of mammography ranges between 70 and 90\%. Computer aided detection (CAD) is an artiﬁcial intelligence (AI) technique that utilizes pattern recognition to highlight suspicious features on imaging and marks them for the radiologist to review and interpret. It aims to decrease oversights made by interpreting radiologists. Here we review the eﬃcacy of CAD and potential future directions.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Clinical Imaging},
	author = {Katzen, Janine and Dodelzon, Katerina},
	month = nov,
	year = {2018},
	pages = {305--309},
	file = {Katzen and Dodelzon - 2018 - A review of computer aided detection in mammograph.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\V5RH5X27\\Katzen and Dodelzon - 2018 - A review of computer aided detection in mammograph.pdf:application/pdf},
}

@article{kohliWhyCADFailed2018,
	title = {Why {CAD} {Failed} in {Mammography}},
	volume = {15},
	issn = {15461440},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1546144017316745},
	doi = {10.1016/j.jacr.2017.12.029},
	language = {en},
	number = {3},
	urldate = {2023-05-24},
	journal = {Journal of the American College of Radiology},
	author = {Kohli, Ajay and Jha, Saurabh},
	month = mar,
	year = {2018},
	pages = {535--537},
	file = {Kohli and Jha - 2018 - Why CAD Failed in Mammography.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4PT3F4WM\\Kohli and Jha - 2018 - Why CAD Failed in Mammography.pdf:application/pdf},
}

@article{chengApproachesAutomatedDetection2006,
	title = {Approaches for automated detection and classification of masses in mammograms},
	volume = {39},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320305002955},
	doi = {10.1016/j.patcog.2005.07.006},
	abstract = {Breast cancer continues to be a signiﬁcant public health problem in the world. Early detection is the key for improving breast cancer prognosis. Mammography has been one of the most reliable methods for early detection of breast carcinomas. However, it is difﬁcult for radiologists to provide both accurate and uniform evaluation for the enormous mammograms generated in widespread screening. The estimated sensitivity of radiologists in breast cancer screening is only about 75\%, but the performance would be improved if they were prompted with the possible locations of abnormalities. Breast cancer CAD systems can provide such help and they are important and necessary for breast cancer control. Microcalciﬁcations and masses are the two most important indicators of malignancy, and their automated detection is very valuable for early breast cancer diagnosis. Since masses are often indistinguishable from the surrounding parenchymal, automated mass detection and classiﬁcation is even more challenging. This paper discusses the methods for mass detection and classiﬁcation, and compares their advantages and drawbacks.},
	language = {en},
	number = {4},
	urldate = {2023-05-24},
	journal = {Pattern Recognition},
	author = {Cheng, H.D. and Shi, X.J. and Min, R. and Hu, L.M. and Cai, X.P. and Du, H.N.},
	month = apr,
	year = {2006},
	pages = {646--668},
	file = {Cheng et al. - 2006 - Approaches for automated detection and classificat.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RSLQUQZD\\Cheng et al. - 2006 - Approaches for automated detection and classificat.pdf:application/pdf},
}

@article{oliverReviewAutomaticMass2010,
	title = {A review of automatic mass detection and segmentation in mammographic images},
	volume = {14},
	issn = {13618415},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841509001492},
	doi = {10.1016/j.media.2009.12.005},
	abstract = {The aim of this paper is to review existing approaches to the automatic detection and segmentation of masses in mammographic images, highlighting the key-points and main differences between the used strategies. The key objective is to point out the advantages and disadvantages of the various approaches. In contrast with other reviews which only describe and compare different approaches qualitatively, this review also provides a quantitative comparison. The performance of seven mass detection methods is compared using two different mammographic databases: a public digitised database and a local full-ﬁeld digital database. The results are given in terms of Receiver Operating Characteristic (ROC) and Freeresponse Receiver Operating Characteristic (FROC) analysis.},
	language = {en},
	number = {2},
	urldate = {2023-05-24},
	journal = {Medical Image Analysis},
	author = {Oliver, Arnau and Freixenet, Jordi and Martí, Joan and Pérez, Elsa and Pont, Josep and Denton, Erika R.E. and Zwiggelaar, Reyer},
	month = apr,
	year = {2010},
	pages = {87--110},
	file = {Oliver et al. - 2010 - A review of automatic mass detection and segmentat.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2ZYA7B6Z\\Oliver et al. - 2010 - A review of automatic mass detection and segmentat.pdf:application/pdf},
}

@inproceedings{zhengSuperresolutionMammograms2010,
	address = {Montreal, QC, Canada},
	title = {Super-resolution of mammograms},
	isbn = {978-1-4244-6766-2},
	url = {http://ieeexplore.ieee.org/document/5510384/},
	doi = {10.1109/CIBCB.2010.5510384},
	abstract = {High-quality mammography is the most effective technology presently available for breast cancer screening. High resolution mammograms usually lead to more accurate diagnoses; however, they require large doses of radiation, which may have harmful effects. In this paper, we present a method to synthesize high-resolution mammograms from low-resolution inputs, which offers the potential of allowing accurate diagnoses while minimizing risks to patients. Our algorithm combines statistical machine learning methods and stochastic search to learn the mapping from low-resolution to high-resolution mammograms using a large dataset of training image pairs. Experimental results show that the super-resolution algorithm can generate highquality, high-resolution breast mammograms from low-resolution input with no human intervention.},
	language = {en},
	urldate = {2023-05-24},
	booktitle = {2010 {IEEE} {Symposium} on {Computational} {Intelligence} in {Bioinformatics} and {Computational} {Biology}},
	publisher = {IEEE},
	author = {Zheng, Jun and Fuentes, Olac and Leung, Ming-Ying},
	month = may,
	year = {2010},
	pages = {1--7},
	file = {Zheng et al. - 2010 - Super-resolution of mammograms.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IPZFZS8Y\\Zheng et al. - 2010 - Super-resolution of mammograms.pdf:application/pdf},
}

@article{sunMultiViewConvolutionalNeural2019,
	title = {Multi-{View} {Convolutional} {Neural} {Networks} for {Mammographic} {Image} {Classification}},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8822935/},
	doi = {10.1109/ACCESS.2019.2939167},
	abstract = {In recent years, deep learning has been widely applied for mammographic image classiﬁcation. However, most of the existing methods are based on a single mammography view and cannot sufﬁciently extract discriminative features, thereby resulting in an unsatisfactory classiﬁcation accuracy. To solve this problem and improve the mammographic image classiﬁcation performance, we propose a novel multi-view convolutional neural network (CNN) based on multiple mammography views in this paper. Considering that the images acquired from different perspectives contain different and complementary breast mass information, we modify the CNN architecture to exploit the complementary information from the various views of mammography. The new architecture can extract discriminative features from the mediolateral oblique (MLO) and craniocaudal (CC) views of the mammographic images and can effectively incorporate these features for mammographic images. The dilated convolutional layers enable the feature maps extracted from the multiple breast mass views to capture information from a large ‘‘ﬁeld of vision’’. Moreover, multiscale features are obtained by using the convolutional and dilated convolutions. In addition, we incorporate a penalty term into the cross entropy loss function, which enables the model evolution to reduce the misclassiﬁcation rate by enhancing the contributions of the samples misclassiﬁed in the training process. The proposed method was evaluated and compared with several state-of-the-art methods on the open Digital Database for Screening Mammography (DDSM) and Mammographic Image Analysis Society (MIAS) datasets. The experimental results show that the proposed method exhibits a better performance than those of the state-of-the-art methods.},
	language = {en},
	urldate = {2023-05-24},
	journal = {IEEE Access},
	author = {Sun, Lilei and Wang, Junqian and Hu, Zhijun and Xu, Yong and Cui, Zhongwei},
	year = {2019},
	pages = {126273--126282},
	file = {Sun et al. - 2019 - Multi-View Convolutional Neural Networks for Mammo.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CVBMGSTW\\Sun et al. - 2019 - Multi-View Convolutional Neural Networks for Mammo.pdf:application/pdf},
}

@article{liMultiViewMammographicDensity2021,
	title = {Multi-{View} {Mammographic} {Density} {Classification} by {Dilated} and {Attention}-{Guided} {Residual} {Learning}},
	volume = {18},
	issn = {1545-5963, 1557-9964, 2374-0043},
	url = {https://ieeexplore.ieee.org/document/8978513/},
	doi = {10.1109/TCBB.2020.2970713},
	abstract = {Breast density is widely adopted to reﬂect the likelihood of early breast cancer development. Existing methods of mammographic density classiﬁcation either require steps of manual operations or achieve only moderate classiﬁcation accuracy due to the limited model capacity. In this study, we present a radiomics approach based on dilated and attention-guided residual learning for the task of mammographic density classiﬁcation. The proposed method was instantiated with two datasets, one clinical dataset and one publicly available dataset, and classiﬁcation accuracies of 88.7 and 70.0 percent were obtained, respectively. Although the classiﬁcation accuracy of the public dataset was lower than the clinical dataset, which was very likely related to the dataset size, our proposed model still achieved a better performance than the naive residual networks and several recently published deep learningbased approaches. Furthermore, we designed a multi-stream network architecture speciﬁcally targeting at analyzing the multi-view mammograms. Utilizing the clinical dataset, we validated that multi-view inputs were beneﬁcial to the breast density classiﬁcation task with an increase of at least 2.0 percent in accuracy and the different views lead to different model classiﬁcation capacities. Our method has a great potential to be further developed and applied in computer-aided diagnosis systems. Our code is available at https://github. com/lich0031/Mammographic\_Density\_Classiﬁcation.},
	language = {en},
	number = {3},
	urldate = {2023-05-24},
	journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
	author = {Li, Cheng and Xu, Jingxu and Liu, Qiegen and Zhou, Yongjin and Mou, Lisha and Pu, Zuhui and Xia, Yong and Zheng, Hairong and Wang, Shanshan},
	month = may,
	year = {2021},
	pages = {1003--1013},
	file = {Li et al. - 2021 - Multi-View Mammographic Density Classification by .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\HVHQBMIF\\Li et al. - 2021 - Multi-View Mammographic Density Classification by .pdf:application/pdf},
}

@article{zhangClassificationWholeMammogram2018,
	title = {Classification of {Whole} {Mammogram} and {Tomosynthesis} {Images} {Using} {Deep} {Convolutional} {Neural} {Networks}},
	volume = {17},
	issn = {1536-1241, 1558-2639},
	url = {https://ieeexplore.ieee.org/document/8374855/},
	doi = {10.1109/TNB.2018.2845103},
	abstract = {Mammography is the most popular technology used for the early detection of breast cancer. Manual classiﬁcation of mammogram images is a hard task because of the variability of the tumor. It yields a noteworthy number of patients being called back to perform biopsies, ensuring no missing diagnosis. The convolutional neural network (CNN) has succeeded in a lot of image classiﬁcation challenges during the recent years. In this paper, we proposed an approach of mammogram and tomosynthesis classiﬁcation based on CNNs. We had acquired more than 3000 mammograms and tomosynthesis data with approval from an institutional review board at the University of Kentucky. Different models of CNNs were built to classify both the 2-D mammograms and 3-D tomosynthesis, and every classiﬁer was assessed with respect to truth-values generated by histology results from the biopsy and twoyear negative mammogram follow-up conﬁrmed by expert radiologists. Our outcomes demonstrated that CNN-based models we had built and optimized utilizing transfer learning and data augmentation have good potential for automatic breast cancer detection based on the mammograms and tomosynthesis data.},
	language = {en},
	number = {3},
	urldate = {2023-05-24},
	journal = {IEEE Transactions on NanoBioscience},
	author = {Zhang, Xiaofei and Zhang, Yi and Han, Erik Y. and Jacobs, Nathan and Han, Qiong and Wang, Xiaoqin and Liu, Jinze},
	month = jul,
	year = {2018},
	pages = {237--242},
	file = {Zhang et al. - 2018 - Classification of Whole Mammogram and Tomosynthesi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SKKYWA6N\\Zhang et al. - 2018 - Classification of Whole Mammogram and Tomosynthesi.pdf:application/pdf},
}

@article{rangarajanUltrahighResolutionMultiscale2022,
	title = {Ultra-high resolution, multi-scale, context-aware approach for detection of small cancers on mammography},
	volume = {12},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-022-15259-7},
	doi = {10.1038/s41598-022-15259-7},
	abstract = {Abstract
            
              While detection of malignancies on mammography has received a boost with the use of Convolutional Neural Networks (CNN), detection of cancers of very small size remains challenging. This is however clinically significant as the purpose of mammography is early detection of cancer, making it imperative to pick them up when they are still very small. Mammography has the highest spatial resolution (image sizes as high as 3328 × 4096 pixels) out of all imaging modalities, a requirement that stems from the need to detect fine features of the smallest cancers on screening. However due to computational constraints, most state of the art CNNs work on reduced resolution images. Those that work on higher resolutions, compromise on global context and work at single scale. In this work, we show that resolution, scale and image-context are all important independent factors in detection of small masses. We thereby use a fully convolutional network, with the ability to take any input size. In addition, we incorporate a systematic multi-scale, multi-resolution approach, and encode image context, which we show are critical factors to detection of small masses. We show that this approach improves the detection of cancer, particularly for small masses in comparison to the baseline model. We perform a single institution multicentre study, and show the performance of the model on a diagnostic mammography dataset, a screening mammography dataset, as well as a curated dataset of small cancers {\textless} 1 cm in size. We show that our approach improves the sensitivity from 61.53 to 87.18\% at 0.3 False Positives per Image (FPI) on this small cancer dataset. Model and code are available from 
              https://github.com/amangupt01/Small\_Cancer\_Detection},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {Scientific Reports},
	author = {Rangarajan, Krithika and Gupta, Aman and Dasgupta, Saptarshi and Marri, Uday and Gupta, Arun Kumar and Hari, Smriti and Banerjee, Subhashis and Arora, Chetan},
	month = jul,
	year = {2022},
	pages = {11622},
	file = {Rangarajan et al. - 2022 - Ultra-high resolution, multi-scale, context-aware .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\D5LLWK9D\\Rangarajan et al. - 2022 - Ultra-high resolution, multi-scale, context-aware .pdf:application/pdf},
}

@incollection{wangBRGANBilateralResidual2020,
	address = {Cham},
	title = {{BR}-{GAN}: {Bilateral} {Residual} {Generating} {Adversarial} {Network} for {Mammogram} {Classification}},
	volume = {12262},
	isbn = {978-3-030-59712-2 978-3-030-59713-9},
	shorttitle = {{BR}-{GAN}},
	url = {https://link.springer.com/10.1007/978-3-030-59713-9_63},
	abstract = {Mammogram malignancy classiﬁcation with only image-level annotations is challenging due to a lack of lesion annotations. If we can generate the healthy version of the diseased data, we can easily explore the lesion features. An intuitive idea of such generation is to use existing Cycle-GAN based methods. They achieve the healthy generation regarding healthy images as reference domain, while maintaining the original content by cycle consistency mechanism. However, healthy mammogram patterns are diverse which may lead to uncertain generations. Moreover, the back translation from healthy to the original remains an ill-posed problem due to lack of lesion information. To address these problems, we propose a novel model called bilateral residual generating adversarial network(BR-GAN). We use the Cycle-GAN as a basic framework while regarding the contralateral as generation reference based on the bilateral symmetry prior. To address the ill-posed back translation problem, we propose a residual-preserved mechanism to try to preserve the lesion features from the original features. The generated features and the original features are aggregated for further classiﬁcation. BR-GAN outperforms current state-of-the-art methods on INBreast and in-house datasets.},
	language = {en},
	urldate = {2023-05-24},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2020},
	publisher = {Springer International Publishing},
	author = {Wang, Chu-ran and Zhang, Fandong and Yu, Yizhou and Wang, Yizhou},
	editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
	year = {2020},
	doi = {10.1007/978-3-030-59713-9_63},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {657--666},
	file = {Wang et al. - 2020 - BR-GAN Bilateral Residual Generating Adversarial .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CBRY9WF2\\Wang et al. - 2020 - BR-GAN Bilateral Residual Generating Adversarial .pdf:application/pdf},
}

@article{elterCADxMammographicMasses2009,
	title = {{CADx} of mammographic masses and clustered microcalcifications: {A} review: {Mammography} {CADx}: {A} review},
	volume = {36},
	issn = {00942405},
	shorttitle = {{CADx} of mammographic masses and clustered microcalcifications},
	url = {http://doi.wiley.com/10.1118/1.3121511},
	doi = {10.1118/1.3121511},
	language = {en},
	number = {6Part1},
	urldate = {2023-05-24},
	journal = {Medical Physics},
	author = {Elter, Matthias and Horsch, Alexander},
	month = may,
	year = {2009},
	pages = {2052--2068},
	file = {Elter and Horsch - 2009 - CADx of mammographic masses and clustered microcal.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZU5S4X6J\\Elter and Horsch - 2009 - CADx of mammographic masses and clustered microcal.pdf:application/pdf},
}

@article{oyeladeStateoftheArtSurveyDeep2020,
	title = {A {State}-of-the-{Art} {Survey} on {Deep} {Learning} {Methods} for {Detection} of {Architectural} {Distortion} {From} {Digital} {Mammography}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9165720/},
	doi = {10.1109/ACCESS.2020.3016223},
	abstract = {Breast cancer is a type of cancer that has risen to be the second cause of death among women. Classiﬁcation of breast tissues into normal, benign, or malignant depends on the presence of abnormalities like microcalciﬁcations, masses, architectural distortions, and asymmetries. Architectural distortion (AD) is subtle in detection with no association with masses but shows the abnormal arrangement of tissue strands, often in a radial, spiculation, or random pattern. It is widely rated as the third symptom of breast cancer which is the most commonly missed abnormality. Most computational approaches characterizing abnormalities in breast images often concentrate on the detection of microcalciﬁcation and masses with architectural distortions appearing as a secondary ﬁnding. The subtle nature and a minimal occurrence of architectural distortions may seem to complicate computational approaches for its detection. As a result, little research interest has been recorded in this area. It is widely reported that some cases of recent breast cancer are wrongly diagnosed due to the omission in detecting the presence of architectural distortion at the early stage of the disease. However, we discovered that most computational solutions to early detection of breast cancer are focused mainly on detecting other abnormalities such as masses and microcalciﬁcation, which are some evidence of the advanced stage of the disease. To emphasise the little efforts channeled towards detection of AD compared to other abnormalities, this article aims to detail the review of such studies in the last decade. To the best of our knowledge, this study presents the ﬁrst review which focuses on the detection of architectural distortion (AD) from mammographic images. Furthermore, this article presents a comprehensive review of approaches, advances, and challenges on the computational methods for detecting AD, with the sole aim of advancing the use of deep learning models in detecting AD. Moreover, a comparative study of performance analyses of articles surveyed in this article is investigated. Our ﬁndings revealed that about 70\% of the existing literature adopted Gabor Filters, while just less than 10\% leveraged on the state-ofthe-art performances recorded in computer vision and deep learning, in building outstanding computational models for the detection of AD. The current study also discovered that using a deep learning approach, such as the convolution neural network (CNN) method, can yield a signiﬁcant increase in performance for the task of detection of architectural distortions. This assertion is based on literature results obtained using the CNN, which generates an accuracy of 99.4\% compared to the use of Gabor ﬁlters method, which accounts for 95\% accuracy.},
	language = {en},
	urldate = {2023-05-24},
	journal = {IEEE Access},
	author = {Oyelade, Olaide Nathaniel and Ezugwu, Absalom El-Shamir},
	year = {2020},
	pages = {148644--148676},
	file = {Oyelade and Ezugwu - 2020 - A State-of-the-Art Survey on Deep Learning Methods.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8ARA4QHB\\Oyelade and Ezugwu - 2020 - A State-of-the-Art Survey on Deep Learning Methods.pdf:application/pdf},
}

@article{biArtificialIntelligenceCancer2019,
	title = {Artificial intelligence in cancer imaging: {Clinical} challenges and applications},
	issn = {0007-9235, 1542-4863},
	shorttitle = {Artificial intelligence in cancer imaging},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.3322/caac.21552},
	doi = {10.3322/caac.21552},
	abstract = {Judgement, as one of the core tenets of medicine, relies upon the integration of multilayered data with nuanced decision making. Cancer offers a unique context for medical decisions given not only its variegated forms with evolution of disease but also the need to take into account the individual condition of patients, their ability to receive treatment, and their responses to treatment. Challenges remain in the accurate detection, characterization, and monitoring of cancers despite improved technologies. Radiographic assessment of disease most commonly relies upon visual evaluations, the interpretations of which may be augmented by advanced computational analyses. In particular, artificial intelligence (AI) promises to make great strides in the qualitative interpretation of cancer imaging by expert clinicians, including volumetric delineation of tumors over time, extrapolation of the tumor genotype and biological course from its radiographic phenotype, prediction of clinical outcome, and assessment of the impact of disease and treatment on adjacent organs. AI may automate processes in the initial interpretation of images and shift the clinical workflow of radiographic detection, management decisions on whether or not to administer an intervention, and subsequent observation to a yet to be envisioned paradigm. Here, the authors review the current state of AI as applied to medical imaging of cancer and describe advances in 4 tumor types (lung, brain, breast, and prostate) to illustrate how common clinical problems are being addressed. Although most studies evaluating AI applications in oncology to date have not been vigorously validated for reproducibility and generalizability, the results do highlight increasingly concerted efforts in pushing AI technology to ­clinical use and to impact future directions in cancer care. CA Cancer J Clin 2019;69:127-157. © 2019 American Cancer Society.},
	language = {en},
	urldate = {2023-05-24},
	journal = {CA: A Cancer Journal for Clinicians},
	author = {Bi, Wenya Linda and Hosny, Ahmed and Schabath, Matthew B. and Giger, Maryellen L. and Birkbak, Nicolai J. and Mehrtash, Alireza and Allison, Tavis and Arnaout, Omar and Abbosh, Christopher and Dunn, Ian F. and Mak, Raymond H. and Tamimi, Rulla M. and Tempany, Clare M. and Swanton, Charles and Hoffmann, Udo and Schwartz, Lawrence H. and Gillies, Robert J. and Huang, Raymond Y. and Aerts, Hugo J. W. L.},
	month = feb,
	year = {2019},
	pages = {caac.21552},
	file = {Bi et al. - 2019 - Artificial intelligence in cancer imaging Clinica.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GREFSD8G\\Bi et al. - 2019 - Artificial intelligence in cancer imaging Clinica.pdf:application/pdf},
}

@article{grimComputerAidedEvaluationScreening2009,
	title = {Computer-{Aided} {Evaluation} of {Screening} {Mammograms} {Based} on {Local} {Texture} {Models}},
	volume = {18},
	issn = {1057-7149, 1941-0042},
	url = {http://ieeexplore.ieee.org/document/4785121/},
	doi = {10.1109/TIP.2008.2011168},
	abstract = {We propose a new approach to diagnostic evaluation of screening mammograms based on local statistical texture models. The local evaluation tool has the form of a multivariate probability density of gray levels in a suitably chosen search window. First, the density function in the form of Gaussian mixture is estimated from data obtained by scanning of the mammogram with the search window. Then we evaluate the estimated mixture at each position and display the corresponding log-likelihood value as a gray level at the window center. The resulting log-likelihood image closely correlates with the structural details of the original mammogram and emphasizes unusual places. We assume that, in parallel use, the log-likelihood image may provide additional information to facilitate the identiﬁcation of malignant lesions as untypical locations of high novelty.},
	language = {en},
	number = {4},
	urldate = {2023-05-24},
	journal = {IEEE Transactions on Image Processing},
	author = {Grim, JirÍ and Somol, Petr and Haindl, Michal and Danes, Jan},
	month = apr,
	year = {2009},
	pages = {765--773},
	file = {Grim et al. - 2009 - Computer-Aided Evaluation of Screening Mammograms .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IUIRINEB\\Grim et al. - 2009 - Computer-Aided Evaluation of Screening Mammograms .pdf:application/pdf},
}

@article{rojasdominguezDetectionMassesMammograms2008,
	title = {Detection of masses in mammograms via statistically based enhancement, multilevel-thresholding segmentation, and region selection},
	volume = {32},
	issn = {08956111},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0895611108000086},
	doi = {10.1016/j.compmedimag.2008.01.006},
	abstract = {A method for automatic detection of mammographic masses is presented. As part of this method, an enhancement algorithm that improves image contrast based on local statistical measures of the mammograms is proposed. After enhancement, regions are segmented via thresholding at multiple levels, and a set of features is computed from each of the segmented regions. A region-ranking system is also presented that identiﬁes the regions most likely to represent abnormalities based on the features computed. The method was tested on 57 mammographic images of masses from the Mini-MIAS database, and achieved a sensitivity of 80\% at 2.3 false-positives per image (average of 0.32 false-positives per image).},
	language = {en},
	number = {4},
	urldate = {2023-05-24},
	journal = {Computerized Medical Imaging and Graphics},
	author = {Rojas Domínguez, Alfonso and Nandi, Asoke K.},
	month = jun,
	year = {2008},
	pages = {304--315},
	file = {Rojas Domínguez and Nandi - 2008 - Detection of masses in mammograms via statisticall.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RMA8AZ6R\\Rojas Domínguez and Nandi - 2008 - Detection of masses in mammograms via statisticall.pdf:application/pdf},
}

@article{lladoTexturalApproachMass2009,
	title = {A textural approach for mass false positive reduction in mammography},
	volume = {33},
	issn = {08956111},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S089561110900038X},
	doi = {10.1016/j.compmedimag.2009.03.007},
	abstract = {During the last decade several algorithms have been proposed for automatic mass detection in mammographic images. However, almost all these methods suffer from a high number of false positives. In this paper we propose a new approach for tackling this false positive reduction problem. The key point of our proposal is the use of Local Binary Patterns (LBP) for representing the textural properties of the masses. We extend the basic LBP histogram descriptor into a spatially enhanced histogram which encodes both the local region appearance and the spatial structure of the masses. Support Vector Machines (SVM) are then used for classifying the true masses from the ones being actually normal parenchyma. Our approach is evaluated using 1792 ROIs extracted from the DDSM database. The experiments show that LBP are effective and efﬁcient descriptors for mammographic masses. Moreover, the comparison with current methods illustrates that our proposal obtains a better performance.},
	language = {en},
	number = {6},
	urldate = {2023-05-24},
	journal = {Computerized Medical Imaging and Graphics},
	author = {Lladó, X. and Oliver, A. and Freixenet, J. and Martí, R. and Martí, J.},
	month = sep,
	year = {2009},
	pages = {415--422},
	file = {Lladó et al. - 2009 - A textural approach for mass false positive reduct.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WQCYWGJI\\Lladó et al. - 2009 - A textural approach for mass false positive reduct.pdf:application/pdf},
}

@article{carneiroAutomatedAnalysisUnregistered2017,
	title = {Automated {Analysis} of {Unregistered} {Multi}-{View} {Mammograms} {With} {Deep} {Learning}},
	volume = {36},
	issn = {0278-0062, 1558-254X},
	url = {https://ieeexplore.ieee.org/document/8032490/},
	doi = {10.1109/TMI.2017.2751523},
	abstract = {We describe an automated methodology for the analysis of unregistered cranio-caudal (CC) and medio-lateral oblique (MLO) mammography views in order to estimate the patient’s risk of developing breast cancer. The main innovation behind this methodology lies in the use of deep learning models for the problem of jointly classifying unregistered mammogram views and respective segmentation maps of breast lesions (i.e., masses and micro-calciﬁcations). This is a holistic methodology that can classify a whole mammographic exam, containing the CC and MLO views and the segmentation maps, as opposed to the classiﬁcation of individual lesions, which is the dominant approach in the ﬁeld. We also demonstrate that the proposed system is capable of using the segmentation maps generated by automated mass and micro-calciﬁcation detection systems, and still producing accurate results. The semi-automated approach (using manually deﬁned mass and micro-calciﬁcation segmentation maps) is tested on two publicly available datasets (INbreast and DDSM), and results show that the volume under ROC surface (VUS) for a 3-class problem (normal tissue, benign and malignant) is over 0.9, the area under ROC curve (AUC) for the 2-class ”benign vs malignant” problem is over 0.9, and for the 2class breast screening problem (malignancy vs normal/benign) is also over 0.9. For the fully automated approach, the VUS results on INbreast is over 0.7, and the AUC for the 2-class ”benign vs malignant” problem is over 0.78, and the AUC for the 2-class breast screening is 0.86.},
	language = {en},
	number = {11},
	urldate = {2023-05-24},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Carneiro, Gustavo and Nascimento, Jacinto and Bradley, Andrew P.},
	month = nov,
	year = {2017},
	pages = {2355--2365},
	file = {Carneiro et al. - 2017 - Automated Analysis of Unregistered Multi-View Mamm.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FKS5PP6V\\Carneiro et al. - 2017 - Automated Analysis of Unregistered Multi-View Mamm.pdf:application/pdf},
}

@article{weiLearningPerceptualSimilarity2009,
	title = {Learning of {Perceptual} {Similarity} {From} {Expert} {Readers} for {Mammogram} {Retrieval}},
	volume = {3},
	issn = {1932-4553},
	url = {http://ieeexplore.ieee.org/document/4786551/},
	doi = {10.1109/JSTSP.2008.2011159},
	abstract = {Content-based image retrieval relies critically on the use of a computerized measure of the similarity (i.e., relevance) of a query image to other images in a database. In this work, we explore a superivised learning approach for retrieval of mammogram images, of which the goal is to serve as a diagnostic aid for breast cancer. We propose that the most meaningful measure is one that is designed speciﬁcally to match that perceived by the radiologists in their interpretation of mammogram lesions. In our approach, we model the notion of similarity as an unknown function of the image features characterizing the lesions, and use modern machine-learning algorithms to learn this function from similarity scores collected from radiologists in reader studies. This approach is evaluated using data collected from an observer study with a set of clinical mammograms. Our results demonstrate that the proposed machine learning approach can be used to model the notion of similarity as judged by expert readers in their interpretation of mammogram images and that it can outperform alternative similarity measures derived from unsupervised learning.},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {IEEE Journal of Selected Topics in Signal Processing},
	author = {Wei, Liyang and Yang, Yongyi and Wernick, Miles N. and Nishikawa, Robert M.},
	month = feb,
	year = {2009},
	pages = {53--61},
	file = {Wei et al. - 2009 - Learning of Perceptual Similarity From Expert Read.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LRNBL36W\\Wei et al. - 2009 - Learning of Perceptual Similarity From Expert Read.pdf:application/pdf},
}

@inproceedings{zhengSuperresolutionMammograms2010a,
	address = {Montreal, QC, Canada},
	title = {Super-resolution of mammograms},
	isbn = {978-1-4244-6766-2},
	url = {http://ieeexplore.ieee.org/document/5510384/},
	doi = {10.1109/CIBCB.2010.5510384},
	abstract = {High-quality mammography is the most effective technology presently available for breast cancer screening. High resolution mammograms usually lead to more accurate diagnoses; however, they require large doses of radiation, which may have harmful effects. In this paper, we present a method to synthesize high-resolution mammograms from low-resolution inputs, which offers the potential of allowing accurate diagnoses while minimizing risks to patients. Our algorithm combines statistical machine learning methods and stochastic search to learn the mapping from low-resolution to high-resolution mammograms using a large dataset of training image pairs. Experimental results show that the super-resolution algorithm can generate highquality, high-resolution breast mammograms from low-resolution input with no human intervention.},
	language = {en},
	urldate = {2023-05-24},
	booktitle = {2010 {IEEE} {Symposium} on {Computational} {Intelligence} in {Bioinformatics} and {Computational} {Biology}},
	publisher = {IEEE},
	author = {Zheng, Jun and Fuentes, Olac and Leung, Ming-Ying},
	month = may,
	year = {2010},
	pages = {1--7},
	file = {Zheng et al. - 2010 - Super-resolution of mammograms.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4EEUWH45\\Zheng et al. - 2010 - Super-resolution of mammograms.pdf:application/pdf},
}

@article{buciuDirectionalFeaturesAutomatic2011,
	title = {Directional features for automatic tumor classification of mammogram images},
	volume = {6},
	issn = {17468094},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1746809410000820},
	doi = {10.1016/j.bspc.2010.10.003},
	language = {en},
	number = {4},
	urldate = {2023-05-24},
	journal = {Biomedical Signal Processing and Control},
	author = {Buciu, Ioan and Gacsadi, Alexandru},
	month = oct,
	year = {2011},
	pages = {370--378},
	file = {Buciu and Gacsadi - 2011 - Directional features for automatic tumor classific.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\E9J4WLAW\\Buciu and Gacsadi - 2011 - Directional features for automatic tumor classific.pdf:application/pdf},
}

@article{akhtarContextBasedEnsemble2020,
	title = {Context‐based ensemble classification for the detection of architectural distortion in a digitised mammogram},
	volume = {14},
	issn = {1751-9667, 1751-9667},
	url = {https://onlinelibrary.wiley.com/doi/10.1049/iet-ipr.2019.0639},
	doi = {10.1049/iet-ipr.2019.0639},
	abstract = {The problem of computer-aided detection of architectural distortion (AD) in a digitised mammogram has been attempted in this manuscript. In examining a mammogram, the decision regarding a particular region of interest (RoI) is dependent on the appearance of the surrounding regions. However, in existing methods to detect AD the inference about an RoI is dependent on the appearance of this RoI alone. In addition, multiple radiologists infer the same mammogram in coming to a final decision about the mammogram. Contrary to popular ensemble classifiers like Adaboost and Random Forest, the authors propose an ensemble based method (imitating multiple radiologists by classifiers) for detecting AD such that the decision on a test RoI is dependent on the decisions of the surrounding RoIs in the proposed ensemble classifier. The proposed contextbased ensemble classifier has been validated on two mammographic databases. The proposal shows promising results in both the databases.},
	language = {en},
	number = {4},
	urldate = {2023-05-24},
	journal = {IET Image Processing},
	author = {Akhtar, Yusuf and Mukherjee, Dipti Prasad},
	month = mar,
	year = {2020},
	pages = {603--614},
	file = {Akhtar and Mukherjee - 2020 - Context‐based ensemble classification for the dete.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\P45EGZW5\\Akhtar and Mukherjee - 2020 - Context‐based ensemble classification for the dete.pdf:application/pdf},
}

@article{huynhDigitalMammographicTumor2016,
	title = {Digital mammographic tumor classification using transfer learning from deep convolutional neural networks},
	volume = {3},
	issn = {2329-4302},
	url = {http://medicalimaging.spiedigitallibrary.org/article.aspx?doi=10.1117/1.JMI.3.3.034501},
	doi = {10.1117/1.JMI.3.3.034501},
	language = {en},
	number = {3},
	urldate = {2023-05-24},
	journal = {Journal of Medical Imaging},
	author = {Huynh, Benjamin Q. and Li, Hui and Giger, Maryellen L.},
	month = aug,
	year = {2016},
	pages = {034501},
	file = {Huynh et al. - 2016 - Digital mammographic tumor classification using tr.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\E83YACSV\\Huynh et al. - 2016 - Digital mammographic tumor classification using tr.pdf:application/pdf},
}

@article{arevaloRepresentationLearningMammography2016,
	title = {Representation learning for mammography mass lesion classification with convolutional neural networks},
	volume = {127},
	issn = {01692607},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169260715300110},
	doi = {10.1016/j.cmpb.2015.12.014},
	abstract = {Background and objective: The automatic classiﬁcation of breast imaging lesions is currently an unsolved problem. This paper describes an innovative representation learning framework for breast cancer diagnosis in mammography that integrates deep learning techniques to automatically learn discriminative features avoiding the design of speciﬁc hand-crafted image-based feature detectors.
Methods: A new biopsy proven benchmarking dataset was built from 344 breast cancer patients’ cases containing a total of 736 ﬁlm mammography (mediolateral oblique and craniocaudal) views, representative of manually segmented lesions associated with masses: 426 benign lesions and 310 malignant lesions. The developed method comprises two main stages: (i) preprocessing to enhance image details and (ii) supervised training for learning both the features and the breast imaging lesions classiﬁer. In contrast to previous works, we adopt a hybrid approach where convolutional neural networks are used to learn the representation in a supervised way instead of designing particular descriptors to explain the content of mammography images.
Results: Experimental results using the developed benchmarking breast cancer dataset demonstrated that our method exhibits signiﬁcant improved performance when compared to state-of-the-art image descriptors, such as histogram of oriented gradients (HOG) and histogram of the gradient divergence (HGD), increasing the performance from 0.787 to 0.822 in terms of the area under the ROC curve (AUC). Interestingly, this model also outperforms a set of hand-crafted features that take advantage of additional information from segmentation by the radiologist. Finally, the combination of both representations, learned and hand-crafted, resulted in the best descriptor for mass lesion classiﬁcation, obtaining 0.826 in the AUC score.
Conclusions: A novel deep learning based framework to automatically address classiﬁcation of breast mass lesions in mammography was developed. © 2015 Elsevier Ireland Ltd. All rights reserved.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Arevalo, John and González, Fabio A. and Ramos-Pollán, Raúl and Oliveira, Jose L. and Guevara Lopez, Miguel Angel},
	month = apr,
	year = {2016},
	pages = {248--257},
	file = {Arevalo et al. - 2016 - Representation learning for mammography mass lesio.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SRRNUDJB\\Arevalo et al. - 2016 - Representation learning for mammography mass lesio.pdf:application/pdf},
}

@article{sunMultiViewConvolutionalNeural2019a,
	title = {Multi-{View} {Convolutional} {Neural} {Networks} for {Mammographic} {Image} {Classification}},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8822935/},
	doi = {10.1109/ACCESS.2019.2939167},
	abstract = {Recent years, deep learning has been widely applied to mammographic image classiﬁcation. However, most of the existing methods are based on a single mammography view, and cannot sufﬁciently extract discriminative features, thereby resulting in unsatisfactory classiﬁcation accuracy. To solve this problem and improve the mammographic image classiﬁcation performance, we propose a novel multi-view convolutional neural network based on multiple mammography views in this paper. Considering that the images acquired from different perspectives contain different and complementary breast mass information, we modify the convolutional neural networks (CNN) architecture to exploit complementary information from various views of mammography. The new architecture can extract discriminative features from the mediolateral oblique (MLO) and craniocaudal (CC) views of the mammographic images and can effectively incorporate these features for mammographic images. The dilated convolutional layers enable the feature maps extracted from the multiple breast mass views to capture information of a large "ﬁeld of vision". Moreover, multi-scale features are obtained by using the convolutional and dilated convolutions. In addition, we incorporate a penalty term into the cross entropy loss function, which enables the model evolution to reduce the misclassiﬁcation rate by enhancing the contributions of samples that are misclassiﬁed in the training process. Our method has been evaluated and compared with several state-of-the-art methods on the open Digital Database for Screening Mammography (DDSM) and Mammographic Image Analysis Society (MIAS) datasets. Experimental results show that our method obtains better performance than the state-ofthe-art methods.},
	language = {en},
	urldate = {2023-05-24},
	journal = {IEEE Access},
	author = {Sun, Lilei and Wang, Junqian and Hu, Zhijun and Xu, Yong and Cui, Zhongwei},
	year = {2019},
	pages = {126273--126282},
	file = {Sun et al. - 2019 - Multi-View Convolutional Neural Networks for Mammo.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QUSRBC3Z\\Sun et al. - 2019 - Multi-View Convolutional Neural Networks for Mammo.pdf:application/pdf},
}

@article{ribliDetectingClassifyingLesions2018,
	title = {Detecting and classifying lesions in mammograms with {Deep} {Learning}},
	volume = {8},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-22437-z},
	doi = {10.1038/s41598-018-22437-z},
	abstract = {Abstract
            
              In the last two decades, Computer Aided Detection (CAD) systems were developed to help radiologists analyse screening mammograms, however benefits of current CAD technologies appear to be contradictory, therefore they should be improved to be ultimately considered useful. Since 2012, deep convolutional neural networks (CNN) have been a tremendous success in image recognition, reaching human performance. These methods have greatly surpassed the traditional approaches, which are similar to currently used CAD solutions. Deep CNN-s have the potential to revolutionize medical image analysis. We propose a CAD system based on one of the most successful object detection frameworks, Faster R-CNN. The system detects and classifies malignant or benign lesions on a mammogram without any human intervention. The proposed method sets the state of the art classification performance on the public INbreast database, AUC = 0.95. The approach described here has achieved 2nd place in the Digital Mammography DREAM Challenge with AUC = 0.85. When used as a detector, the system reaches high sensitivity with very few false positive marks per image on the INbreast dataset. Source code, the trained model and an OsiriX plugin are published online at
              https://github.com/riblidezso/frcnn\_cad
              .},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {Scientific Reports},
	author = {Ribli, Dezső and Horváth, Anna and Unger, Zsuzsa and Pollner, Péter and Csabai, István},
	month = mar,
	year = {2018},
	pages = {4165},
	file = {Ribli et al. - 2018 - Detecting and classifying lesions in mammograms wi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NST8C8KY\\Ribli et al. - 2018 - Detecting and classifying lesions in mammograms wi.pdf:application/pdf},
}

@article{carneiroAutomatedAnalysisUnregistered2017a,
	title = {Automated {Analysis} of {Unregistered} {Multi}-{View} {Mammograms} {With} {Deep} {Learning}},
	volume = {36},
	issn = {0278-0062, 1558-254X},
	url = {https://ieeexplore.ieee.org/document/8032490/},
	doi = {10.1109/TMI.2017.2751523},
	abstract = {We describe an automated methodology for the analysis of unregistered cranio-caudal (CC) and mediolateral oblique (MLO) mammography views in order to estimate the patient’s risk of developing breast cancer. The main innovation behind this methodology lies in the use of deep learning models for the problem of jointly classifying unregistered mammogram views and respective segmentation maps of breast lesions (i.e., masses and micro-calciﬁcations).This is a holistic methodology that can classify a whole mammographic exam, containing the CC and MLO views and the segmentation maps, as opposed to the classiﬁcation of individual lesions, which is the dominant approach in the ﬁeld. We also demonstrate that the proposed system is capable of using the segmentation maps generated by automated mass and micro-calciﬁcation detection systems, and still producing accurate results. The semi-automated approach (using manually deﬁned mass and micro-calciﬁcation segmentation maps) is tested on two publicly available data sets (INbreast and DDSM), and results show that the volume under ROC surface (VUS) for a 3-class problem (normal tissue, benign, and malignant) is over 0.9, the area under ROC curve (AUC) for the 2-class “benign versus malignant” problem is over 0.9, and for the 2-class breast screening problem (malignancy versus normal/benign) is also over 0.9. For the fully automated approach, the VUS results on INbreast is over 0.7, and the AUC for the 2-class “benign versus malignant” problem is over 0.78, and the AUC for the 2-class breast screening is 0.86.},
	language = {en},
	number = {11},
	urldate = {2023-05-24},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Carneiro, Gustavo and Nascimento, Jacinto and Bradley, Andrew P.},
	month = nov,
	year = {2017},
	pages = {2355--2365},
	file = {Carneiro et al. - 2017 - Automated Analysis of Unregistered Multi-View Mamm.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6X72ML63\\Carneiro et al. - 2017 - Automated Analysis of Unregistered Multi-View Mamm.pdf:application/pdf},
}

@article{jungDetectionMassesMammograms2018,
	title = {Detection of masses in mammograms using a one-stage object detector based on a deep convolutional neural network},
	volume = {13},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0203355},
	doi = {10.1371/journal.pone.0203355},
	language = {en},
	number = {9},
	urldate = {2023-05-24},
	journal = {PLOS ONE},
	author = {Jung, Hwejin and Kim, Bumsoo and Lee, Inyeop and Yoo, Minhwan and Lee, Junhyun and Ham, Sooyoun and Woo, Okhee and Kang, Jaewoo},
	editor = {Kaderali, Lars},
	month = sep,
	year = {2018},
	pages = {e0203355},
	file = {Jung et al. - 2018 - Detection of masses in mammograms using a one-stag.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JILWFN2G\\Jung et al. - 2018 - Detection of masses in mammograms using a one-stag.pdf:application/pdf},
}

@article{ribliDetectingClassifyingLesions2018a,
	title = {Detecting and classifying lesions in mammograms with {Deep} {Learning}},
	volume = {8},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-22437-z},
	doi = {10.1038/s41598-018-22437-z},
	abstract = {Abstract
            
              In the last two decades, Computer Aided Detection (CAD) systems were developed to help radiologists analyse screening mammograms, however benefits of current CAD technologies appear to be contradictory, therefore they should be improved to be ultimately considered useful. Since 2012, deep convolutional neural networks (CNN) have been a tremendous success in image recognition, reaching human performance. These methods have greatly surpassed the traditional approaches, which are similar to currently used CAD solutions. Deep CNN-s have the potential to revolutionize medical image analysis. We propose a CAD system based on one of the most successful object detection frameworks, Faster R-CNN. The system detects and classifies malignant or benign lesions on a mammogram without any human intervention. The proposed method sets the state of the art classification performance on the public INbreast database, AUC = 0.95. The approach described here has achieved 2nd place in the Digital Mammography DREAM Challenge with AUC = 0.85. When used as a detector, the system reaches high sensitivity with very few false positive marks per image on the INbreast dataset. Source code, the trained model and an OsiriX plugin are published online at
              https://github.com/riblidezso/frcnn\_cad
              .},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {Scientific Reports},
	author = {Ribli, Dezső and Horváth, Anna and Unger, Zsuzsa and Pollner, Péter and Csabai, István},
	month = mar,
	year = {2018},
	pages = {4165},
	file = {Ribli et al. - 2018 - Detecting and classifying lesions in mammograms wi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FYWK4X98\\Ribli et al. - 2018 - Detecting and classifying lesions in mammograms wi.pdf:application/pdf},
}

@article{ribliDetectingClassifyingLesions2018b,
	title = {Detecting and classifying lesions in mammograms with {Deep} {Learning}},
	volume = {8},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-22437-z},
	doi = {10.1038/s41598-018-22437-z},
	abstract = {Abstract
            
              In the last two decades, Computer Aided Detection (CAD) systems were developed to help radiologists analyse screening mammograms, however benefits of current CAD technologies appear to be contradictory, therefore they should be improved to be ultimately considered useful. Since 2012, deep convolutional neural networks (CNN) have been a tremendous success in image recognition, reaching human performance. These methods have greatly surpassed the traditional approaches, which are similar to currently used CAD solutions. Deep CNN-s have the potential to revolutionize medical image analysis. We propose a CAD system based on one of the most successful object detection frameworks, Faster R-CNN. The system detects and classifies malignant or benign lesions on a mammogram without any human intervention. The proposed method sets the state of the art classification performance on the public INbreast database, AUC = 0.95. The approach described here has achieved 2nd place in the Digital Mammography DREAM Challenge with AUC = 0.85. When used as a detector, the system reaches high sensitivity with very few false positive marks per image on the INbreast dataset. Source code, the trained model and an OsiriX plugin are published online at
              https://github.com/riblidezso/frcnn\_cad
              .},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {Scientific Reports},
	author = {Ribli, Dezső and Horváth, Anna and Unger, Zsuzsa and Pollner, Péter and Csabai, István},
	month = mar,
	year = {2018},
	pages = {4165},
	file = {Ribli et al. - 2018 - Detecting and classifying lesions in mammograms wi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\A7TE4H6J\\Ribli et al. - 2018 - Detecting and classifying lesions in mammograms wi.pdf:application/pdf},
}

@article{al-antariFullyIntegratedComputeraided2018,
	title = {A fully integrated computer-aided diagnosis system for digital {X}-ray mammograms via deep learning detection, segmentation, and classification},
	volume = {117},
	issn = {13865056},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1386505618302880},
	doi = {10.1016/j.ijmedinf.2018.06.003},
	abstract = {A computer-aided diagnosis (CAD) system requires detection, segmentation, and classiﬁcation in one framework to assist radiologists eﬃciently in an accurate diagnosis. In this paper, a completely integrated CAD system is proposed to screen digital X-ray mammograms involving detection, segmentation, and classiﬁcation of breast masses via deep learning methodologies.},
	language = {en},
	urldate = {2023-05-24},
	journal = {International Journal of Medical Informatics},
	author = {Al-antari, Mugahed A. and Al-masni, Mohammed A. and Choi, Mun-Taek and Han, Seung-Moo and Kim, Tae-Seong},
	month = sep,
	year = {2018},
	pages = {44--54},
	file = {Al-antari et al. - 2018 - A fully integrated computer-aided diagnosis system.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZG5AYX7Z\\Al-antari et al. - 2018 - A fully integrated computer-aided diagnosis system.pdf:application/pdf},
}

@misc{wuConditionalInfillingGANs2018,
	title = {Conditional {Infilling} {GANs} for {Data} {Augmentation} in {Mammogram} {Classification}},
	url = {http://arxiv.org/abs/1807.08093},
	abstract = {Deep learning approaches to breast cancer detection in mammograms have recently shown promising results. However, such models are constrained by the limited size of publicly available mammography datasets, in large part due to privacy concerns and the high cost of generating expert annotations. Limited dataset size is further exacerbated by substantial class imbalance since “normal” images dramatically outnumber those with ﬁndings. Given the rapid progress of generative models in synthesizing realistic images, and the known eﬀectiveness of simple data augmentation techniques (e.g. horizontal ﬂipping), we ask if it is possible to synthetically augment mammogram datasets using generative adversarial networks (GANs). We train a class-conditional GAN to perform contextual in-ﬁlling, which we then use to synthesize lesions onto healthy screening mammograms. First, we show that GANs are capable of generating high-resolution synthetic mammogram patches. Next, we experimentally evaluate using the augmented dataset to improve breast cancer classiﬁcation performance. We observe that a ResNet-50 classiﬁer trained with GAN-augmented training data produces a higher AUROC compared to the same model trained only on traditionally augmented data, demonstrating the potential of our approach.},
	language = {en},
	urldate = {2023-05-24},
	publisher = {arXiv},
	author = {Wu, Eric and Wu, Kevin and Cox, David and Lotter, William},
	month = aug,
	year = {2018},
	note = {arXiv:1807.08093 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Wu et al. - 2018 - Conditional Infilling GANs for Data Augmentation i.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2PE6VD93\\Wu et al. - 2018 - Conditional Infilling GANs for Data Augmentation i.pdf:application/pdf},
}

@article{zhangClassificationWholeMammogram2018a,
	title = {Classification of {Whole} {Mammogram} and {Tomosynthesis} {Images} {Using} {Deep} {Convolutional} {Neural} {Networks}},
	volume = {17},
	issn = {1536-1241, 1558-2639},
	url = {https://ieeexplore.ieee.org/document/8374855/},
	doi = {10.1109/TNB.2018.2845103},
	abstract = {Mammography is the most popular technology used for the early detection of breast cancer. Manual classiﬁcation of mammogram images is a hard task because of the variability of the tumor. It yields a noteworthy number of patients being called back to perform biopsies, ensuring no missing diagnosis. The convolutional neural network (CNN) has succeeded in a lot of image classiﬁcation challenges during the recent years. In this paper, we proposed an approach of mammogram and tomosynthesis classiﬁcation based on CNNs. We had acquired more than 3000 mammograms and tomosynthesis data with approval from an institutional review board at the University of Kentucky. Different models of CNNs were built to classify both the 2-D mammograms and 3-D tomosynthesis, and every classiﬁer was assessed with respect to truth-values generated by histology results from the biopsy and twoyear negative mammogram follow-up conﬁrmed by expert radiologists. Our outcomes demonstrated that CNN-based models we had built and optimized utilizing transfer learning and data augmentation have good potential for automatic breast cancer detection based on the mammograms and tomosynthesis data.},
	language = {en},
	number = {3},
	urldate = {2023-05-24},
	journal = {IEEE Transactions on NanoBioscience},
	author = {Zhang, Xiaofei and Zhang, Yi and Han, Erik Y. and Jacobs, Nathan and Han, Qiong and Wang, Xiaoqin and Liu, Jinze},
	month = jul,
	year = {2018},
	pages = {237--242},
	file = {Zhang et al. - 2018 - Classification of Whole Mammogram and Tomosynthesi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CW8SY3F7\\Zhang et al. - 2018 - Classification of Whole Mammogram and Tomosynthesi.pdf:application/pdf},
}

@article{wangContextsensitiveDeepLearning2018,
	title = {A context-sensitive deep learning approach for microcalcification detection in mammograms},
	volume = {78},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320318300086},
	doi = {10.1016/j.patcog.2018.01.009},
	abstract = {A challenging issue in computerized detection of clustered microcalciﬁcations (MCs) is the frequent occurrence of false positives (FPs) caused by local image patterns that resemble MCs. We develop a context-sensitive deep neural network (DNN), aimed to take into account both the local image features of an MC and its surrounding tissue background, for MC detection. The DNN classiﬁer is trained to automatically extract the relevant image features of an MC as well as its image context. The proposed approach was evaluated on a set of 292 mammograms using free-response receiver operating characteristic (FROC) analysis on the accuracy both in detecting individual MCs and in detecting MC clusters. The results demonstrate that the proposed approach could achieve signiﬁcantly higher FROC curves when compared to two MC-based detectors. It indicates that incorporating image context information in MC detection can be beneﬁcial for reducing the FPs in detections.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Pattern Recognition},
	author = {Wang, Juan and Yang, Yongyi},
	month = jun,
	year = {2018},
	pages = {12--22},
	file = {Wang and Yang - 2018 - A context-sensitive deep learning approach for mic.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PBSGB7HQ\\Wang and Yang - 2018 - A context-sensitive deep learning approach for mic.pdf:application/pdf},
}

@inproceedings{zhuAdversarialDeepStructured2018,
	address = {Washington, DC},
	title = {Adversarial deep structured nets for mass segmentation from mammograms},
	isbn = {978-1-5386-3636-7},
	url = {https://ieeexplore.ieee.org/document/8363704/},
	doi = {10.1109/ISBI.2018.8363704},
	language = {en},
	urldate = {2023-05-24},
	booktitle = {2018 {IEEE} 15th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI} 2018)},
	publisher = {IEEE},
	author = {Zhu, Wentao and Xiang, Xiang and Tran, Trac D. and Hager, Gregory D. and Xie, Xiaohui},
	month = apr,
	year = {2018},
	pages = {847--850},
	file = {Zhu et al. - 2018 - Adversarial deep structured nets for mass segmenta.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JJ53UMDM\\Zhu et al. - 2018 - Adversarial deep structured nets for mass segmenta.pdf:application/pdf},
}

@article{sunMultiViewConvolutionalNeural2019b,
	title = {Multi-{View} {Convolutional} {Neural} {Networks} for {Mammographic} {Image} {Classification}},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8822935/},
	doi = {10.1109/ACCESS.2019.2939167},
	abstract = {In recent years, deep learning has been widely applied for mammographic image classiﬁcation. However, most of the existing methods are based on a single mammography view and cannot sufﬁciently extract discriminative features, thereby resulting in an unsatisfactory classiﬁcation accuracy. To solve this problem and improve the mammographic image classiﬁcation performance, we propose a novel multi-view convolutional neural network (CNN) based on multiple mammography views in this paper. Considering that the images acquired from different perspectives contain different and complementary breast mass information, we modify the CNN architecture to exploit the complementary information from the various views of mammography. The new architecture can extract discriminative features from the mediolateral oblique (MLO) and craniocaudal (CC) views of the mammographic images and can effectively incorporate these features for mammographic images. The dilated convolutional layers enable the feature maps extracted from the multiple breast mass views to capture information from a large ‘‘ﬁeld of vision’’. Moreover, multiscale features are obtained by using the convolutional and dilated convolutions. In addition, we incorporate a penalty term into the cross entropy loss function, which enables the model evolution to reduce the misclassiﬁcation rate by enhancing the contributions of the samples misclassiﬁed in the training process. The proposed method was evaluated and compared with several state-of-the-art methods on the open Digital Database for Screening Mammography (DDSM) and Mammographic Image Analysis Society (MIAS) datasets. The experimental results show that the proposed method exhibits a better performance than those of the state-of-the-art methods.},
	language = {en},
	urldate = {2023-05-24},
	journal = {IEEE Access},
	author = {Sun, Lilei and Wang, Junqian and Hu, Zhijun and Xu, Yong and Cui, Zhongwei},
	year = {2019},
	pages = {126273--126282},
	file = {Sun et al. - 2019 - Multi-View Convolutional Neural Networks for Mammo.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PE2YPLSQ\\Sun et al. - 2019 - Multi-View Convolutional Neural Networks for Mammo.pdf:application/pdf},
}

@article{agarwalAutomaticMassDetection2019,
	title = {Automatic mass detection in mammograms using deep convolutional neural networks},
	volume = {6},
	issn = {2329-4302},
	url = {https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-6/issue-03/031409/Automatic-mass-detection-in-mammograms-using-deep-convolutional-neural-networks/10.1117/1.JMI.6.3.031409.full},
	doi = {10.1117/1.JMI.6.3.031409},
	language = {en},
	number = {03},
	urldate = {2023-05-24},
	journal = {Journal of Medical Imaging},
	author = {Agarwal, Richa and Diaz, Oliver and Lladó, Xavier and Yap, Moi Hoon and Martí, Robert},
	month = feb,
	year = {2019},
	pages = {1},
	file = {Agarwal et al. - 2019 - Automatic mass detection in mammograms using deep .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QFV3YX2K\\Agarwal et al. - 2019 - Automatic mass detection in mammograms using deep .pdf:application/pdf},
}

@misc{maCrossviewRelationNetworks2019,
	title = {Cross-view {Relation} {Networks} for {Mammogram} {Mass} {Detection}},
	url = {http://arxiv.org/abs/1907.00528},
	abstract = {Mammogram is the most eﬀective imaging modality for the mass lesion detection of breast cancer at the early stage. The information from the two paired views (i.e., medio-lateral oblique and craniocaudal) are highly relational and complementary, and this is crucial for doctors’ decisions in clinical practice. However, existing mass detection methods do not consider jointly learning eﬀective features from the two relational views. To address this issue, this paper proposes a novel mammogram mass detection framework, termed Cross-View Relation Region-based Convolutional Neural Networks (CVR-RCNN). The proposed CVR-RCNN is expected to capture the latent relation information between the corresponding mass region of interests (ROIs) from the two paired views. Evaluations on a new large-scale private dataset and a public mammogram dataset show that the proposed CVR-RCNN outperforms existing state-of-the-art mass detection methods. Meanwhile, our experimental results suggest that incorporating the relation information across two views helps to train a superior detection model, which is a promising avenue for mammogram mass detection.},
	language = {en},
	urldate = {2023-05-24},
	publisher = {arXiv},
	author = {Ma, Jiechao and Liang, Sen and Li, Xiang and Li, Hongwei and Menze, Bjoern H. and Zhang, Rongguo and Zheng, Wei-Shi},
	month = jun,
	year = {2019},
	note = {arXiv:1907.00528 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Ma et al. - 2019 - Cross-view Relation Networks for Mammogram Mass De.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QHK8G6FS\\Ma et al. - 2019 - Cross-view Relation Networks for Mammogram Mass De.pdf:application/pdf},
}

@article{guanDetectingAsymmetricPatterns2020,
	title = {Detecting {Asymmetric} {Patterns} and {Localizing} {Cancers} on {Mammograms}},
	volume = {1},
	issn = {26663899},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2666389920301409},
	doi = {10.1016/j.patter.2020.100106},
	abstract = {One in eight women develops invasive breast cancer in her lifetime. The frontline protection against this disease is mammography. While computer-assisted diagnosis algorithms have made great progress in generating reliable global predictions, few focus on simultaneously producing regions of interest (ROIs) for biopsy. Can we combine ROI-oriented algorithms with global classiﬁcation of cancer status, which simultaneously highlight suspicious regions and optimize classiﬁcation performance? Can the asymmetry of breasts be adopted in deep learning for ﬁnding lesions and classifying cancers? We answer the above questions by building deep-learning networks that identify masses and microcalciﬁcations in paired mammograms, exclude false positives, and stepwisely improve performance of the model with asymmetric information regarding the breasts. This method achieved a co-leading place in the Digital Mammography DREAM Challenge for predicting breast cancer. We highlight here the importance of this dual-purpose process that simultaneously provides the locations of potential lesions in mammograms.},
	language = {en},
	number = {7},
	urldate = {2023-05-24},
	journal = {Patterns},
	author = {Guan, Yuanfang and Wang, Xueqing and Li, Hongyang and Zhang, Zhenning and Chen, Xianghao and Siddiqui, Omer and Nehring, Sara and Huang, Xiuzhen},
	month = oct,
	year = {2020},
	pages = {100106},
	file = {Guan et al. - 2020 - Detecting Asymmetric Patterns and Localizing Cance.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\F7F9IRJU\\Guan et al. - 2020 - Detecting Asymmetric Patterns and Localizing Cance.pdf:application/pdf},
}

@article{nDeeplySupervisedNet2021,
	title = {Deeply supervised {\textless}span style="font-variant:small-caps;"{\textgreater}{U}‐{Net}{\textless}/span{\textgreater} for mass segmentation in digital mammograms},
	volume = {31},
	issn = {0899-9457, 1098-1098},
	shorttitle = {Deeply supervised {\textless}span style="font-variant},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/ima.22516},
	doi = {10.1002/ima.22516},
	abstract = {Mass detection is a critical process in the examination of mammograms. The shape and texture of the mass are key parameters used in the diagnosis of breast cancer. To recover the shape of the mass, semantic segmentation is found to be more useful rather than mere object detection (or) localization. The main challenges involved in the mass segmentation include: (a) low signal to noise ratio (b) indiscernible mass boundaries, and (c) more false positives. These problems arise due to the significant overlap in the intensities of both the normal parenchymal region and the mass region. To address these challenges, deeply supervised U-Net model (DS U-Net) coupled with dense conditional random fields (CRFs) is proposed. Here, the input images are preprocessed using CLAHE and a modified encoder-decoder-based deep learning model is used for segmentation. In general, the encoder captures the textual information of various regions in an input image, whereas the decoder recovers the spatial location of the desired region of interest. The encoderdecoder-based models lack the ability to recover the non-conspicuous and spiculated mass boundaries. In the proposed work, deep supervision is integrated with a popular encoder-decoder model (U-Net) to improve the attention of the network toward the boundary of the suspicious regions. The final segmentation map is also created as a linear combination of the intermediate feature maps and the output feature map. The dense CRF is then used to finetune the segmentation map for the recovery of definite edges. The DS U-Net with dense CRF is evaluated on two publicly available benchmark datasets CBIS-DDSM and INBREAST. It provides a dice score of 82.9\% for CBIS-DDSM and 79\% for INBREAST.},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {International Journal of Imaging Systems and Technology},
	author = {N, Ravitha Rajalakshmi and R, Vidhyapriya and N, Elango and Ramesh, Nikhil},
	month = mar,
	year = {2021},
	pages = {59--71},
	file = {N et al. - 2021 - Deeply supervised span style=font-variantsmall-.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YIJB4IDK\\N et al. - 2021 - Deeply supervised span style=font-variantsmall-.pdf:application/pdf},
}

@article{ibtehazMultiResUNetRethinkingUNet2020,
	title = {{MultiResUNet} : {Rethinking} the {U}-{Net} architecture for multimodal biomedical image segmentation},
	volume = {121},
	issn = {08936080},
	shorttitle = {{MultiResUNet}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608019302503},
	doi = {10.1016/j.neunet.2019.08.025},
	abstract = {In recent years Deep Learning has brought about a breakthrough in Medical Image Segmentation. In this regard, U-Net has been the most popular architecture in the medical imaging community. Despite outstanding overall performance in segmenting multimodal medical images, through extensive experimentations on some challenging datasets, we demonstrate that the classical U-Net architecture seems to be lacking in certain aspects. Therefore, we propose some modifications to improve upon the already state-of-the-art U-Net model. Following these modifications, we develop a novel architecture, MultiResUNet, as the potential successor to the U-Net architecture. We have tested and compared MultiResUNet with the classical U-Net on a vast repertoire of multimodal medical images. Although only slight improvements in the cases of ideal images are noticed, remarkable gains in performance have been attained for the challenging ones. We have evaluated our model on five different datasets, each with their own unique challenges, and have obtained a relative improvement in performance of 10.15\%, 5.07\%, 2.63\%, 1.41\%, and 0.62\% respectively. We have also discussed and highlighted some qualitatively superior aspects of MultiResUNet over classical U-Net that are not really reflected in the quantitative measures.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Neural Networks},
	author = {Ibtehaz, Nabil and Rahman, M. Sohel},
	month = jan,
	year = {2020},
	pages = {74--87},
	file = {Ibtehaz and Rahman - 2020 - MultiResUNet  Rethinking the U-Net architecture f.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GQERP9VA\\Ibtehaz and Rahman - 2020 - MultiResUNet  Rethinking the U-Net architecture f.pdf:application/pdf},
}

@incollection{wangBRGANBilateralResidual2020a,
	address = {Cham},
	title = {{BR}-{GAN}: {Bilateral} {Residual} {Generating} {Adversarial} {Network} for {Mammogram} {Classification}},
	volume = {12262},
	isbn = {978-3-030-59712-2 978-3-030-59713-9},
	shorttitle = {{BR}-{GAN}},
	url = {https://link.springer.com/10.1007/978-3-030-59713-9_63},
	abstract = {Mammogram malignancy classiﬁcation with only image-level annotations is challenging due to a lack of lesion annotations. If we can generate the healthy version of the diseased data, we can easily explore the lesion features. An intuitive idea of such generation is to use existing Cycle-GAN based methods. They achieve the healthy generation regarding healthy images as reference domain, while maintaining the original content by cycle consistency mechanism. However, healthy mammogram patterns are diverse which may lead to uncertain generations. Moreover, the back translation from healthy to the original remains an ill-posed problem due to lack of lesion information. To address these problems, we propose a novel model called bilateral residual generating adversarial network(BR-GAN). We use the Cycle-GAN as a basic framework while regarding the contralateral as generation reference based on the bilateral symmetry prior. To address the ill-posed back translation problem, we propose a residual-preserved mechanism to try to preserve the lesion features from the original features. The generated features and the original features are aggregated for further classiﬁcation. BR-GAN outperforms current state-of-the-art methods on INBreast and in-house datasets.},
	language = {en},
	urldate = {2023-05-24},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2020},
	publisher = {Springer International Publishing},
	author = {Wang, Chu-ran and Zhang, Fandong and Yu, Yizhou and Wang, Yizhou},
	editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
	year = {2020},
	doi = {10.1007/978-3-030-59713-9_63},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {657--666},
	file = {Wang et al. - 2020 - BR-GAN Bilateral Residual Generating Adversarial .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BD2UBJH7\\Wang et al. - 2020 - BR-GAN Bilateral Residual Generating Adversarial .pdf:application/pdf},
}

@article{liuActRadiologistReliable2022,
	title = {Act {Like} a {Radiologist}: {Towards} {Reliable} {Multi}-{View} {Correspondence} {Reasoning} for {Mammogram} {Mass} {Detection}},
	volume = {44},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {Act {Like} a {Radiologist}},
	url = {https://ieeexplore.ieee.org/document/9444895/},
	doi = {10.1109/TPAMI.2021.3085783},
	abstract = {Mammogram mass detection is crucial for diagnosing and preventing the breast cancers in clinical practice. The complementary effect of multi-view mammogram images provides valuable information about the breast anatomical prior structure and is of great signiﬁcance in digital mammography interpretation. However, unlike radiologists who can utilize the natural reasoning ability to identify masses based on multiple mammographic views, how to endow the existing object detection models with the capability of multi-view reasoning is vital for decision-making in clinical diagnosis but remains the boundary to explore. In this paper, we propose an Anatomy-aware Graph convolutional Network (AGN), which is tailored for mammogram mass detection and endows existing detection methods with multi-view reasoning ability. The proposed AGN consists of three steps. Firstly, we introduce a Bipartite Graph convolutional Network (BGN) to model the intrinsic geometric and semantic relations of ipsilateral views. Secondly, considering that the visual asymmetry of bilateral views is widely adopted in clinical practice to assist the diagnosis of breast lesions, we propose an Inception Graph convolutional Network (IGN) to model the structural similarities of bilateral views. Finally, based on the constructed graphs, the multi-view information is propagated through nodes methodically, which equips the features learned from the examined view with multi-view reasoning ability. Experiments on two standard benchmarks reveal that AGN signiﬁcantly exceeds the state-of-the-art performance. Visualization results show that AGN provides interpretable visual cues for clinical diagnosis.},
	language = {en},
	number = {10},
	urldate = {2023-05-24},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Liu, Yuhang and Zhang, Fandong and Chen, Chaoqi and Wang, Siwen and Wang, Yizhou and Yu, Yizhou},
	month = oct,
	year = {2022},
	pages = {5947--5961},
	file = {Liu et al. - 2022 - Act Like a Radiologist Towards Reliable Multi-Vie.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MMIDLIKF\\Liu et al. - 2022 - Act Like a Radiologist Towards Reliable Multi-Vie.pdf:application/pdf},
}

@article{savelliMulticontextCNNEnsemble2020,
	title = {A multi-context {CNN} ensemble for small lesion detection},
	volume = {103},
	issn = {09333657},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0933365719303082},
	doi = {10.1016/j.artmed.2019.101749},
	abstract = {In this paper, we propose a novel method for the detection of small lesions in digital medical images. Our approach is based on a multi-context ensemble of convolutional neural networks (CNNs), aiming at learning different levels of image spatial context and improving detection performance. The main innovation behind the proposed method is the use of multiple-depth CNNs, individually trained on image patches of different dimensions and then combined together. In this way, the final ensemble is able to find and locate abnormalities on the images by exploiting both the local features and the surrounding context of a lesion. Experiments were focused on two well-known medical detection problems that have been recently faced with CNNs: microcalcification detection on full-field digital mammograms and microaneurysm detection on ocular fundus images. To this end, we used two publicly available datasets, INbreast and E-ophtha. Statistically significantly better detection performance were obtained by the proposed ensemble with respect to other approaches in the literature, demonstrating its effectiveness in the detection of small abnormalities.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Artificial Intelligence in Medicine},
	author = {Savelli, B. and Bria, A. and Molinara, M. and Marrocco, C. and Tortorella, F.},
	month = mar,
	year = {2020},
	pages = {101749},
	file = {Savelli et al. - 2020 - A multi-context CNN ensemble for small lesion dete.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7THAR2CE\\Savelli et al. - 2020 - A multi-context CNN ensemble for small lesion dete.pdf:application/pdf},
}

@article{liMultiViewMammographicDensity2021a,
	title = {Multi-{View} {Mammographic} {Density} {Classification} by {Dilated} and {Attention}-{Guided} {Residual} {Learning}},
	volume = {18},
	issn = {1545-5963, 1557-9964, 2374-0043},
	url = {https://ieeexplore.ieee.org/document/8978513/},
	doi = {10.1109/TCBB.2020.2970713},
	abstract = {Breast density is widely adopted to reﬂect the likelihood of early breast cancer development. Existing methods of mammographic density classiﬁcation either require steps of manual operations or achieve only moderate classiﬁcation accuracy due to the limited model capacity. In this study, we present a radiomics approach based on dilated and attention-guided residual learning for the task of mammographic density classiﬁcation. The proposed method was instantiated with two datasets, one clinical dataset and one publicly available dataset, and classiﬁcation accuracies of 88.7 and 70.0 percent were obtained, respectively. Although the classiﬁcation accuracy of the public dataset was lower than the clinical dataset, which was very likely related to the dataset size, our proposed model still achieved a better performance than the naive residual networks and several recently published deep learningbased approaches. Furthermore, we designed a multi-stream network architecture speciﬁcally targeting at analyzing the multi-view mammograms. Utilizing the clinical dataset, we validated that multi-view inputs were beneﬁcial to the breast density classiﬁcation task with an increase of at least 2.0 percent in accuracy and the different views lead to different model classiﬁcation capacities. Our method has a great potential to be further developed and applied in computer-aided diagnosis systems. Our code is available at https://github. com/lich0031/Mammographic\_Density\_Classiﬁcation.},
	language = {en},
	number = {3},
	urldate = {2023-05-24},
	journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
	author = {Li, Cheng and Xu, Jingxu and Liu, Qiegen and Zhou, Yongjin and Mou, Lisha and Pu, Zuhui and Xia, Yong and Zheng, Hairong and Wang, Shanshan},
	month = may,
	year = {2021},
	pages = {1003--1013},
	file = {Li et al. - 2021 - Multi-View Mammographic Density Classification by .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LM239BHF\\Li et al. - 2021 - Multi-View Mammographic Density Classification by .pdf:application/pdf},
}

@article{yangMommiNetv2MammographicMultiview2021,
	title = {{MommiNet}-v2: {Mammographic} multi-view mass identification networks},
	volume = {73},
	issn = {13618415},
	shorttitle = {{MommiNet}-v2},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841521002498},
	doi = {10.1016/j.media.2021.102204},
	language = {en},
	urldate = {2023-05-24},
	journal = {Medical Image Analysis},
	author = {Yang, Zhicheng and Cao, Zhenjie and Zhang, Yanbo and Tang, Yuxing and Lin, Xiaohui and Ouyang, Rushan and Wu, Mingxiang and Han, Mei and Xiao, Jing and Huang, Lingyun and Wu, Shibin and Chang, Peng and Ma, Jie},
	month = oct,
	year = {2021},
	pages = {102204},
	file = {Yang et al. - 2021 - MommiNet-v2 Mammographic multi-view mass identifi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TNWACREQ\\Yang et al. - 2021 - MommiNet-v2 Mammographic multi-view mass identifi.pdf:application/pdf},
}

@inproceedings{choEvaluationUnetbasedImage2021,
	address = {Online Only, United States},
	title = {Evaluation of {U}-net-based image segmentation model to digital mammography},
	isbn = {978-1-5106-4021-4 978-1-5106-4022-1},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11596/2581401/Evaluation-of-U-net-based-image-segmentation-model-to-digital/10.1117/12.2581401.full},
	doi = {10.1117/12.2581401},
	abstract = {Detecting suspicious lesions in medical imaging is the important ﬁrst step in computer-aided detection (CAD) systems. However, detecting abnormalities in breast tissue is diﬃcult due to the lesion’s varying size, shape, margin, and contrast with the background tissue. We focused on mass segmentation, a method that provides notable morphological features by outlining contours of masses. Accurate segmentation is crucial for correct diagnosis. Recent advancements in deep learning have improved object detection and segmentation, and these techniques are also being applied to medical imaging studies. We focused on U-net, which is a recently developed mass segmentation algorithm based on a fully convolutional network. The U-net architecture consists of (1) a contracting path to increase the resolution of the output and (2) a symmetric expanding path to better locate the region of interest. The performance of a U-net model was tested with 63 digital mammograms from INbreast, a publicly available database. We trained the model with images resized to 40x40 pixels and conducted 10-fold cross-validation to prevent overﬁtting. The model’s performance with respect to breast density and the lesion’s BI-RADS rating was also investigated. Dice coeﬃcients (DC) were used as a performance measure to compare the predicted segmentation of the model with the ground truth. Logistic regression and an analysis of variance were performed to determine the signiﬁcance of the DCs with regards to breast density and lesion behavior and to calculate the 95\% conﬁdence interval. The average DC was 0.80. The diﬀerence between DCs for BI-RADS 2 and 4c and for BI-RADS 2 and 5 were signiﬁcant, suggesting that the model has more diﬃculty in segmenting benign lesions.},
	language = {en},
	urldate = {2023-05-24},
	booktitle = {Medical {Imaging} 2021: {Image} {Processing}},
	publisher = {SPIE},
	author = {Cho, Priscilla and Yoon, Hong-Jun},
	editor = {Landman, Bennett A. and Išgum, Ivana},
	month = feb,
	year = {2021},
	pages = {74},
	file = {Cho and Yoon - 2021 - Evaluation of U-net-based image segmentation model.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RXR5GMY8\\Cho and Yoon - 2021 - Evaluation of U-net-based image segmentation model.pdf:application/pdf},
}

@article{oyeladeGenerativeAdversarialNetwork2022,
	title = {A generative adversarial network for synthetization of regions of interest based on digital mammograms},
	volume = {12},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-022-09929-9},
	doi = {10.1038/s41598-022-09929-9},
	abstract = {Abstract
            Deep learning (DL) models are becoming pervasive and applicable to computer vision, image processing, and synthesis problems. The performance of these models is often improved through architectural configuration, tweaks, the use of enormous training data, and skillful selection of hyperparameters. The application of deep learning models to medical image processing has yielded interesting performance, capable of correctly detecting abnormalities in medical digital images, making them surpass human physicians. However, advancing research in this domain largely relies on the availability of training datasets. These datasets are sometimes not publicly accessible, insufficient for training, and may also be characterized by a class imbalance among samples. As a result, inadequate training samples and difficulty in accessing new datasets for training deep learning models limit performance and research into new domains. Hence, generative adversarial networks (GANs) have been proposed to mediate this gap by synthesizing data similar to real sample images. However, we observed that benchmark datasets with regions of interest (ROIs) for characterizing abnormalities in breast cancer using digital mammography do not contain sufficient data with a fair distribution of all cases of abnormalities. For instance, the architectural distortion and breast asymmetry in digital mammograms are sparsely distributed across most publicly available datasets. This paper proposes a GAN model, named ROImammoGAN, which synthesizes ROI-based digital mammograms. Our approach involves the design of a GAN model consisting of both a generator and a discriminator to learn a hierarchy of representations for abnormalities in digital mammograms. Attention is given to architectural distortion, asymmetry, mass, and microcalcification abnormalities so that training distinctively learns the features of each abnormality and generates sufficient images for each category. The proposed GAN model was applied to MIAS datasets, and the performance evaluation yielded a competitive accuracy for the synthesized samples. In addition, the quality of the images generated was also evaluated using PSNR, SSIM, FSIM, BRISQUE, PQUE, NIQUE, FID, and geometry scores. The results showed that ROImammoGAN performed competitively with state-of-the-art GANs. The outcome of this study is a model for augmenting CNN models with ROI-centric image samples for the characterization of abnormalities in breast images.},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {Scientific Reports},
	author = {Oyelade, Olaide N. and Ezugwu, Absalom E. and Almutairi, Mubarak S. and Saha, Apu Kumar and Abualigah, Laith and Chiroma, Haruna},
	month = apr,
	year = {2022},
	pages = {6166},
	file = {Oyelade et al. - 2022 - A generative adversarial network for synthetizatio.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UXT6HMCW\\Oyelade et al. - 2022 - A generative adversarial network for synthetizatio.pdf:application/pdf},
}

@article{rangarajanUltrahighResolutionMultiscale2022a,
	title = {Ultra-high resolution, multi-scale, context-aware approach for detection of small cancers on mammography},
	volume = {12},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-022-15259-7},
	doi = {10.1038/s41598-022-15259-7},
	abstract = {Abstract
            
              While detection of malignancies on mammography has received a boost with the use of Convolutional Neural Networks (CNN), detection of cancers of very small size remains challenging. This is however clinically significant as the purpose of mammography is early detection of cancer, making it imperative to pick them up when they are still very small. Mammography has the highest spatial resolution (image sizes as high as 3328 × 4096 pixels) out of all imaging modalities, a requirement that stems from the need to detect fine features of the smallest cancers on screening. However due to computational constraints, most state of the art CNNs work on reduced resolution images. Those that work on higher resolutions, compromise on global context and work at single scale. In this work, we show that resolution, scale and image-context are all important independent factors in detection of small masses. We thereby use a fully convolutional network, with the ability to take any input size. In addition, we incorporate a systematic multi-scale, multi-resolution approach, and encode image context, which we show are critical factors to detection of small masses. We show that this approach improves the detection of cancer, particularly for small masses in comparison to the baseline model. We perform a single institution multicentre study, and show the performance of the model on a diagnostic mammography dataset, a screening mammography dataset, as well as a curated dataset of small cancers {\textless} 1 cm in size. We show that our approach improves the sensitivity from 61.53 to 87.18\% at 0.3 False Positives per Image (FPI) on this small cancer dataset. Model and code are available from 
              https://github.com/amangupt01/Small\_Cancer\_Detection},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {Scientific Reports},
	author = {Rangarajan, Krithika and Gupta, Aman and Dasgupta, Saptarshi and Marri, Uday and Gupta, Arun Kumar and Hari, Smriti and Banerjee, Subhashis and Arora, Chetan},
	month = jul,
	year = {2022},
	pages = {11622},
	file = {Rangarajan et al. - 2022 - Ultra-high resolution, multi-scale, context-aware .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CGWMTXJG\\Rangarajan et al. - 2022 - Ultra-high resolution, multi-scale, context-aware .pdf:application/pdf},
}

@article{braggImagingNoncalcifiedDuctal2021,
	title = {Imaging of {Noncalcified} {Ductal} {Carcinoma} \textit{{In} {Situ}}},
	volume = {11},
	issn = {2156-5597, 2156-7514},
	url = {https://clinicalimagingscience.org/imaging-of-noncalcified-ductal-carcinoma/},
	doi = {10.25259/JCIS_48_2021},
	abstract = {Ductal carcinoma in situ (DCIS) is a commonly encountered malignancy, accounting for approximately 20\% of new breast cancer diagnoses in the United States. DCIS is characterized by a proliferation of tumor cells within the terminal duct lobular unit with preservation of the basement membrane. Typically nonpalpable and asymptomatic, DCIS is most often detected as calcifications on screening mammography. However, DCIS may also be noncalcified. When compared to calcified DCIS, noncalcified DCIS is more likely to be symptomatic, with patients most often presenting with nipple discharge or a palpable mass. Diagnosing noncalcified DCIS is challenging since it may be occult or subtle on mammography, and ultrasound findings can be nonspecific and may be interpreted as benign fibrocystic changes. In cases with a calcified component of DCIS, the extent of DCIS may be underestimated by mammography because not all involved areas may calcify. Breast magnetic resonance imaging (MRI), although less readily available than mammography and ultrasound, is advantageous in detecting noncalcified DCIS, especially high grade DCIS, which may not develop microcalcifications. MRI relies on abnormal contrast uptake due to tumor vascularity and changes in vessel density and permeability. This pictoral review presents the spectrum of imaging findings of noncalcified DCIS to assist radiologists in accurately detecting and describing its key imaging findings. Utilizing different modalities, we review the differential diagnoses for noncalcified DCIS, show illustrative cases of noncalcified DCIS, and discuss the importance of this entity.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Journal of Clinical Imaging Science},
	author = {Bragg, Ashley and Candelaria, Rosalind and Adrada, Beatriz and Huang, Monica and Rauch, Gaiane and Santiago, Lumarie and Scoggins, Marion and Whitman, Gary},
	month = jun,
	year = {2021},
	pages = {34},
	file = {Bragg et al. - 2021 - Imaging of Noncalcified Ductal Carcinoma In Sit.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PF9MA7YG\\Bragg et al. - 2021 - Imaging of Noncalcified Ductal Carcinoma In Sit.pdf:application/pdf},
}

@article{duanMultiscaleContrastEnhancement2019,
	title = {A {Multiscale} {Contrast} {Enhancement} for {Mammogram} {Using} {Dynamic} {Unsharp} {Masking} in {Laplacian} {Pyramid}},
	volume = {3},
	issn = {2469-7311, 2469-7303},
	url = {https://ieeexplore.ieee.org/document/8500347/},
	doi = {10.1109/TRPMS.2018.2876873},
	abstract = {Mammography is the ﬁrst option for screening breast cancer. However, some lesions may be missed due to superimposition of breast parenchymal patterns/tissues. Current enhancement methods can highlight some speciﬁc tissues, but the edges are weakened or extra noise is added. For improving the solution, this paper proposed a multiscale contrast enhancement for mammogram using dynamic unsharp masking (UM) in Laplacian pyramid. Laplacian pyramid is utilized to preserve the ﬁne structure at each scale. Dynamic UM is presented to adaptively enhance details and suppress noise simultaneously. The proposed method mainly consists of ﬁve steps: 1) downsample images; 2) calculate the weight of dynamic UM; 3) utilize dynamic UM enhancement for each scale image; 4) subtract the enhancement results for each scale image to acquire Laplacian pyramid images; and 5) restore the ﬁnal enhanced image. To evaluate the proposed method qualitatively and quantitatively, one phantom and three clinical mammography cases were evaluated. Results showed the proposed method can provide much clearer details of the mammary gland. Quantitatively, information entropy and peak signal-to-noise ratio are increased by 0.96 and 3.89 at most compared with the state-of-the-art method. The proposed method has demonstrated that different types of regions are enhanced with the help of a regional adaptive evolution, which has great potential for adaptively enhancing the details and relatively suppressing the noise.},
	language = {en},
	number = {5},
	urldate = {2023-05-24},
	journal = {IEEE Transactions on Radiation and Plasma Medical Sciences},
	author = {Duan, Xiaoman and Xu, Yuan and Mei, Yingjie and Wu, Shuyu and Ling, Qingqing and Qin, Genggeng and Ma, Jianhui and Chen, Chaomin and Qi, Hongliang and Zhou, Linghong},
	month = sep,
	year = {2019},
	pages = {557--564},
	file = {Duan et al. - 2019 - A Multiscale Contrast Enhancement for Mammogram Us.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WQAX585U\\Duan et al. - 2019 - A Multiscale Contrast Enhancement for Mammogram Us.pdf:application/pdf},
}

@article{liuDetectingCancerMetastases,
	title = {Detecting {Cancer} {Metastases} on {Gigapixel} {Pathology} {Images}},
	abstract = {Each year, the treatment decisions for more than 230, 000 breast cancer patients in the U.S. hinge on whether the cancer has metastasized away from the breast. Metastasis detection is currently performed by pathologists reviewing large expanses of biological tissues. This process is labor intensive and error-prone. We present a framework to automatically detect and localize tumors as small as 100 × 100 pixels in gigapixel microscopy images sized 100, 000×100, 000 pixels. Our method leverages a convolutional neural network (CNN) architecture and obtains state-of-the-art results on the Camelyon16 dataset in the challenging lesion-level tumor detection task. At 8 false positives per image, we detect 92.4\% of the tumors, relative to 82.7\% by the previous best automated approach. For comparison, a human pathologist attempting exhaustive search achieved 73.2\% sensitivity. We achieve image-level AUC scores above 97\% on both the Camelyon16 test set and an independent set of 110 slides. In addition, we discover that two slides in the Camelyon16 training set were erroneously labeled normal. Our approach could considerably reduce false negative rates in metastasis detection.},
	language = {en},
	author = {Liu, Yun and Gadepalli, Krishna and Norouzi, Mohammad and Dahl, George E and Kohlberger, Timo and Boyko, Aleksey and Venugopalan, Subhashini and Timofeev, Aleksei and Nelson, Philip Q and Corrado, Greg S and Hipp, Jason D and Peng, Lily and Stumpe, Martin C},
	file = {Liu et al. - Detecting Cancer Metastases on Gigapixel Pathology.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MYE92GQT\\Liu et al. - Detecting Cancer Metastases on Gigapixel Pathology.pdf:application/pdf},
}

@article{dominguesUsingDeepLearning2020,
	title = {Using deep learning techniques in medical imaging: a systematic review of applications on {CT} and {PET}},
	volume = {53},
	issn = {0269-2821, 1573-7462},
	shorttitle = {Using deep learning techniques in medical imaging},
	url = {http://link.springer.com/10.1007/s10462-019-09788-3},
	doi = {10.1007/s10462-019-09788-3},
	abstract = {Medical imaging is a rich source of invaluable information necessary for clinical judgements. However, the analysis of those exams is not a trivial assignment. In recent times, the use of deep learning (DL) techniques, supervised or unsupervised, has been empowered and it is one of the current research key areas in medical image analysis. This paper presents a survey of the use of DL architectures in computer-assisted imaging contexts, attending two different image modalities: the actively studied computed tomography and the under-studied positron emission tomography, as well as the combination of both modalities, which has been an important landmark in several decisions related to numerous diseases. In the making of this review, we analysed over 180 relevant studies, published between 2014 and 2019, that are sectioned by the purpose of the research and the imaging modality type. We conclude by addressing research issues and suggesting future directions for further improvement. To our best knowledge, there is no previous work making a review of this issue.},
	language = {en},
	number = {6},
	urldate = {2023-05-24},
	journal = {Artificial Intelligence Review},
	author = {Domingues, Inês and Pereira, Gisèle and Martins, Pedro and Duarte, Hugo and Santos, João and Abreu, Pedro Henriques},
	month = aug,
	year = {2020},
	pages = {4093--4160},
	file = {Domingues et al. - 2020 - Using deep learning techniques in medical imaging.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MWLJUEJ6\\Domingues et al. - 2020 - Using deep learning techniques in medical imaging.pdf:application/pdf},
}

@article{leeCuratedMammographyData2017a,
	title = {A curated mammography data set for use in computer-aided detection and diagnosis research},
	volume = {4},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/sdata2017177},
	doi = {10.1038/sdata.2017.177},
	abstract = {Abstract
            Published research results are difficult to replicate due to the lack of a standard evaluation data set in the area of decision support systems in mammography; most computer-aided diagnosis (CADx) and detection (CADe) algorithms for breast cancer in mammography are evaluated on private data sets or on unspecified subsets of public databases. This causes an inability to directly compare the performance of methods or to replicate prior results. We seek to resolve this substantial challenge by releasing an updated and standardized version of the Digital Database for Screening Mammography (DDSM) for evaluation of future CADx and CADe systems (sometimes referred to generally as CAD) research in mammography. Our data set, the CBIS-DDSM (Curated Breast Imaging Subset of DDSM), includes decompressed images, data selection and curation by trained mammographers, updated mass segmentation and bounding boxes, and pathologic diagnosis for training data, formatted similarly to modern computer vision data sets. The data set contains 753 calcification cases and 891 mass cases, providing a data-set size capable of analyzing decision support systems in mammography.},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {Scientific Data},
	author = {Lee, Rebecca Sawyer and Gimenez, Francisco and Hoogi, Assaf and Miyake, Kanae Kawai and Gorovoy, Mia and Rubin, Daniel L.},
	month = dec,
	year = {2017},
	pages = {170177},
	file = {Lee et al. - 2017 - A curated mammography data set for use in computer.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EG44BNNF\\Lee et al. - 2017 - A curated mammography data set for use in computer.pdf:application/pdf},
}

@article{yalaDeepLearningModel2019,
	title = {A {Deep} {Learning} {Model} to {Triage} {Screening} {Mammograms}: {A} {Simulation} {Study}},
	volume = {293},
	issn = {0033-8419, 1527-1315},
	shorttitle = {A {Deep} {Learning} {Model} to {Triage} {Screening} {Mammograms}},
	url = {http://pubs.rsna.org/doi/10.1148/radiol.2019182908},
	doi = {10.1148/radiol.2019182908},
	abstract = {Background:  Recent deep learning (DL) approaches have shown promise in improving sensitivity but have not addressed limitations in radiologist specificity or efficiency.
Purpose:  To develop a DL model to triage a portion of mammograms as cancer free, improving performance and workflow efficiency. Materials and Methods:  In this retrospective study, 223 109 consecutive screening mammograms performed in 66 661 women from January 2009 to December 2016 were collected with cancer outcomes obtained through linkage to a regional tumor registry. This cohort was split by patient into 212 272, 25 999, and 26 540 mammograms from 56 831, 7021, and 7176 patients for training, validation, and testing, respectively. A DL model was developed to triage mammograms as cancer free and evaluated on the test set. A DL-triage workflow was simulated in which radiologists skipped mammograms triaged as cancer free (interpreting them as negative for cancer) and read mammograms not triaged as cancer free by using the original interpreting radiologists’ assessments. Sensitivities, specificities, and percentage of mammograms read were calculated, with and without the DL-triage–simulated workflow. Statistics were computed across 5000 bootstrap samples to assess confidence intervals (CIs). Specificities were compared by using a two-tailed t test (P , .05) and sensitivities were compared by using a one-sided t test with a noninferiority margin of 5\% (P , .05).
Results:  The test set included 7176 women (mean age, 57.8 years 6 10.9 [standard deviation]). When reading all mammograms, radiologists obtained a sensitivity and specificity of 90.6\% (173 of 191; 95\% CI: 86.6\%, 94.7\%) and 93.5\% (24 625 of 26 349; 95\% CI: 93.3\%, 93.9\%). In the DL-simulated workflow, the radiologists obtained a sensitivity and specificity of 90.1\% (172 of 191; 95\% CI: 86.0\%, 94.3\%) and 94.2\% (24 814 of 26 349; 95\% CI: 94.0\%, 94.6\%) while reading 80.7\% (21 420 of 26 540) of the mammograms. The simulated workflow improved specificity (P = .002) and obtained a noninferior sensitivity with a margin of 5\% (P , .001).
Conclusion:  This deep learning model has the potential to reduce radiologist workload and significantly improve specificity without harming sensitivity.},
	language = {en},
	number = {1},
	urldate = {2023-05-24},
	journal = {Radiology},
	author = {Yala, Adam and Schuster, Tal and Miles, Randy and Barzilay, Regina and Lehman, Constance},
	month = oct,
	year = {2019},
	pages = {38--46},
	file = {Yala et al. - 2019 - A Deep Learning Model to Triage Screening Mammogra.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9VTMHFAE\\Yala et al. - 2019 - A Deep Learning Model to Triage Screening Mammogra.pdf:application/pdf},
}

@article{musaArtificialIntelligenceMachine2022,
	title = {Artificial {Intelligence} and {Machine} {Learning} in {Cancer} {Research}: {A} {Systematic} and {Thematic} {Analysis} of the {Top} 100 {Cited} {Articles} {Indexed} in {Scopus} {Database}},
	volume = {29},
	issn = {1073-2748, 1526-2359},
	shorttitle = {Artificial {Intelligence} and {Machine} {Learning} in {Cancer} {Research}},
	url = {http://journals.sagepub.com/doi/10.1177/10732748221095946},
	doi = {10.1177/10732748221095946},
	abstract = {Introduction: Cancer is a major public health problem and a global leading cause of death where the screening, diagnosis, prediction, survival estimation, and treatment of cancer and control measures are still a major challenge. The rise of Artiﬁcial Intelligence (AI) and Machine Learning (ML) techniques and their applications in various ﬁelds have brought immense value in providing insights into advancement in support of cancer control.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Cancer Control},
	author = {Musa, Ibrahim H. and Afolabi, Lukman O. and Zamit, Ibrahim and Musa, Taha H. and Musa, Hassan H. and Tassang, Andrew and Akintunde, Tosin Y. and Li, Wei},
	month = nov,
	year = {2022},
	pages = {107327482210959},
	file = {Musa et al. - 2022 - Artificial Intelligence and Machine Learning in Ca.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RF5K8GQY\\Musa et al. - 2022 - Artificial Intelligence and Machine Learning in Ca.pdf:application/pdf},
}

@article{kyonoTriage2DMammographic2021,
	title = {Triage of {2D} {Mammographic} {Images} {Using} {Multi}-view {Multi}-task {Convolutional} {Neural} {Networks}},
	volume = {2},
	issn = {2691-1957, 2637-8051},
	url = {https://dl.acm.org/doi/10.1145/3453166},
	doi = {10.1145/3453166},
	abstract = {With an aging and growing population, the number of women receiving mammograms is increasing. However, existing techniques for autonomous diagnosis do not surpass a well-trained radiologist. Therefore, to reduce the number of mammograms that require examination by a radiologist, subject to preserving the diagnostic accuracy observed in current clinical practice, we develop Man and Machine Mammography Oracle (MAMMO)—a clinical decision support system capable of determining whether its predicted diagnoses require further radiologist examination. We first introduce a novel multi-view convolutional neural network (CNN) trained using multi-task learning (MTL) to diagnose mammograms and predict the radiological assessments known to be associated with cancer. MTL improves diagnostic performance and triage efficiency while providing an additional layer of model interpretability. Furthermore, we introduce a novel triage network that takes as input the radiological assessment and diagnostic predictions of the multi-view CNN and determines whether the radiologist or CNN will most likely provide the correct diagnosis. Results obtained on a dataset of over 7,000 patients show that MAMMO reduced the number of diagnostic mammograms requiring radiologist reading by 42.8\% while improving the overall diagnostic accuracy in comparison to readings done by radiologists alone.},
	language = {en},
	number = {3},
	urldate = {2023-05-24},
	journal = {ACM Transactions on Computing for Healthcare},
	author = {Kyono, Trent and Gilbert, Fiona J. and Schaar, Mihaela Van Der},
	month = jul,
	year = {2021},
	pages = {1--24},
	file = {Kyono et al. - 2021 - Triage of 2D Mammographic Images Using Multi-view .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JJ353XVR\\Kyono et al. - 2021 - Triage of 2D Mammographic Images Using Multi-view .pdf:application/pdf},
}

@inproceedings{10.1145/3368756.3369039,
	address = {New York, NY, USA},
	series = {{SCA} '19},
	title = {Breast cancer image segmentation and classification},
	isbn = {978-1-4503-6289-4},
	url = {https://doi.org/10.1145/3368756.3369039},
	doi = {10.1145/3368756.3369039},
	abstract = {Breast cancer is the most common malignancy that affects women all over the world, especially in morocco with 35.8\% [1]. The effective way to diagnose and treat breast cancer is the earlier detection of symptoms and signs which can arise the chances of preconizing an earlier treatment, probably specific one. Thus, it can extremely affect the chances to survive by delaying the cancer or the total elimination. Mammography is a highly accurate radiography technique that represents a basic and crucial phase in preventing and detecting early this disease. Using the mammography images, we propose in this paper to develop a computer aided diagnosis (CAD) system named Earlier Breast Cancer Computer Aided Detection (EBCCAD) which aim is to detect and classify breast cancer images. The obtained results are promising and encouraging to evaluate the proposed system for real cases.},
	booktitle = {Proceedings of the 4th international conference on smart city applications},
	publisher = {Association for Computing Machinery},
	author = {Khoulqi, Ichrak and Idrissi, Najlae},
	year = {2019},
	note = {Number of pages: 9
Place: Casablanca, Morocco
tex.articleno: 59},
	keywords = {classification, breast cancer, computer aided diagnosis (CAD), earlier breast cancer computer aided detection (EBCCAD), microcalcifications (MCCs), segmentation, vote},
}

@inproceedings{10.1145/3307339.3342166,
	address = {New York, NY, USA},
	series = {{BCB} '19},
	title = {Fusion in breast cancer histology classification},
	isbn = {978-1-4503-6666-3},
	url = {https://doi.org/10.1145/3307339.3342166},
	doi = {10.1145/3307339.3342166},
	abstract = {Breast cancer is a deadly disease that affects millions of women worldwide. The International Conference on Image Analysis and Recognition in 2018 presents the BreAst Cancer Histology (ICIAR2018 BACH) image data challenge that calls for computer tools to assist pathologists and doctors in the clinical diagnosis of breast cancer subtypes. Using the BACH dataset, we have developed an image classification pipeline that combines both a shallow learner (support vector machine) and a deep learner (convolutional neural network). The shallow learner and deep learners achieved moderate accuracies of 79\% and 81\% individually. When being integrated by fusion algorithms, the system outperformed any individual learner with the highest accuracy as 92\%. The fusion presents big potential for improving clinical design support. KEYWORDS},
	booktitle = {Proceedings of the 10th {ACM} international conference on bioinformatics, computational biology and health informatics},
	publisher = {Association for Computing Machinery},
	author = {Vizcarra, Juan and Place, Ryan and Tong, Li and Gutman, David and Wang, May Dongmei},
	year = {2019},
	note = {Number of pages: 9
Place: Niagara Falls, NY, USA},
	keywords = {deep learning, support vector machines, breast cancer, fusion, histology, surf},
	pages = {485--493},
	file = {Vizcarra et al_2019_Fusion in breast cancer histology classification.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\W76979L5\\Vizcarra et al_2019_Fusion in breast cancer histology classification.pdf:application/pdf},
}

@inproceedings{10.1145/3373477.3373489,
	address = {New York, NY, USA},
	series = {{AISS} '19},
	title = {Fuzzy decision tree for breast cancer prediction},
	isbn = {978-1-4503-7291-6},
	url = {https://doi.org/10.1145/3373477.3373489},
	doi = {10.1145/3373477.3373489},
	abstract = {Medical errors are considered as the leading cause of death and injury. Breast cancer becomes one of the leading causes of death among women, not only in the Philippines but worldwide. In this paper, data mining was used to predict the stage of breast cancer using a hybrid of fuzzy logic and decision tree. This aims to help experts to make decisions rather than replacing them. The result will only give an expert a recommendation, but the final decision is still on the hands of the experts. Feature selection was used to determine the best attribute in the dataset from Surveillance Epidemiology and End Results (SEER). The data set consists of incidence from 1975 to 2016, but the study limits the analysis from 2010 to 2016. Different cleaning and preprocessing of data are conducted. After thorough preprocessing of data, six (6) attributes are selected, and one (1) target class. Performance comparison shows that the fuzzy decision tree achieved a higher accuracy of 99.96\%, sensitivity of 99.26\% and specificity of 99.98\% than the decision tree classification technique. The simulation result shows a correctly classified instance of 165,124, which is equivalent to 99.97\% and only 351 incorrect classified instances or 0.21\%. Thus, a fuzzy decision tree is more robust than the traditional decision tree classifier for predicting the stage of breast cancer.},
	booktitle = {Proceedings of the 1st international conference on advanced information science and system},
	publisher = {Association for Computing Machinery},
	author = {Domingo, Mylene J. and Gerardo, Bobby D. and Medina, Ruji P.},
	year = {2020},
	note = {Number of pages: 6
Place: Singapore, Singapore
tex.articleno: 12},
	keywords = {classification, data mining, prediction, medical errors, breast cancer, decision tree, fuzzy decision tree, fuzzy logic, stage of breast cancer},
}

@inproceedings{10.1145/3328833.3328867,
	address = {New York, NY, USA},
	series = {{ICSIE} '19},
	title = {Deep learning approach for breast cancer diagnosis},
	isbn = {978-1-4503-6105-7},
	url = {https://doi.org/10.1145/3328833.3328867},
	doi = {10.1145/3328833.3328867},
	abstract = {Breast cancer is one of the leading fatal disease worldwide with high risk control if early discovered. Conventional method for breast screening is x-ray mammography, which is known to be challenging for early detection of cancer lesions. The dense breast structure produced due to the compression process during imaging lead to difficulties to recognize small size abnormalities. Also, inter- and intra-variations of breast tissues lead to significant difficulties to achieve high diagnosis accuracy using hand-crafted features. Deep learning is an emerging machine learning technology that requires a relatively high computation power. Yet, it proved to be very effective in several difficult tasks that requires decision making at the level of human intelligence. In this paper, we develop a new network architecture inspired by the U-net structure that can be used for effective and early detection of breast cancer. Results indicate a high rate of sensitivity and specificity that indicate potential usefulness of the proposed approach in clinical use.},
	booktitle = {Proceedings of the 8th international conference on software and information engineering},
	publisher = {Association for Computing Machinery},
	author = {Rashed, Essam and El Seoud, M. Samir Abou},
	year = {2019},
	note = {Number of pages: 5
Place: Cairo, Egypt},
	keywords = {Deep learning, Breast cancer diagnosis, Convolution neural networks},
	pages = {243--247},
	file = {Rashed_El Seoud_2019_Deep learning approach for breast cancer diagnosis.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\AS85YC25\\Rashed_El Seoud_2019_Deep learning approach for breast cancer diagnosis.pdf:application/pdf},
}

@inproceedings{10.1145/3444884.3444918,
	address = {New York, NY, USA},
	series = {{ICBBE} '20},
	title = {The whole view of therapies for breast cancer},
	isbn = {978-1-4503-8822-1},
	url = {https://doi.org/10.1145/3444884.3444918},
	doi = {10.1145/3444884.3444918},
	abstract = {Breast cancer is a leading cause of death in women, influencing on 1.7 million patients every year in the whole world. So regarding to the treatments of the breast cancer it is very important to summarize and review. \&nbsp;Although there are many treatments to the breast cancer, the comparison among these were not addressed. In this review, we did a comprehensive literature research and summarized treatments for breast cancer, especially going through the popular therapy for human breast cancer, analyzing the clinical trial data and the adverse effect of the way. We all know breast cancer contains many subtypes and we summarized the treatments in terms of different cancer subtypes.A deep understanding of human breast cancer could bring a better effect of curing the disease, based on the known therapies we also predicted the future directions of the treatments for human breast cancer, which potentially showed a great achievements in breast cancer treatment.},
	booktitle = {Proceedings of the 2020 7th international conference on biomedical and bioinformatics engineering},
	publisher = {Association for Computing Machinery},
	author = {Jie, Cui},
	year = {2021},
	note = {Number of pages: 4
Place: Kyoto, Japan},
	keywords = {Breast cancer, CTLA-4, PD-1/PD-L1, therapy},
	pages = {176--179},
}

@inproceedings{10.1145/3511808.3557107,
	address = {New York, NY, USA},
	series = {{CIKM} '22},
	title = {Breast cancer early detection with time series classification},
	isbn = {978-1-4503-9236-5},
	url = {https://doi.org/10.1145/3511808.3557107},
	doi = {10.1145/3511808.3557107},
	abstract = {Breast cancer has become the leading cause of women cancer death worldwide. Despite the consensus that breast cancer early detection can significantly reduce treatment difficulty and cancer mortality, people still are reluctant to go to hospital for regular checkups due to the high costs incurred. A timely, private, affordable, and effective household breast cancer early detection solution is badly needed. In this paper, we propose a household solution that utilizes pairs of sensors embedded in the bra to measure the thermal and moisture time series data (BTMTSD) of the breast surface and conduct time series classification (TSC) to diagnose breast cancer. Three main challenges are encountered when doing BTMTSD classification, (1) small supervised dataset, which is a common limitation of medical research, (2) noisy time series with unique noise patterns, and (3) complex interplay patterns across multiple time series dimensions. To mitigate these problems, we incorporate multiple data augmentation and transformation techniques with various deep learning TSC approaches and compare their performances for the BTMTSD classification task. Experimental results validate the effectiveness of our framework in providing reliable breast cancer early detection.},
	booktitle = {Proceedings of the 31st {ACM} international conference on information \&amp; knowledge management},
	publisher = {Association for Computing Machinery},
	author = {Zhu, Haoren and Zhao, Pengfei and Chan, Yiu-Pong and Kang, Hong and Lee, Dik Lun},
	year = {2022},
	note = {Number of pages: 11
Place: Atlanta, GA, USA},
	keywords = {breast cancer early detection, convolutional neural networks, time series classification},
	pages = {3735--3745},
	file = {Zhu et al_2022_Breast cancer early detection with time series classification.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\R9IMXU79\\Zhu et al_2022_Breast cancer early detection with time series classification.pdf:application/pdf},
}

@inproceedings{10.1145/3549206.3549274,
	address = {New York, NY, USA},
	series = {{IC3}-2022},
	title = {Prediction of breast cancer using machine learning techniques},
	isbn = {978-1-4503-9675-2},
	url = {https://doi.org/10.1145/3549206.3549274},
	doi = {10.1145/3549206.3549274},
	abstract = {Breast cancer is a topic that is frequently discussed these days. It is one of the most widespread diseases and forms of cancer. The National Cancer Institute says that the second most frequent malignancy in women is breast cancer. Every year, around 2000 new instances of breast cancer are diagnosed in males, while approximately 2,30,000 new cases are diagnosed in women. The diagnosis of this disease is crucial so that women can get it treated in its early stages. It is best for an precise and timely diagnosis. This is a critical phase in the therapy and recovery process. Breast cancer detection is done with the help of mammograms, which are basically X-rays of the breasts. It's a tool that is used to detect and help diagnose breast cancer. But detection is not easy due to different kinds of uncertainties in using these mammograms. Breast cancer can be detected using machine learning algorithms. These approaches can be used to create tools for clinicians that can be used to identify and diagnose breast cancer at an early stage. This will significantly improve patient survival rates.},
	booktitle = {Proceedings of the 2022 fourteenth international conference on contemporary computing},
	publisher = {Association for Computing Machinery},
	author = {Dikshit, Parthh and Dey, Bhawna and Shukla, Ayush and Singh, Akhilesh and Chadha, Tarankit and Sehgal, Vivek Kumar},
	year = {2022},
	note = {Number of pages: 6
Place: Noida, India},
	pages = {382--387},
}

@inproceedings{10.1145/3368691.3368703,
	address = {New York, NY, USA},
	series = {{DATA} '19},
	title = {An experimental study for breast cancer prediction algorithms},
	isbn = {978-1-4503-7284-8},
	url = {https://doi.org/10.1145/3368691.3368703},
	doi = {10.1145/3368691.3368703},
	abstract = {Breast Cancer has turned into a typical disease around the globe and harvesting the life of young women and the main source of cancer death and caused 22.9\% of a wide range of cancers in women. The development of massive breast cancer screening has led to earlier diagnosis and rapid management with a significant improvement in survival rate. The problem of automatically searching for information contained in medical images is urgently needed. In order to process this large volume of information, doctors are currently turning to the use of Machine Learning (ML) systems to assist in the analysis and interpretation of these images. In this paper, we have conducted an experimental study for three major ML algorithms such as K-Nearest Neighbor (KNN), Random Forest (RF) and Multilayer Perceptron (MLP) to classify Brest cancer. The experiments were conducted on WDBC dataset to obtain the best algorithms in term of accuracy. Finally, identify the most specific and relevant attributes of the malignant tumour classification through more than one feature selection algorithms.},
	booktitle = {Proceedings of the second international conference on data science, {E}-learning and information systems},
	publisher = {Association for Computing Machinery},
	author = {Al-Shargabi, Bassam and Al-Shami, Fida'a},
	year = {2019},
	note = {Number of pages: 6
Place: Dubai, United Arab Emirates
tex.articleno: 12},
	keywords = {machine learning, feature selection, breast cancer, evaluation metrics, multi-layer perceptron},
}

@article{10.1109/TCBB.2020.2980831,
	title = {Imbalanced breast cancer classification using transfer learning},
	volume = {18},
	issn = {1545-5963},
	url = {https://doi.org/10.1109/TCBB.2020.2980831},
	doi = {10.1109/TCBB.2020.2980831},
	abstract = {Accurate breast cancer detection using automated algorithms remains a problem within the literature. Although a plethora of work has tried to address this issue, an exact solution is yet to be found. This problem is further exacerbated by the fact that most of the existing datasets are imbalanced, i.e., the number of instances of a particular class far exceeds that of the others. In this paper, we propose a framework based on the notion of transfer learning to address this issue and focus our efforts on histopathological and imbalanced image classification. We use the popular VGG-19 as the base model and complement it with several state-of-the-art techniques to improve the overall performance of the system. With the ImageNet dataset taken as the source domain, we apply the learned knowledge in the target domain consisting of histopathological images. With experimentation performed on a large-scale dataset consisting of 277,524 images, we show that the framework proposed in this paper gives superior performance than those available in the existing literature. Through numerical simulations conducted on a supercomputer, we also present guidelines for work in transfer learning and imbalanced image classification.},
	number = {1},
	journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
	author = {Singh, Rishav and Ahmed, Tanveer and Kumar, Abhinav and Singh, Amit Kumar and Pandey, Anil Kumar and Singh, Sanjay Kumar},
	month = feb,
	year = {2021},
	note = {Number of pages: 11
Place: Washington, DC, USA
Publisher: IEEE Computer Society Press
tex.issue\_date: Jan.-Feb. 2021},
	pages = {83--93},
}

@inproceedings{10.1145/3368756.3369089,
	address = {New York, NY, USA},
	series = {{SCA} '19},
	title = {Proposed approach for breast cancer diagnosis using machine learning},
	isbn = {978-1-4503-6289-4},
	url = {https://doi.org/10.1145/3368756.3369089},
	doi = {10.1145/3368756.3369089},
	abstract = {Due to the danger that represents breast cancer in the society and also due to the number of death that causes every year in women's sector, the proposition of solutions that aim to reduce this number of death has become a primordial need. In this paper we present an approach for breast cancer diagnosis using machines learning techniques that can be helpful in breast cancer detection in earlier stage and with the right way without errors to propose the suitable treatment for the patients. We will use machines learning techniques in our approach due to their performance in domain of medicine.},
	booktitle = {Proceedings of the 4th international conference on smart city applications},
	publisher = {Association for Computing Machinery},
	author = {Saoud, Hajar and Ghadi, Abderrahim and Ghailani, Mohamed},
	year = {2019},
	note = {Number of pages: 5
Place: Casablanca, Morocco
tex.articleno: 102},
	keywords = {classification, breast cancer, clustering, diagnostic},
}

@inproceedings{10.1145/3468945.3468965,
	address = {New York, NY, USA},
	series = {{IMIP} '21},
	title = {Data science for characterizing breast cancer},
	isbn = {978-1-4503-9005-7},
	url = {https://doi.org/10.1145/3468945.3468965},
	doi = {10.1145/3468945.3468965},
	booktitle = {2021 3rd international conference on intelligent medicine and image processing},
	publisher = {Association for Computing Machinery},
	author = {Wang, Lingxiao},
	year = {2021},
	note = {Number of pages: 5
Place: Tianjin, China},
	keywords = {Machine learning, data science, breast cancer, Random forest},
	pages = {122--126},
	file = {Wang_2021_Data science for characterizing breast cancer.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\AYLK3HYC\\Wang_2021_Data science for characterizing breast cancer.pdf:application/pdf},
}

@inproceedings{10.1145/3316615.3316695,
	address = {New York, NY, USA},
	series = {{ICSCA} '19},
	title = {Segmentation of microscopic breast cancer images for cancer detection},
	isbn = {978-1-4503-6573-4},
	url = {https://doi.org/10.1145/3316615.3316695},
	doi = {10.1145/3316615.3316695},
	abstract = {Breast cancer is one of serious diseases that affect mainly woman and late diagnosis can lead to death. However early diagnosis increases survivability significantly, therefore making it very important. There are different diagnosis techniques for early detection of breast cancer. Breast tissue samples analyzed under microscope is considered reliable way to diagnose breast cancer. Automated classification techniques are so popular in many areas in order to reduce human dependency considering third world countries. Our purpose is to determine if sample is malignant or benign in automated manner. Many algorithms are studied so far in medical area along with other areas. However, algorithms are generally too complex even for simple tasks. We propose a simple algorithm that can differentiate cancerous and non-cancerous samples from breast tissue in automated manner. The images were taken from Near East University Hospital which is consisted of 50 cancerous and 100 healthy images. Total of 150 images were correctly differentiated through our algorithm.},
	booktitle = {Proceedings of the 2019 8th international conference on software and computer applications},
	publisher = {Association for Computing Machinery},
	author = {Altıparmak, Hamit and Nurçin, Fatih Veysel},
	year = {2019},
	note = {Number of pages: 4
Place: Penang, Malaysia},
	keywords = {Breast cancer, cancer detection, color segmentation, image process, microscopic imaging},
	pages = {268--271},
}

@inproceedings{10.1145/3473258.3473270,
	address = {New York, NY, USA},
	series = {{ICBBT} 2021},
	title = {Expression and prognosis of {HER} family in breast cancer},
	isbn = {978-1-4503-8965-5},
	url = {https://doi.org/10.1145/3473258.3473270},
	doi = {10.1145/3473258.3473270},
	abstract = {Human epidermal growth factor receptor (HER) family belongs to the transmembrane protein receptor family of the tyrosine kinase I subfamily and has been determined to be closely related to the clinicopathological characteristics of various tumors and the poor prognosis of tumor patients. However, the different expression patterns of the four HER family genes and their correlation with cancer immune infiltrate have not been analyzed. In this article, we analyzed the expression level of the HER family in breast cancer patients through Oncomine and Interactive Analysis of Gene Expression Profiles (GEPIA). We found that the expression levels of ERBB2 and ERBB3 in invasive breast cancer (BCRA) tissues were higher than those in normal breast tissues, while the expression levels of EGFR in the former were lower than those in the latter. Using the Tumor Immunity Estimation Resource (TIMER) database to study the correlation between the HER family and breast cancer immune infiltrates shows that the expression of the HER family is significantly associated with tumor purity and the level of infiltration of various immune cells. This study laid the foundation for further exploration of the molecular mechanism of the HER family in breast cancer.},
	booktitle = {2021 13th international conference on bioinformatics and biomedical technology},
	publisher = {Association for Computing Machinery},
	author = {Huang, You and Chen, Xuan and Wu, Qifang and Zhang, Huimin and Wang, Jun and Shen, Chao and Liao, Xinghua},
	year = {2021},
	note = {Number of pages: 10
Place: Xi'an, China},
	keywords = {breast cancer, EGFR, ERBB2, ERBB3, ERBB4, HER family, prognosis, umor-infiltrating},
	pages = {71--80},
}

@inproceedings{10.1145/3431943.3432282,
	address = {New York, NY, USA},
	series = {{ICBBS} '20},
	title = {Screening potential biomarkers of breast cancer based on bioinformatics},
	isbn = {978-1-4503-8865-8},
	url = {https://doi.org/10.1145/3431943.3432282},
	doi = {10.1145/3431943.3432282},
	abstract = {Breast cancer (BRCA) is a common cancer, and incidence is highest among women with cancer. This study chose gene expression profile of GSE65194, GSE42568, GSE7904 and GSE10810 from GEO databases in order to screen potential biomarkers of breast cancer. There are 393 samples, including 331 cancer samples and 62 normal samples. We screened the differentially expressed genes (DEGs) from four groups between BRCA samples and normal samples, then 150 common DEGs were detected, including 37 upregulated genes and 113 downregulated genes. Next this study used Database for Annotation Visualization and Integrated Discovery (DAVID) performed Gene Ontology (GO) and Kyoto Encyclopedia of Gene and Genomes (KEGG) pathway analysis. Moreover, we selected 15 core genes with high connectivity, including CCNB1, CDC20, BUB1B, AURKA, CDK1 and RRM2. The Kaplan Meier plotter (KM plotter) analyzed these six core genes survival rate. Finally, this study analyzed that all six genes were significantly expressed in BRCA. In conclusion, the bioinformatics analysis demonstrated that the six core genes CCNB1, CDK1, BUB1B, CDC20, AURKA and RRM2 might promote the development of BRCA, that could became new biomarkers for diagnosis and medications of BRCA.},
	booktitle = {Proceedings of the 2020 9th international conference on bioinformatics and biomedical science},
	publisher = {Association for Computing Machinery},
	author = {Liu, Wenjia and Ying, Nanjiao and Mo, Qiusi and Zhu, Lei},
	year = {2021},
	note = {Number of pages: 9
Place: Xiamen, China},
	keywords = {Diagnosis, Breast cancer, Bioinformatics analysis, Biomarkers, Differential expressed genes, Gene expression profile, GEO databases},
	pages = {49--57},
}

@inproceedings{10.1145/3309129.3309133,
	address = {New York, NY, USA},
	series = {{ICBRA} '18},
	title = {Breast cancer prediction using spark {MLlib} and {ML} packages},
	isbn = {978-1-4503-6611-3},
	url = {https://doi.org/10.1145/3309129.3309133},
	doi = {10.1145/3309129.3309133},
	abstract = {Nowadays, Machine Learning has been applied in variety aspects of life especially in health care. Classifications using Machine learning has been greatly improved in order to make predictions and to support doctors making diagnoses. Furthermore, human lives are changing with Big Data covering a wide of array of science knowledge and with Data Mining solving problems by analyzing data and discovering patterns in present databases. The prediction process is heavily data driven and therefore advanced machine learning techniques are often utilized. In this paper, we will take a look at what types experiment data are typically used, do preliminary analysis on them, and generate breast cancer prediction models - all with PySpark and its machine learning frameworks. Using a database with more than a hundred sets of data gathered in routine blood analysis, the accuracy rates of detection and classification are about 72\% and 83\% respectively.},
	booktitle = {Proceedings of the 5th international conference on bioinformatics research and applications},
	publisher = {Association for Computing Machinery},
	author = {Hung, Phan Duy and Hanh, Tran Duc and Diep, Vu Thu},
	year = {2018},
	note = {Number of pages: 8
Place: Hong Kong, Hong Kong},
	keywords = {Breast cancer, Apache spark, ML packages, MLlib},
	pages = {52--59},
}

@inproceedings{10.1145/3286606.3286861,
	address = {New York, NY, USA},
	series = {{SCA} '18},
	title = {Application of data mining classification algorithms for breast cancer diagnosis},
	isbn = {978-1-4503-6562-8},
	url = {https://doi.org/10.1145/3286606.3286861},
	doi = {10.1145/3286606.3286861},
	abstract = {Breast cancer is one of the diseases that represent a large number of incidence and mortality in the world. Data mining classifications techniques will be effective tools for classifying data of cancer to facilitate decision-making.The objective of this paper is to compare the performance of different machine learning algorithms in the diagnosis of breast cancer, to define exactly if this type of cancer is a benign or malignant tumor.Six machine learning algorithms were evaluated in this research Bayes Network (BN), Support Vector Machine (SVM), k-nearest neighbors algorithm (Knn), Artificial Neural Network (ANN), Decision Tree (C4.5) and Logistic Regression. The simulation of the algorithms is done using the WEKA tool (The Waikato Environment for Knowledge Analysis) on the Wisconsin breast cancer dataset available in UCI machine learning repository.},
	booktitle = {Proceedings of the 3rd international conference on smart city applications},
	publisher = {Association for Computing Machinery},
	author = {Saoud, Hajar and Ghadi, Abderrahim and Ghailani, Mohamed and Abdelhakim, Boudhir Anouar},
	year = {2018},
	note = {Number of pages: 7
Place: Tetouan, Morocco
tex.articleno: 84},
	keywords = {classification, diagnostic, Brest cancer, machines learning algorithms, WEKA},
}

@inproceedings{10.1145/3529190.3534769,
	address = {New York, NY, USA},
	series = {{PETRA} '22},
	title = {A deep-learning based diagnostic framework for breast cancer},
	isbn = {978-1-4503-9631-8},
	url = {https://doi.org/10.1145/3529190.3534769},
	doi = {10.1145/3529190.3534769},
	abstract = {In this paper, we present a deep-learning based diagnostic pipeline for breast cancer that has been designed in the H2020 INCISIVE project. The design of the pipeline has taken into consideration the needs of medical professionals and has been adapted to focus on early and accurate detection of malignant lesions to improve the patient’s survival rate. The main goal of our approach is to create a complete diagnostic service and bridge the gap towards real-world adoption of Artificial Intelligence on medical imaging. The pipeline will be offered as a service to medical professionals during the pilots of the project to evaluate its performance and assess the maturity of integrating such a service in a clinical workflow.},
	booktitle = {Proceedings of the 15th international conference on {PErvasive} technologies related to assistive environments},
	publisher = {Association for Computing Machinery},
	author = {Sykiotis, Stavros and Tzortzis, Ioannis and Angeli, Aikaterini and Doulamis, Nikolaos and Kalogeras, Dimitrios},
	year = {2022},
	note = {Number of pages: 5
Place: Corfu, Greece},
	keywords = {artificial intelligence, breast cancer, AI as a service, early diagnosis, INCISIVE},
	pages = {641--645},
}

@inproceedings{10.1145/3459930.3471167,
	address = {New York, NY, USA},
	series = {{BCB} '21},
	title = {Identifying biomarkers of nottingham prognosis index in breast cancer survivability},
	isbn = {978-1-4503-8450-6},
	url = {https://doi.org/10.1145/3459930.3471167},
	doi = {10.1145/3459930.3471167},
	abstract = {Nottingham Prognostics Index (NPI) is a widely-used prognostics measure used to predict survival of operable primary breast cancer. The NPI value is calculated based on the size of the tumor, the number of lymph nodes and the grade of the tumor. This work builds a prediction model for the NPI \&lt; 3.4 versus NPI ≥ 3.4, where this threshold is the cut-off between high survival rate versus the low survival rate.In this study, we present a supervised learning method used to predict the breast cancer NPI. The objectives of this research are (i) build a diagnosis system for breast cancer NPI based on multi-omics data; (ii) find gene biomarkers for each low and high NPI scores; (iii) build a novel prediction model based on t-distributed stochastic neighbor embedding (t-SNE) and residual neural network (ResNet) to integrate multi-omics data in the classification mechanism. The results show that two sets of biomarkers that include two different omics, namely gene expression and copy number alteration, can be integrated in the model to achieve a high prediction accuracy. Findings in the literature confirm the associations between some of these genes and breast cancer.},
	booktitle = {Proceedings of the 12th {ACM} conference on bioinformatics, computational biology, and health informatics},
	publisher = {Association for Computing Machinery},
	author = {Zhou, Li and Rueda, Maria and Alkhateeb, Abedalrhman},
	year = {2021},
	note = {Number of pages: 9
Place: Gainesville, Florida
tex.articleno: 87},
	keywords = {breast cancer, multi-omics data integration, nottingham prognostics index, residual neural network, t-distributed stochastic neighbor embedding},
}

@inproceedings{10.1145/3570991.3571046,
	address = {New York, NY, USA},
	series = {{CODS}-{COMAD} '23},
	title = {Addressing data intrinsic characteristics for augmentation for breast cancer classification},
	isbn = {978-1-4503-9797-1},
	url = {https://doi.org/10.1145/3570991.3571046},
	doi = {10.1145/3570991.3571046},
	abstract = {Breast cancer is the most frequently diagnosed cancer among females worldwide. The task of correctly diagnosing cancer using histopathology in its very earlier stages is a challenging and critical task. Most of the present machine learning techniques require a lot of data to analyze and predict a benign tumour in its early stages, and such data is not available readily. In this paper, we propose the idea of data augmentation of breast cancer tissue images by addressing data intrinsic characteristics. The aim is to detect the micro presence of the tumour cells and highlight it over multiple synthetic images for classifiers to predict benign tumours in very early stages with high accuracy. The initial experimental analysis highlights the proposed technique’s impact and significance in boosting the performance of standard classifier(s).},
	booktitle = {Proceedings of the 6th joint international conference on data science \&amp; management of data (10th {ACM} {IKDD} {CODS} and 28th {COMAD})},
	publisher = {Association for Computing Machinery},
	author = {Garg, Armaan},
	year = {2023},
	note = {Number of pages: 2
Place: Mumbai, India},
	keywords = {Breast cancer, Classification., Data augmentation, Histopathology, Tissue images},
	pages = {299--300},
}

@inproceedings{10.1145/3404555.3404577,
	address = {New York, NY, USA},
	series = {{ICCAI} '20},
	title = {An improved breast cancer nuclei segmentation method based on {UNet}++},
	isbn = {978-1-4503-7708-9},
	url = {https://doi.org/10.1145/3404555.3404577},
	doi = {10.1145/3404555.3404577},
	abstract = {Nuclei segmentation plays an important role in medical image analysis but it is also a challenging area due to the tiny size of nuclei especially for breast cancer nuclei. To address these challenges, in this paper we present an improved UNet++ architecture, a more powerful architecture for nuclei segmentation. The original UNet++, which is an encoder-decoder architecture with a series of nested and dense skip pathways, is used as the framework in our work. The main reason for the increase in ability is that the Inception-ResNet-V2 network is added as backbone, which is a very deep network with brilliant performance in object detection. We have evaluated our improved UNet++ in comparison with UNet and the original UNet++ architectures in breast cancer nuclei segmentation dataset. The experiments demonstrate that our improved UNet++ is superior to U-Net and the original U-Net++.},
	booktitle = {Proceedings of the 2020 6th international conference on computing and artificial intelligence},
	publisher = {Association for Computing Machinery},
	author = {Wang, Haonan and Li, Yinhan and Luo, Zhiyi},
	year = {2020},
	note = {Number of pages: 5
Place: Tianjin, China},
	keywords = {deep learning, Breast cancer, deep neural network, improved UNet++, nuclei segmentation},
	pages = {193--197},
}

@article{uedaDevelopmentValidationDeep2022,
	title = {Development and validation of a deep learning model for detection of breast cancers in mammography from multi-institutional datasets},
	volume = {17},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0265751},
	doi = {10.1371/journal.pone.0265751},
	abstract = {Objectives
              The objective of this study was to develop and validate a state-of-the-art, deep learning (DL)-based model for detecting breast cancers on mammography.
            
            
              Methods
              Mammograms in a hospital development dataset, a hospital test dataset, and a clinic test dataset were retrospectively collected from January 2006 through December 2017 in Osaka City University Hospital and Medcity21 Clinic. The hospital development dataset and a publicly available digital database for screening mammography (DDSM) dataset were used to train and to validate the RetinaNet, one type of DL-based model, with five-fold cross-validation. The model’s sensitivity and mean false positive indications per image (mFPI) and partial area under the curve (AUC) with 1.0 mFPI for both test datasets were externally assessed with the test datasets.
            
            
              Results
              The hospital development dataset, hospital test dataset, clinic test dataset, and DDSM development dataset included a total of 3179 images (1448 malignant images), 491 images (225 malignant images), 2821 images (37 malignant images), and 1457 malignant images, respectively. The proposed model detected all cancers with a 0.45–0.47 mFPI and had partial AUCs of 0.93 in both test datasets.
            
            
              Conclusions
              The DL-based model developed for this study was able to detect all breast cancers with a very low mFPI. Our DL-based model achieved the highest performance to date, which might lead to improved diagnosis for breast cancer.},
	language = {en},
	number = {3},
	urldate = {2023-05-25},
	journal = {PLOS ONE},
	author = {Ueda, Daiju and Yamamoto, Akira and Onoda, Naoyoshi and Takashima, Tsutomu and Noda, Satoru and Kashiwagi, Shinichiro and Morisaki, Tamami and Fukumoto, Shinya and Shiba, Masatsugu and Morimura, Mina and Shimono, Taro and Kageyama, Ken and Tatekawa, Hiroyuki and Murai, Kazuki and Honjo, Takashi and Shimazaki, Akitoshi and Kabata, Daijiro and Miki, Yukio},
	editor = {Baltzer, Pascal A. T.},
	month = mar,
	year = {2022},
	pages = {e0265751},
	file = {Ueda et al_2022_Development and validation of a deep learning model for detection of breast.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SIWPEV2P\\Ueda et al_2022_Development and validation of a deep learning model for detection of breast.pdf:application/pdf},
}
